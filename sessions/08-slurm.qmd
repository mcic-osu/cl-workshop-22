---
title: "Compute Jobs with Slurm"
subtitle: "With a focus on submitting shell scripts"
highlight-style: oblivion
number-sections: true
knitr:
  opts_knit:
    root.dir: "sandbox"
---

```{r knitr_options, echo=FALSE}
knitr::opts_chunk$set(eval = FALSE,
                      class.output = "bash-out")
```

:::{.callout-warning}
PAGE STILL UNDER CONSTRUCTION
:::

----

We have so far been working on _login nodes_ at OSC,
but in order to run some actual analyses,
you will need access to **compute nodes**.
  
Automated scheduling software allows hundreds of people with different
requirements to access compute nodes effectively and fairly.
For this purpose, OSC uses the **Slurm** scheduler
(**S**imple **l**inux **u**tility for **r**esource **m**anagement).

A temporary reservation of (parts of) a compute node is called a _compute job_.
What are the options to start a compute job at OSC?

1. "**Interactive Apps**" &mdash; We can start programs with GUIs,
   such as RStudio or Jupyter Notebook on OnDemand,
   and they will run in a browser window.
2. **Interactive shell jobs** &mdash; Start a Bash shell on a compute node.
3. **Batch (non-interactive) jobs**: &mdash; Run a _script_ on a compute node.

When running command-line programs for genomics analyses,
_batch jobs_ are the most useful and will be the focus of this module.
We'll also touch on _interactive shell jobs_,
which can occasionally be handy
and are requested and managed in a very similar way to batch jobs.

## Setup

:::{.callout-note collapse="true"}
## Starting a VS Code session with an active terminal (click here)

1. Log in to OSC at <https://ondemand.osc.edu>.
2. In the blue top bar, select `Interactive Apps` and then `Code Server`.
3. In the form that appears:
   - Enter `3` in the box `Number of hours`
   - Enter `/fs/ess/scratch/PAS2250` in the box `Working Directory`
   - Click `Launch`.
4. On the next page, once the top bar of the box has turned green and says `Runnning`,
   click `Connect to VS Code`.
5. Open a terminal by clicking the "hamburger menu" way in the top left,
   and then `Terminal` > `New Terminal`.
6. In the terminal, type `Bash` and press <kbd>Enter</kbd>.

:::

<br>

## Interactive shell jobs

Interactive shell jobs will grant you interactive shell access on a compute node.
Working in an interactive shell job is operationally identical to working on
a login node as we've been doing so far, but
**the difference is that it's now okay to use significant computing resources**.
(How much and for how long depends on what you reserve.)

### Using `srun`

A couple of different commands can be used to start an interactive shell job.
I prefer the general `srun` command[^1],
which we can use with the `--pty /bin/bash` option to get an interactive
Bash shell.

[^1]: Other options: `salloc` works almost identically to `srun`,
      whereas `sinteractive` is an OSC convenience wrapper but with very
      limited options.
     
However, if we run that command without additional options, we get an error:

```{bash}
srun --pty /bin/bash
```

:::{.bash-out}
srun: error: ERROR: Job invalid: Must specify account for job  
srun: error: Unable to allocate resources: Unspecified error
:::

As the error message `Must specify account for job` tries to tell us,
**we need to indicate which _OSC project_ (or as SLURM puts it, "_account_")**
**we want to use** for this compute job.
This is because an OSC project always has to be charged for the computing
resources used during a compute job.

To specify the project/account,
we can use the `--account=` option followed by the project number:

```{bash}
srun --account=PAS2250 --pty /bin/bash
```

:::{.bash-out}
srun: job 12431932 queued and waiting for resources  
srun: job 12431932 has been allocated resources

[...regular login info, such as quota, not shown...]

[jelmer@p0133 PAS2250]$
:::

There we go! First we got some Slurm scheduling info:

- Initially, the job is "queued": that is, waiting to start.
- Very soon (usually!), the job has been "allocated resources": that is,
  computing resources such as a compute node were found and reserved for the job.

Then:

- The job starts and because we've reserved an _interactive_ shell job,
  this means that a new Bash shell is initiated:
  for that reason, we get to see our regular login info once again.

- Most importantly, we are no longer on a login node but on a **compute node**,
  as our prompt hints at:
  we switched from something like `[jelmer@pitzer-login04 PAS2250]$` to
  the `[jelmer@p0133 PAS2250]$` shown above.

- Note also that the job has a number (above: `job 12431932`):
  every compute job has such a **unique identifier** among all jobs by all users
  at OSC, and we can use this number to monitor and manage it.
  All of us will therefore see a different job number pop up.

:::{.callout-note}
## OSC projects

During this workshop, we can all use the project `PAS2250`,
which is actually a project that OSC has freely given me to introduce people
to working at OSC.
The project will still be charged but the credits on it were freely awarded.

To work on your own research project at OSC,
you will either have to get your own project
(typically, PIs get one for their lab or for a specific research project)
or you can become an MCIC member and use the MCIC project.
:::


### Compute job options

The `--account=` option is just one of out of _many_ options we can use
when reserving a compute job,
but is the only one that _always_ has to be specified
(including for batch jobs and for Interactive Apps).

Defaults exist for all other options,
such as the amount of time (1 hour) and the number of cores (1).
These options are all specified in the same way for interactive and
batch jobs, and we'll dive into them below.

:::{.callout-tip}
Many SLURM options have a long format (`--account=PAS2250`) and a short
format (`-A PAS2250`), which can generally be used interchangeably.
For clarity, we'll try to stick to long format options during this workshop.
:::

<br>

## Intro to batch jobs

When requesting batch jobs,
we are asking the Slurm scheduler to _run a script_ on a compute node.
For this reason,
we can also refer to it as "submitting a script (to the queue)".

In contrast to interactive shell jobs,
we stay in our current shell on a login node when submitting a script,
and cannot really interact with the process on the compute node, other than:

- **Output from the script that would normally be printed to screen ends up in**
  **a file**.
- We can do things like **monitoring** whether the job is still running and
  **cancelling** the job,
  which will revoke the compute node reservation and stop the ongoing process.

:::{.callout-tip}
The script that we submit can be in different languages but typically,
including in all examples in this workshop, they are shell (Bash) scripts.
:::

### The `sbatch` command

Whereas we used Slurm's `srun` command to start an interactive shell job,
we use its **`sbatch`** command to submit a script.

Recall from the Bash scripting module that we can run a Bash script as follows:

```{bash, eval=TRUE}
bash printname.sh Jane Doe
```

:::{.callout-caution collapse="true"}
## Can't find the printname.sh script?

- Open a new file in the `VS Code` editor
  (&nbsp; {{< fa bars >}} &nbsp; => &nbsp; `File` &nbsp; => &nbsp; `New File`)
  and save it as `printname.sh`
- Copy the code below into the script:
  
```{bash, eval=FALSE}
#!/bin/bash
set -ueo pipefail

first_name=$1
last_name=$2
  
echo "First name: $first_name"
echo "Last name: $last_name"
```

:::

The above command ran the script on our current node, a login node.
To instead submit the script to the Slurm queue,
we would start by simply **replacing `bash` by `sbatch`**:

```{bash}
sbatch printname.sh Jane Doe
```

:::{.bash-out}
srun: error: ERROR: Job invalid: Must specify account for job  
srun: error: Unable to allocate resources: Unspecified error
:::

As we've learned,
we always have to specify the OSC account when submitting a compute job.
Conveniently, we can also specify Slurm/`sbatch` options inside our script,
but first, let's add the `--account` option on the command line:

```{bash}
sbatch --account=PAS2250 printname.sh Jane Doe
```

:::{.bash-out}
Submitted batch job 12431935
:::


:::{.callout-tip}
## `sbatch` options vs. script arguments

```{bash}
sbatch [sbatch-options] myscript.sh [script-arguments]
```

```{bash}
sbatch printname.sh                             # No options/arguments for either
sbatch printname.sh Jane Doe                    # Script arguments but no sbatch option
sbatch --account=PAS2250 printname.sh           # sbatch option but no script arguments
sbatch --account=PAS2250 printname.sh Jane Doe  # Both sbatch option and script arguments
```

:::

### Adding `sbatch` options in scripts

Instead of specifying Slurm/`sbatch` options on the command-line when we submit
the script, we can also add these options _inside the script_.

This is handy because
even though we have so far only seen the `account=` option,
you often want to specify several options.
That would lead to very long `sbatch` commands.
Additionally, it can be practical to store a script's typical Slurm options
along with the script itself.

We add the options in the script using another type of special comment line
akin to the shebang line, marked by `#SBATCH`.
The equivalent of adding `--account=PAS2250` after `sbatch` on the command line,
is a line in  a script that reads `#SBATCH --account=PAS2250`.

Just like the shebang line,
the `#SBATCH` line(s) should be at the top of the script.
Let's add one to the `printname.sh` script, such that the first few lines read:

```{bash, eval=FALSE}
#!/bin/bash

#SBATCH --account=PAS2250

set -ueo pipefail
```

After having added this to the script,
we _can_ successfully run our earlier `sbatch` command without options:

```{bash}
sbatch printname.sh Jane Doe
```

:::{.bash-out}
Submitted batch job 12431942
:::

:::{.callout-tip}
## Running a script with `#SBATCH` in other contexts
Because the `#SBATCH` lines are special _comment_ lines,
they will simply be ignored and not throw any errors when you run a script that
contains them in other contexts: when not running them as a batch job at OSC,
or even when running them on a computer without Slurm installed.
:::

:::{.callout-tip}
Any `sbatch` options provided on the command-line will override the equivalent
options provided in the script itself.
:::

### Where does the output go?

:::{.callout-tip}
Both interactive and batch jobs start in the directory that they were submitted from: that is, your working directory will remain the same.
:::

<br>

## Other common `sbatch` options

### `--time`: Time limit ("wall time").

- Your job gets killed as soon as it hits the time limit!
- Wall time means...
- You will only be charged for the time your job *actually used*.
- In general, shorter jobs are likely to start running sooner
- The default is 1 hour. Acceptable time formats include:
  - `minutes`
  - `hours:minutes:seconds`
  - `days-hours`

```{bash}
#!/bin/bash
#SBATCH --time=1:00:00
```

:::{.callout-tip}
If you are uncertain about the time your job will take,
ask for (much) more time than you think you will need.
:::

:::{.callout-info}
## Walltime limits
For single-node jobs, up to 168 hours (7 days) can be requested.
If that's not enough, you can request access to the `longserial` queue for jobs
of up to 336 hours (14 days).
:::

