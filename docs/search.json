[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Workshop Schedule",
    "section": "",
    "text": "Day\nModule\nInstructor\nTime\n\n\n\n\nDay-1\nIntroduction to the workshop\nMike / Jelmer\nWed 1:00 - 1:15 pm\n\n\n\nThe Ohio Supercomputer Center\nMike\nWed 1:15 - 1:45 pm\n\n\n\nThe VS Code text editor\nJelmer\nWed 1:45 - 2:00 pm\n\n\n\nThe Unix shell\nMike\nWed 2:15 - 4:30 pm\n\n\nDay-2\nShell scripting\nJelmer\nThu 12:00 - 1:30 pm\n\n\n\nUsing software at OSC\nJelmer\nThu 1:45 - 2:30 pm\n\n\nDay-3\nSubmitting scripts with SLURM\nJelmer\nFri 1:00 - 2:30 pm\n\n\n\nRunning example compute jobs\nMike / Jelmer\nFri 2:45 - 4:30 pm"
  },
  {
    "objectID": "info/glossary.html",
    "href": "info/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Directory\nSyntax\nString\npseudocode\nbinary (executable)\ndependency"
  },
  {
    "objectID": "info/info.html#locations-and-links",
    "href": "info/info.html#locations-and-links",
    "title": "Practical Workshop Information",
    "section": "Locations and links",
    "text": "Locations and links\n\nColumbus: Aronoff Laboratory, room 104 (instructor: Mike Sovic)\nWooster: Selby Hall, room 203 (instructor: Jelmer Poelstra)\nZoom: email us for the link! (instructor: Jelmer Poelstra)\nGoogle Doc for sharing links and code, and for non-urgent questions"
  },
  {
    "objectID": "info/info.html#computer-setup",
    "href": "info/info.html#computer-setup",
    "title": "Practical Workshop Information",
    "section": "Computer Setup",
    "text": "Computer Setup\n\nYour computer\nSince we will be working entirely at the Ohio Supercomputer Center (OSC), and will be doing so through our internet browsers:\n\nYou won’t need to install anything\nAny operating system will work\nYou won’t need an especially powerful machine (though browsers, especially in combination with Zoom, can use their fair share of memory).\n\nIf you’re attending in person, you will need to bring a laptop. You can watch the presentation on a big screen in the room, which will make it easier to code along (see below). You won’t need to connect to the Zoom call.\nIf you’re attending via Zoom, we would recommend a two-monitor setup. This is because much of the time, you need to be able to simultaneously see the instructor’s screen via Zoom as well as your own browser window.\n\n\nOSC account and project\nTo work with OSC resources, we need access to an “OSC project”. We will be using the project PAS2250 during the workshop, and all participants will be added to that project. If you don’t yet have a personal OSC account, you will receive an invitation to create one when you’ve been added to the project.\n\n\nGoogle Doc\nWe’ll use this Google Doc for sharing links and code, and for non-urgent questions."
  },
  {
    "objectID": "info/info.html#miscellaneous-info",
    "href": "info/info.html#miscellaneous-info",
    "title": "Practical Workshop Information",
    "section": "Miscellaneous info",
    "text": "Miscellaneous info\n\nExpect to participate!\nThe modules will be a mixture of lectures that include “participatory live-coding” (also called “code-along”; with the instructor slowly demonstrating and participants expected to follow along for themselves) and small single-person exercises (we won’t be doing breakout rooms / groups). Therefore, be prepared to actively participate during much of the workshop!\n\n\nExample data\nWe will mainly use a set of FASTQ files from a published RNAseq experiment as example data. (It may be worth emphasizing that the exact data type matters relatively little for the purposes of our workshop, since we focus on foundational skills and not specific genomic analyses.)\nIf you have any genomic data of your own, you can bring it along and you should be able to experiment a bit with it during our second session on Friday afternoon. If this is a large dataset (say, >10GB), uploading it to OSC will take some time. You could try to start this after Wednesday’s sessions, when you’ve had some background on this. Alternatively, you can contact the instructors about this prior to the workshop.\n\n\nParticipants\nWe’re expecting up to 14 people in Wooster, 14 in Columbus, and 6 via Zoom."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Command line basics  for genomic analysis at OSC",
    "section": "",
    "text": "Approximate Schedule\nStart and end times for every day will be respected, but individual modules may take shorter or longer than indicated below. The instructors will be available for additional questions from about 15 minutes before we start and for about 30 minutes after we end.\n\n\n\nDay\nModule\nInstructor\nTime\n\n\n\n\nDay 1\n1. Introduction to the Workshop\nMike / Jelmer\nWed 1:00 - 1:15 pm\n\n\n\n2. The Ohio Supercomputer Center (OSC)\nMike\nWed 1:15 - 1:45 pm\n\n\n\n3. The VS Code Text Editor\nJelmer\nWed 1:45 - 2:00 pm\n\n\n\n4. The Unix Shell\nMike\nWed 2:15 - 4:30 pm\n\n\nDay 2\n5. Variables and Loops\nJelmer\nThu 12:00 - 12:45 pm\n\n\n\n6. Shell Scripting\nJelmer\nThu 12:45 - 1:30 pm\n\n\n\n7. Using Software at OSC\nJelmer\nThu 1:45 - 2:30 pm\n\n\nDay 3\n8. Compute Jobs with Slurm\nJelmer\nFri 1:00 - 2:30 pm\n\n\n\n9. Running Example Compute Jobs\nMike / Jelmer\nFri 2:45 - 4:30 pm"
  },
  {
    "objectID": "sessions/vscode.html",
    "href": "sessions/vscode.html",
    "title": "The VS Code Text Editor",
    "section": "",
    "text": "In this module, we will learn the basics of a fancy text editor called Visual Studio Code (VS Code for short). Conveniently, we can use a version of this editor (sometimes referred to as Code Server) in our browser via the OSC OnDemand website.\nWe will use VS Code throughout the workshop as practically a one-stop solution for our computing activities at OSC: accessing the Unix shell and writing scripts. This is also how I use this editor in my daily work.\nTo emphasize the additional functionality relative to basic text editors like Notepad and TextEdit, editors like VS Code are also referred to as “IDEs”: Integrated Development Environments. If you’ve ever worked with R, the RStudio program is another good example of an IDE."
  },
  {
    "objectID": "sessions/vscode.html#starting-vs-code-at-osc",
    "href": "sessions/vscode.html#starting-vs-code-at-osc",
    "title": "The VS Code Text Editor",
    "section": "1 Starting VS Code at OSC",
    "text": "1 Starting VS Code at OSC\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nEnter 3 in the box Number of hours\nEnter /fs/ess/scratch/PAS2250 in the box Working Directory\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\n\n\n\n\n\n\n\n\n\n\nVS Code runs on a login node\n\n\n\nIn the previous module, we’ve learned that all serious computation at OSC should be done not on login nodes but on compute nodes.\nStarting an RStudio session, for instance, requires filling out a similar form, and RStudio will subsequently run on a compute node and your selected OSC project will be charged.\nRunning VS Code is a slightly peculiar case: we do have to fill out a form and reserve a pre-specified number of hours (the session will actually stop working after the allotted time has passed), but we’re on a login node and are not being charged."
  },
  {
    "objectID": "sessions/vscode.html#getting-started-with-vs-code",
    "href": "sessions/vscode.html#getting-started-with-vs-code",
    "title": "The VS Code Text Editor",
    "section": "2 Getting started with VS Code",
    "text": "2 Getting started with VS Code\n\n\n\n\n2.1 Side bars\nThe narrow side bar on the far left has:\n\nA      (“hamburger menu” icon) in the top, which has most of the standard menu items that you often find in a top bar, like File.\nA      (cog wheel icon) in the bottom, through which you can mainly access settings.\nA bunch of icons in the middle that serve to switch between different options for the wide side bar (to the right of the narrow side bar), which can show:\n\nExplorer: File browser (and, e.g., an outline for the active file)\nSearch: To search recursively across all files in the active folder\nSource Control: To work with version control systems like Git (not used in this workshop)\nRun and Debug: For debugging your code (not used in this workshop)\nExtensions: To install extensions (we’ll install one later)\n\n\n\n\n2.2 Editor pane and Welcome document\nThe main part of the VS Code is the editor pane. Whenever you open VS Code, a tab with a Welcome document is automatically opened. This provides some help for beginners, but also, for example, an overview of recently opened folders.\nWe can also use the Welcome document to open a new text file by clicking New file below Start (alternatively, click      =>   File   =>   New File). We’ll work with files starting tomorrow, but if you want, you could already start a file with notes on the workshop now.\n\n\n\n\n\n\nRe-open the Welcome document\n\n\n\nIf you’ve closed the Welcome document but want it back, click      =>   Help   =>   Welcome.\n\n\n\n\n2.3 Terminal\nBy default, no terminal is open in VS Code – to do so, click the      => Terminal => New Terminal.\nIn the terminal, the prompt says Singularity>. This is because in OSC OnDemand, VS Code runs inside a Singularity container (for our purposes, it is not important what that means, exactly). To break out of the Singularity shell and get a regular Bash Unix shell, type bash and press Enter.\nIn the next module, Mike will teach us how to use the terminal."
  },
  {
    "objectID": "sessions/vscode.html#some-tips-and-tricks",
    "href": "sessions/vscode.html#some-tips-and-tricks",
    "title": "The VS Code Text Editor",
    "section": "3 Some tips and tricks",
    "text": "3 Some tips and tricks\n\n3.1 Making use of your screen’s real estate\nSince we are using VS Code inside a browser window, we are unfortunately losing some screen space. Make sure to maximize the browser window and if you have a bookmarks bar, try to hide it (for Chrome: Ctrl/⌘+Shift+B).\n\n\n3.2 Resizing panes\nYou can resize panes (the terminal, editor, and wide sidebar) by hovering your cursor over the borders and then dragging it.\n\n\n3.3 The Command Palette\nTo access all the menu options that are available in VS Code, the so-called “Command Palette” can be handy, especially if you know what you are looking for.\nTo access the Command Palette, click      and then Command Palette or press F1 (or Ctrl/⌘+Shift+P).\nFor a quick test, open the Command Palette and start typing “color theme”, and you’ll see the relevant options pop up.\n\n\n3.4 Color themes\nTo try out different color themes for the entire program, click      and then Color Theme. (I like “Quiet Light”.)"
  },
  {
    "objectID": "sessions/vscode.html#working-directory",
    "href": "sessions/vscode.html#working-directory",
    "title": "The VS Code Text Editor",
    "section": "4 Working directory",
    "text": "4 Working directory\nSetting a “working directory” means that you designate a folder on a computer as the starting point for your operations.\n\n\n\n\n\n\nFolder vs. directory\n\n\n\n“Folder” and “directory” mean the same thing – the latter is most commonly used in the context of the Unix Shell.\n\n\nVS Code has a concept of a working directory that is effective in all parts of the program: in the file explorer in the side bar, in the terminal, and when saving or opening files in the editor.\nIn this workshop, we’ll exclusively work within the folder /fs/ess/scratch/PAS2250 (you’ll make personal folders within there shortly). By opening this folder beforehand (we did this in the form on the OnDemand site), we make sure that VS Code always takes this folder as a starting point, which will make navigation and saving files much easier.\n\n\n\n\n\n\nTaking off where you were\n\n\n\nAdditionally, when you reopen a folder later, VS Code will to some extent resume where you were before! It will reopen the text files that you had open and if you had an active terminal, it will also open a terminal. This is very convenient, especially when you start working on multiple projects (different folders) in VS Code and switch between those.\n\n\n\n\n\n\n\n\nSwitching folders\n\n\n\nTo switch to a different folder from within VS Code, click      =>   File   =>   Open Folder."
  },
  {
    "objectID": "sessions/vscode.html#addendum-keyboard-shortcuts",
    "href": "sessions/vscode.html#addendum-keyboard-shortcuts",
    "title": "The VS Code Text Editor",
    "section": "5 Addendum: keyboard shortcuts",
    "text": "5 Addendum: keyboard shortcuts\nWorking with keyboard shortcuts (also called “keybindings”) for common operations can be a lot faster than using your mouse. Below are some useful ones for VS Code (for Mac, replace Ctrl by ⌘).\n\n\n\n\n\n\nKeyboard shortcut cheatsheet\n\n\n\nFor a single-page PDF overview of keyboard shortcuts for your operating system:      =>   Help   =>   Keyboard Shortcut Reference. (Or for direct links to these PDFs: Windows / Mac / Linux.)\n\n\n\nToggle the wide side bar: Ctrl+B\nOpen a terminal: Ctrl+` or Ctrl+Shift+C.\nToggle between the terminal and the editor pane: Ctrl+` and Ctrl+1.\nLine actions:\n\nCtrl+X / C will cut/copy the entire line where the cursor is, when nothing is selected (!)\nCtrl+Shift+K will delete a line\nAlt+⬆/⬇ will move lines up or down.\n\nMultiple cursors: Press & hold Ctrl+Shift, then ⬆/⬇ arrows to add cursors upwards or downwards.\nToggle line comment (“comment out” code, and removing those comment signs): Ctrl+/\nSplit the editor window vertically: Ctrl+\\ (See also the options in      View => Editor Layout)\n\n\n\n\n\n\n\nBrowser interference\n\n\n\nUnfortunately, some VS Code and terminal keyboard shortcuts don’t work in this setting where we are using VS Code inside a browser, because existing browser keyboard shortcuts take precedence.\nIf you end up using VS Code a lot in your work, it is therefore worth switching to your own installation of the program (see At-home bonus: local installation)"
  },
  {
    "objectID": "sessions/vscode.html#at-home-bonus-local-installation",
    "href": "sessions/vscode.html#at-home-bonus-local-installation",
    "title": "The VS Code Text Editor",
    "section": "6 At-home bonus: local installation",
    "text": "6 At-home bonus: local installation\nAnother nice feature of VS Code is that it is freely available for all operating systems (and even though it is made by Microsoft, it is also open source).\nTherefore, if you like the program, you can also install it on your own computer and do your local text editing / script writing in the same environment at OSC (it is also easy to install on OSU-managed computers, because it is available in the Self Service software installer).\nEven better, the program can be “tunneled into” OSC, so that your working directory for the entire program can be at OSC rather than on your local computer. This gives the same experience as using VS Code through OSC OnDemand, except that you’re not working witin a browser window, which has some advantages (also: no need to fill out a form, or to break out of the Singularity shell).\nFor installation and SSH-tunneling setup, see this page - TBA."
  },
  {
    "objectID": "sessions/shell-scripting.html#tba",
    "href": "sessions/shell-scripting.html#tba",
    "title": "Shell Scripting",
    "section": "1 TBA",
    "text": "1 TBA\n\n\n\n\n\n\nKeyboard shortcut to run shell commands from the editor\n\n\n\nTo add a keyboard shortcut that will send code selected in the editor pane to the terminal (such that you don’t have to copy and paste):\n\nClick the      (bottom-left) => Keyboard Shortcuts.\nFind Terminal: Run Selected Text in Active Terminal, click on it, then add a shortcut, e.g. Ctrl+Enter."
  },
  {
    "objectID": "sessions/software.html#tba",
    "href": "sessions/software.html#tba",
    "title": "Using Software at OSC",
    "section": "1 TBA",
    "text": "1 TBA"
  },
  {
    "objectID": "sessions/scripting-prep.html",
    "href": "sessions/scripting-prep.html",
    "title": "Variables, Loops, and Conditionals",
    "section": "",
    "text": "In this module, we will cover a couple of topics that are good to know about before you start writing and running shell scripts:"
  },
  {
    "objectID": "sessions/scripting-prep.html#setup",
    "href": "sessions/scripting-prep.html#setup",
    "title": "Variables, Loops, and Conditionals",
    "section": "1 Setup",
    "text": "1 Setup\n\n\n\n\n\n\nStarting a VS Code session with an active terminal (click here)\n\n\n\n\n\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 3 in the box Number of hours\nEnter /fs/ess/scratch/PAS2250 in the box Working Directory\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\nOpen a terminal by clicking the “hamburger menu” way in the top left, and then Terminal > New Terminal.\nIn the terminal, type Bash and press Enter."
  },
  {
    "objectID": "sessions/scripting-prep.html#variables",
    "href": "sessions/scripting-prep.html#variables",
    "title": "Variables, Loops, and Conditionals",
    "section": "2 Variables",
    "text": "2 Variables\nIn programming, we use variables for things that:\n\nWe refer to repeatedly and/or\nAre subject to change.\n\nThese tend to be settings like the paths to input and output files, and parameter values for programs.\n\n2.1 Assigning and referencing variables\nTo assign a value to a variable in Bash (in short: to assign a variable), use the syntax variable=value. For example:\n\n# Assign the value \"low\" to the variable \"treatment\":\ntreatment=low\n\n# Assign the value \"200\" to the variable \"nlines\":\nnlines=200\n\n\n\n\n\n\n\nSpace-sensitive\n\n\n\nBe aware that there can be no spaces around the equals sign (=)!\n\n\nTo reference a variable (i.e., to access its value), you need to put a dollar sign $ in front of its name. We’ll use the echo command to review the values that our variables contain:\n\necho $treatment\n\n\n\nlow\n\n\n\necho $nlines\n\n\n\n200\n\n\nConveniently, we can directly use variables in lots of contexts, as if we had instead typed their values:\n\nls_options=\"-lh\"\n\nls $ls_options\n\ntotal 120K\n-rw-rw-r-- 1 jelmer jelmer  232 Aug  9 21:42 examples.qmd\ndrwxrwxr-x 2 jelmer jelmer 4.0K Aug 10 15:09 img\n-rw-rw-r-- 1 jelmer jelmer  248 Aug  9 21:42 osc-intro.qmd\ndrwxrwxr-x 3 jelmer jelmer 4.0K Aug 14 10:43 scripting-prep_files\n-rw-rw-r-- 1 jelmer jelmer  13K Aug 13 18:33 scripting-prep.qmd\n-rw-rw-r-- 1 jelmer jelmer  13K Aug 14 10:44 scripting-prep.rmarkdown\n-rw-rw-r-- 1 jelmer jelmer  218 Aug  9 21:43 shell-intro.qmd\n-rw-rw-r-- 1 jelmer jelmer  35K Aug 14 10:43 shell-scripting.html\n-rw-rw-r-- 1 jelmer jelmer 6.4K Aug 14 10:16 shell-scripting.qmd\n-rw-rw-r-- 1 jelmer jelmer  235 Aug  9 21:44 slurm.qmd\n-rw-rw-r-- 1 jelmer jelmer  225 Aug  9 21:45 software.qmd\n-rw-rw-r-- 1 jelmer jelmer  11K Aug 11 10:43 vscode.qmd\n-rw-rw-r-- 1 jelmer jelmer 2.9K Aug 11 10:01 workshop-intro.qmd\n\n\n\ninput_file=shell-scripting.qmd\n\nls -lh $input_file \n\n-rw-rw-r-- 1 jelmer jelmer 6.4K Aug 14 10:16 shell-scripting.qmd\n\n\n\n\n2.2 Rules for naming variables\nVariable names:\n\nCan contain letters, numbers, and underscores\nCannot contain spaces, periods, or other special symbols\nCannot start with a number\n\n\n\n2.3 Command substitution\nIf you want to store the result of a command in a variable, you can use a construct called “command substitution” by wrapping the command inside $():\n\n# (date +%F will return the date in YYYY-MM-DD format)\ntoday=$(date +%F)\n\n# Create a file with our $today variable:\ntouch README_$today.txt\n\n# Check the name of our newly created file:\nls README_*\n\nREADME_2022-08-14.txt\n\n\n\n# Define a filename that we'll use in the next few commands:\ninput_file=shell-scripting.qmd\n\n# `wc -l` will count the number of lines\n# Using `<` (input redirection) is a trick to avoid the filename from being printed \nnlines=$(wc -l < $input_file)\n\n# We can directly use the variables in our quoted echo statement:\necho \"The file $input_file has $nlines lines\"\n\nThe file shell-scripting.qmd has 228 lines\n\n\nCommand substitution can for instance be useful when you want your script to report some results, or when a next step in the script depends on a previous result.\n\n\n2.4 Environment variables\nThere are also predefined variables in the Unix shell: that is, variables that exist in your environment by default. These so-called “environment variables” are always spelled in all-caps:\n\n# Environment variable $USER contains your user name \necho $USER\n\njelmer\n\n\n\n# Environment variable $HOME contains the path to your home directory\necho $HOME\n\n/home/jelmer\n\n\nEnvironment variables can provide useful information. We’ll see them again when we talk about the SLURM compute job scheduler.\n\n\n2.5 Quoting variables\nAbove, we learned that a variable name cannot contain spaces. But what happens if our variable’s value contains spaces?\nFirst off, when we try to assign the variable without quotes, we get an error:\n\ntoday=Thu, Aug 18\n\n\nAug: command not found\n\nBut it works when we quote (with double quotes, \"...\") the entire string that makes up the value:\n\ntoday=\"Thu, Aug 18\"\necho $today\n\nThu, Aug 18\n\n\nNow, let’s try to reference this variable:\n\ntouch README2_$today.txt\nls README2_*\n\n\n\nREADME2_Thu,\n\n\n\n\n\n\n\n\nWhat went wrong here? How many files were created?\n\n\n\n\n\nThe shell performed so-called field splitting using a space as a separator, splitting the value into three separate units – as a result, three files were created: README2_Thu, (listed above), as well as Aug and 18.txt.\nThe following code will list all these three files:\n\n# `ls -t` will sort by last-modified date, and `head -n 3` prints the top 3\n# Therefore, this will print the last 3 files that were created/modified\nls -t | head -n 3\n\n18.txt\nAug\nREADME2_Thu,\n\n\n\n\n\nSimilar to what we had to do when assigning the variable, our problems can be avoided by quoting the variable when we reference it:\n\ntouch README3_\"$today\".txt\nls README3_*\n\n\n\nREADME3_Thu, Aug 18.txt\n\n\nAnother issue we can run into when we don’t quote variables is that we can’t explicitly define where a variable name ends within a longer string of text:\n\necho README_$today_final.txt\n\n\n\nREADME_.txt\n\n\n\n\n\n\n\n\nWhat went wrong here?\n\n\n\n\n\n\nFollowing a $, the shell will stop interpreting characters as being part of the variable name only when it encounters a character that cannot be part of a variable name, such as a space or a period.\nSince variable names can contain underscores, it will look for the variable $today_final, which does not exist.\nImportantly, the shell does not error out when you reference a non-existing variable – it basically ignores it, such that README_$today_final.txt becomes README_.txt, as if we hadn’t referenced any variable.\n\n\n\n\nQuoting solves this issue, too:\n\necho README_\"$today\"_final.txt\n\n\n\nREADME_Thu, Aug 18_final.txt\n\n\n\n\n\n\n\n\nMore on quoting – and double vs. single quotes\n\n\n\n\n\nBy double-quoting a variable, we are essentially escaping (or “turning off”) the default special meaning of the space as a separator, and are asking the shell to interpret it as a literal space.\nSimilarly, we are escaping other “special characters”, such as globbing wildcards, with double quotes. Compare:\n\necho *\n\n18.txt Aug examples.qmd img osc-intro.qmd README_2022-08-14.txt README2_Thu, README3_Thu, Aug 18.txt scripting-prep_files scripting-prep.qmd scripting-prep.rmarkdown shell-intro.qmd shell-scripting.html shell-scripting.qmd slurm.qmd software.qmd vscode.qmd workshop-intro.qmd\n\n\n\necho \"*\"\n\n*\n\n\nHowever, as we also saw above, double quotes do not turn off the special meaning of $ (i.e., denoting a string as a variable):\n\necho \"$today\"\n\n\n\nThu, Aug 18\n\n\n…but single quotes will:\n\necho '$today'\n\n$today\n\n\n\n\n\nAll in all, it is good practice to quote variables when you reference them: it never hurts, and avoids unexpected surprises."
  },
  {
    "objectID": "sessions/scripting-prep.html#for-loops",
    "href": "sessions/scripting-prep.html#for-loops",
    "title": "Variables, Loops, and Conditionals",
    "section": "3 For loops",
    "text": "3 For loops\nLoops are a universal element of programming languages, and are extremely useful to repeat operations, such as when you want to run the same script or command for multiple files.\nHere, we’ll only cover what is by far the most common type of loop: the for loop.\nfor loops iterate over a collection, such as a list of files: that is, they allow you to perform one or more actions for each element in the collection, one element at a time.\n\n3.1 for loop syntax and mechanics\nLet’s see a first example, where our collection is just a very short list of numbers (1, 2, and 3):\n\nfor a_number in 1 2 3; do\n    echo \"In this iteration of the loop, the number is $a_number\"\n    echo \"--------\"\ndone\n\nIn this iteration of the loop, the number is 1\n--------\nIn this iteration of the loop, the number is 2\n--------\nIn this iteration of the loop, the number is 3\n--------\n\n\nfor loops contain the following mandatory keywords:\n\n\n\n\n\n\n\nKeyword\nPurpose\n\n\n\n\nfor\nAfter for, we set the variable name\n\n\nin\nAfter in, we specify the collection we are looping over\n\n\ndo\nAfter do, we have one ore more lines specifying what to do with each item\n\n\ndone\nTells the shell we are done with the loop\n\n\n\n\n\n\n\n\n\nWhy the semicolon ; before do?\n\n\n\n\n\nA semicolon separates two commands written on a single line – for instance, instead of:\n\nmkdir results\ncd results\n\n…you could equivalently type:\n\nmkdir results; cd results\n\nThe ; in the for loop syntax has the same function, and as such, an alternative way to format a for loop is:\n\nfor a_number in 1 2 3\ndo\n    echo \"In this iteration of the loop, the number is $a_number\"\ndone\n\nBut that’s one line longer and a bit awkwardly asymmetric.\n\n\n\nIt is important to realize that the loop runs sequentially for each item in the collection, and will therefore run as many times as there are items in the collection.\nThe following example, where we let the computer sleep for 1 second before printing the date and time with the date command, demonstrates that the loop is being executed sequentially:\n\nfor a_number in 1 2 3; do\n    echo \"In this iteration of the loop, the number is $a_number\"\n    #sleep 1s          # Let the computer sleep for 1 second\n    date              # Print the date and time\n    echo \"--------\"\ndone\n\nIn this iteration of the loop, the number is 1\nSun Aug 14 10:44:00 AM CEST 2022\n--------\nIn this iteration of the loop, the number is 2\nSun Aug 14 10:44:00 AM CEST 2022\n--------\nIn this iteration of the loop, the number is 3\nSun Aug 14 10:44:00 AM CEST 2022\n--------\n\n\nThe aspect that is perhaps most difficult to understand is that in each iteration of the loop, one element in the collection (in the example above, either 1, 2, or 3) is being assigned to the variable specified after for (in the example above, a_number).\nWhen we specify the collection “manually”, like we did above with numbers, we separate the elements by a space, as this example also shows:\n\nfor mushroom in morel destroying_angel eyelash_cup; do\n    echo \"$mushroom is an Ohio mushroom\"\ndone\n\nmorel is an Ohio mushroom\ndestroying_angel is an Ohio mushroom\neyelash_cup is an Ohio mushroom\n\n\n\n\n3.2 Looping over files with globbing\nIn practice, we rarely manually list the collection of items we want to loop over. Instead, we commonly loop over files directly using globbing:\n\n# We make sure we only select gzipped FASTQ files using the `*fastq.gz` glob\nfor fastq_file in data/raw/*fastq.gz; do\n    echo \"File $fastq_file has $(wc -l < $fastq_file) lines.\"\n    # More processing...\ndone\n\nIf needed, you can use your globbing / wild card skills to narrow down the file selection:\n\n# Perhaps we only want to select R1 files (forward reads): \nfor fastq_file in data/raw/*R1*fastq.gz; do\n    # Some file processing...\ndone\n\n# Or only filenames starting with A or B:\nfor fastq_file in data/raw/[AB]*fastq.gz; do\n    # Some file processing...\ndone\n\n\n\n\n\n\n\nAlternatives to looping with a glob\n\n\n\n\n\nWith genomics data, the routine of looping over an entire directory of files, or selections made with simple globbing patterns, should serve you very well.\nBut in some cases, you may want to iterate only over a specific list of filenames (or partial filenames such as sample IDs) that represent a complex selection.\n\nIf this is a short list, you could directly specify it in the loop:\n\nfor sample in A1 B6 D3; do\n    R1=data/fastq/\"$sample\"_R1.fastq.gz\n    R2=data/fastq/\"$sample\"_R2.fastq.gz\n    # Some file processing...\ndone\n\nIf it is a longer list, you could create a simple text file with one line per sample ID / filename, and use command substitution as follows:\n\nfor fastq_file in $(cat file_of_filenames.txt); do\n    # Some file processing...\ndone\n\n\n(In cases like this, Bash arrays (basically, variables that consist of multiple values, like a vector in R) or while loops may provide more elegant solutions, but those are outside the scope of this introduction.)"
  },
  {
    "objectID": "sessions/scripting-prep.html#conditionals",
    "href": "sessions/scripting-prep.html#conditionals",
    "title": "Variables, Loops, and Conditionals",
    "section": "4 Conditionals",
    "text": "4 Conditionals\nWith conditionals, we can run one or more commands only if some condition is true. Also, we can run a different set of commands if the condition is not true. For instance, we may want to process a file differently depending on its file type.\nThis is the basic syntax of an if statement in Bash:\nif [ <test> ]; then\n    # Command(s) if condition is true\nelse\n    # Commands(s) if condition is false\nfi\nAbove, <test> is pseudo-code denoting that a logical test\nFor instance (pseudocode):\nif [ <file is a text file> ]; then\n    # <count the number of lines and columns>\nelse\n    # <print the file type and file size>\nfi\nWe can also omit the else statement:\nif <test>; then\n    # Command(s) if condition is true\nfi\nFor example, an if statement that depends on the file type of the input file:\n\nif [ \"$filetype\" = \"fastq\" ]; then\n    echo \"Processing fastq file...\"\n    # Commands to process fastq file\nelse\n    echo \"Unknown filetype!\"\nfi\n\nAbove, we use the [ ] syntax to perform the test:\n\nThe square brackets represent a test statement.\nThe spaces bordering the brackets on the inside are necessary:\n[\"$filetype\" = \"fastq\"] would fail!"
  },
  {
    "objectID": "sessions/shell-intro.html#tba",
    "href": "sessions/shell-intro.html#tba",
    "title": "The Unix Shell",
    "section": "1 TBA",
    "text": "1 TBA"
  },
  {
    "objectID": "sessions/workshop-intro.html#what-you-will-and-wont-learn",
    "href": "sessions/workshop-intro.html#what-you-will-and-wont-learn",
    "title": "Introduction to the Workshop",
    "section": "What you will and won’t learn",
    "text": "What you will and won’t learn\nThe focus of the workshop is on building some general skills for analyzing genomics data.\nCommand-line programs are preferred for many of the steps to analyze genomic sequencing data. Because such datasets tend to contain a lot of data, it is also preferable to run your analyses not on a laptop or desktop, but at a compute cluster like the Ohio Supercomputer Center (OSC).\nThese realities mean that in genomics, you need the following set of skills that you may not have been thought during your biology education:\n\nHaving a basic understanding of a compute cluster (supercomputer)\n\nAnd being able to:\n\nUse the Unix shell (work in a terminal)\nWrite small shell scripts\nSubmit scripts to a “queue” and monitor and manage the resulting compute jobs\nActivate and probably install software in a Linux environment where you don’t have “admin rights”\n\nWe will teach the basics of these skills during this workshop!\nIt may be useful to point out that given this focus, we will not teach you much if anything about:\n\nDetails of genomic data file types\nDetails of specific (genomic) analyses\nMaking biological inferences from your data"
  },
  {
    "objectID": "sessions/workshop-intro.html#mechanics-of-a-hybrid-workshop",
    "href": "sessions/workshop-intro.html#mechanics-of-a-hybrid-workshop",
    "title": "Introduction to the Workshop",
    "section": "Mechanics of a hybrid workshop",
    "text": "Mechanics of a hybrid workshop\nWe have a slightly complicated set up with people in-person in Wooster with one instructor, in-person in Columbus with another instructor, and some people via Zoom. Some notes:\n\nThis website has all the material that we will go through during each of the modules! See the links in the schedule as well as under the top bar menus.\nIn-person participants don’t need to connect to the Zoom call (but are of course allowed to connect, if they can better see the instructor’s screen that way).\nBecause we’re not all on Zoom, we’ll use this Google Doc to share links, inpromptu code that is not on the website, and non-urgent questions.\nWhenever you have a question, please feel free to interrupt and speak up, both in-person and on Zoom. Only if your question is not urgent and you don’t want to interrupt the flow, put it in the Google Doc or ask about it during a break."
  },
  {
    "objectID": "sessions/workshop-intro.html#personal-introductions",
    "href": "sessions/workshop-intro.html#personal-introductions",
    "title": "Introduction to the Workshop",
    "section": "Personal introductions",
    "text": "Personal introductions\n\nInstructors\n\nJelmer Poelstra, Molecular and Cellular Imaging Center (MCIC), Wooster\nMike Sovic, Center for Applied Plant Sciences (CAPS), Wooster\n\n\n\nYou!\nPlease very briefly introduce yourself – include your position, department, and why you wanted to go to this workshop.\n\nAdd figure(s) showing data types & previous experience"
  },
  {
    "objectID": "sessions/slurm.html#tba",
    "href": "sessions/slurm.html#tba",
    "title": "Submitting scripts with SLURM",
    "section": "1 TBA",
    "text": "1 TBA"
  },
  {
    "objectID": "sessions/examples.html#tba",
    "href": "sessions/examples.html#tba",
    "title": "Running example compute jobs",
    "section": "1 TBA",
    "text": "1 TBA"
  },
  {
    "objectID": "sessions/osc-intro.html#tba",
    "href": "sessions/osc-intro.html#tba",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "1 TBA",
    "text": "1 TBA"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "General Workshop Info & Signing Up",
    "section": "",
    "text": "This workshop is geared towards people who would like to get started with analyzing genomic datasets.\nIt will be taught in-person with video-linking at the Wooster and Columbus Ohio State campuses, and it is also possible to join online through Zoom. We will have an instructor at each campus: Jelmer Poelstra from the Molecular and Cellular Imaging Center (MCIC) at the Wooster campus, and Mike Sovic from the Center for Applied Plant Sciences (CAPS) at the Columbus campus.\nThe workshop will be highly hands-on and take place across three afternoons:\nWed, Aug 17 - Fri, Aug 19, 2022.\n\nAnyone affiliated with The Ohio State University or Wooster USDA can attend\nAttendance is free\nNo prior experience with coding or genomic data is required\nYou will need to bring a laptop and don’t need to install anything prior to or during the workshop\nWe will work with example genomics data but if you have any, you are also welcome to bring your own data.\n\nSee below for information about the contents of the workshop and to sign up.\nFor questions, please email Jelmer."
  },
  {
    "objectID": "about.html#contents-of-the-workshop",
    "href": "about.html#contents-of-the-workshop",
    "title": "General Workshop Info & Signing Up",
    "section": "Contents of the workshop",
    "text": "Contents of the workshop\nThe focus of the workshop is on building general skills for analyzing genomics data, such as RNAseq, metabarcoding, metagenomic shotgun sequencing, or whole-genome sequencing. These skills boil down to the ability to write small shell scripts that run command-line programs and submit these scripts to a compute cluster – in our case, at the Ohio Supercomputer Center (OSC).\n\nTopics\n\nIntroduction to the Ohio Supercomputer Center (OSC)\nUsing the VS Code text editor at OSC\nIntroduction to the Unix shell (= the terminal / command line)\nBasics of shell scripts\nSoftware at OSC with modules & Conda\nSubmitting your scripts using the SLURM scheduler\nPutting it all together: practical examples of running analysis jobs at OSC\n\nThe modules will be a mixture of lectures that include “participatory live-coding” (with the instructor slowly demonstrating and participants expected to follow along for themselves) and exercises.\n\n\nSome more background\nCommand-line programs are preferred for many of the steps to analyze genomic sequencing data, such as those involving quality control, trimming or adapter removal, and assembly or mapping. Other features of such datasets are that they tend to contain a lot of data, and that many analysis steps can be done independently for each sample. It therefore pays off -or may be necessary- to run your analyses not on a laptop or desktop, but at a supercomputer like OSC.\nBeing able to run your analysis with command-line programs at OSC involves a number of skills that may seem overwhelming at first. Fortunately, learning the basics of these skills does not take a lot of time, and will enable you to be up-and-running with working on your own genomic data! Keep in mind that these days, excellent programs are available for almost any genomics analysis, so you do not need to be able to code it all up from scratch. You will just need to know how to efficiently run such programs, which is what this workshop aims to teach you."
  },
  {
    "objectID": "about.html#sign-up",
    "href": "about.html#sign-up",
    "title": "General Workshop Info & Signing Up",
    "section": "Sign up!",
    "text": "Sign up!\nTo apply to attend the workshop, please fill out the form below. There is no real selection procedure: we accept anyone who is at OSU/USDA and signs up before we have reached the maximum number of participants.\n\nLoading…"
  },
  {
    "objectID": "sessions/shell-scripting.html",
    "href": "sessions/shell-scripting.html",
    "title": "Shell Scripting",
    "section": "",
    "text": "Shell scripts (or to be slightly more species, Bash scripts) enable us to run sets of commands non-interactively. This is especially beneficial or necessary when a set of commands:\nScripts form the basis for analysis pipelines and if we code things cleverly, it should be straightforward to rerun much of our project workflow:"
  },
  {
    "objectID": "sessions/shell-scripting.html#setup",
    "href": "sessions/shell-scripting.html#setup",
    "title": "Shell Scripting",
    "section": "1 Setup",
    "text": "1 Setup\n\n\n\n\n\n\nStarting a VS Code session with an active terminal (click here)\n\n\n\n\n\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 3 in the box Number of hours\nEnter /fs/ess/scratch/PAS2250 in the box Working Directory\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\nOpen a terminal by clicking the “hamburger menu” way in the top left, and then Terminal > New Terminal.\nIn the terminal, type Bash and press Enter."
  },
  {
    "objectID": "sessions/shell-scripting.html#script-header-lines-and-zombie-scripts",
    "href": "sessions/shell-scripting.html#script-header-lines-and-zombie-scripts",
    "title": "Shell Scripting",
    "section": "2 Script header lines and zombie scripts",
    "text": "2 Script header lines and zombie scripts\n\n2.1 Shebang line\nUse a so-called “shebang” line as the first line of a script to indicate which language your script use. More specifically, this line tell the computer where to find the binary (executable) that will run your script.\nSuch a line starts with #!, basically marking it as a special type of comment. After that, we provide the location to the relevant program: in our case Bash, which is always located at /bin/bash on Linux and Mac computers.\n#!/bin/bash\nAdding a shebang line is good practice and is necessary when we want to submit our script to OSC’s SLURM queue, which we’ll do tomorrow.\nAnother line that is good practice to add to your Bash scripts changes some default settings to safer alternatives.\n\n\n2.2 Bash script settings\nTwo Bash default settings are bad ideas inside scripts.\nFirst, and as we’ve seen in the previous module, Bash does not complain when you reference a variable that does not exist (in other words, it does not consider that an error).\nIn scripts, this can lead to all sorts of downstream problems, because you probably tried to do something with an existing variable but made a typo. Even more problematically, it can lead to potentially very destructive file removal:\n\n# Using a variable, we try to remove some temporary files whose names start with tmp_\ntemp_prefix=\"temp_\"\nrm \"$tmp_prefix\"*     # DON'T TRY THIS!\n\n\n# Using a variable, we try to remove a temporary directory\ntempdir=output/tmp\nrm -rf $tmpdir/*      # DON'T TRY THIS!\n\n\n\n\n\n\n\nThe comments above specified the intent we had. What would have actually happened?\n\n\n\n\n\nIn the first example, rm \"$tmp_prefix\"* would have been interpreted as rm *, because the non-existent variable is simply ignored. Therefore, we would have removed all files in the current working directory.\nAlong similar lines, in the second example, rm -rf $tmpdir/* would have been interpreted as rm -rf /*. Horrifyingly, this would attempt to remove the entire filesystem (recall that a leading / in a path is a computer’s root directory).1\n\n\n\n\nSecond, a Bash script keeps running after encountering errors. That is, if an error is encountered when running line 2 of a script, any remaining lines in the script will nevertheless be executed.\nIn the best case, this is a waste of computer resources, and in worse cases, it can lead to all kinds of unintended consequences. Additionally, if your script prints a lot of output, and much of it appears to be okay, you might not notice an error somewhere in the middle; but this error which might still have led to completely wrong results downstream.\n\nThe following three settings will make your Bash scripts more robust and safer. With these settings, the script terminates, with an appropriate error message, if:\n\nset -u — An unset variable is referenced.\nset -e — Almost any error occurs.\nset -o pipefail — An error occurs in a shell “pipeline” (e.g., sort | uniq).\n\nWe can change all of these settings in one line in a script:\n\nset -u -e -o pipefail     # (Don't run in the terminal)\n\nOr even more concisely:\n\nset -ueo pipefail         # (Don't run in the terminal)\n\n\n\n2.3 Our header lines as a rudimentary script\nLet’s go ahead and start a script with the header lines that we have so far discussed.\n\nOpen a new file in the VS Code editor (     =>   File   =>   New File) and save it as printname.sh (shell scripts, including Bash scripts, most commonly have the extension .sh).\nType the following lines in that script (please actually type instead of copy-pasting):\n\n\n#!/bin/bash\nset -ueo pipefail\n\n# (Note: this is a partial script. Don't enter this directly in your terminal.)\n\nAlready now, we could run (execute) the script. One way of doing this is calling the bash command followed by the name of the script2:\n\nbash printname.sh\n\nDoing this won’t print anything to screen (or file). This makes sense because our script doesn’t have any output, and as we’ve seen before with Bash, no output can be a good sign because it means that no errors were encountered."
  },
  {
    "objectID": "sessions/shell-scripting.html#command-line-arguments-for-scripts",
    "href": "sessions/shell-scripting.html#command-line-arguments-for-scripts",
    "title": "Shell Scripting",
    "section": "3 Command-line arguments for scripts",
    "text": "3 Command-line arguments for scripts\n\n3.1 Calling a script with arguments\nWhen you call a script, you can pass it command-line arguments, such as a file to operate on.\nThis is much like when you provide commands like ls with arguments:\n\n# Run ls without arguments:\nls\n\n# Pass 1 filename as an argument to ls:\nls data/sampleA.fastq.gz\n\n# Pass 2 filenames as arguments to ls, separated by spaces:\nls data/sampleA.fastq.gz data/sampleB.fastq.gz\n\nLet’s see the same thing with our printname.sh script and a fictional script fastqc.sh (which would probably run the FastQC program – we’ll make such a script later):\n\n# Run scripts without any arguments:\nbash fastqc.sh                          # (Fictional script)\nbash printname.sh\n\n# Run scripts with 1 or 2 arguments:\nbash fastqc.sh data/sampleA.fastq.gz    # 1 argument, a filename\nbash printname.sh John Doe              # 2 arguments, strings representing names\n\nIn the next section, we’ll see what happens when we pass arguments on the command line (in short: command-line arguments) to a script.\n\n\n3.2 Placeholder variables\nInside the script, any command-line arguments are automatically assigned to placeholder variables.\nA first argument will be assigned to the variable $1, any second argument will be assigned to $2, any third argument will be assigned to $3, and so on.\n\n\n\n\n\n\nIn the calls to fastqc.sh and printname.sh above, which placeholder variables were created and what were there values?\n\n\n\n\n\nIn bash fastqc.sh data/sampleA.fastq.gz, a single argument, data/sampleA.fastq.gz, is passed to the script, and will be assigned to $1.\nIn bash printname.sh John Doe, two arguments are passed to the script: the first one (John) will be stored in $1, and the second one (Doe) in $2.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nArguments passed to a script are only assigned to placeholder varaibles; unless we explicitly include code in the script to do something with those variables, nothing else happens.\n\n\nLet’s add code to our script to “process” any first and last name that are passed to the script as command-line arguments. First, our small script will simply echo the placeholder variables, so that we can see what happens. We’ll add two echo commands to our printname.sh script, such that the script reads:\n\n#!/bin/bash\nset -ueo pipefail\n\necho \"First name: $1\"\necho \"Last name: $2\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\n\n\n\nNext, we’ll run the script, passing the arguments John and Doe:\n\nbash printname.sh John Doe\n\nFirst name: John\nLast name: Doe\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nIn each case below, think about what might happen before you run the script. If you didn’t make a successful predictions, try to figure out what happened instead.\n\nRun the script without passing arguments to it.\nDeactivate (“comment out”) the line with set settings by inserting a # as the first character. Then, run the script again without passing arguments to it.\nDouble-quote John Doe when you run the script, i.e. run bash printname.sh \"John Doe\"\nRemove the # you inserted in the script in step 2 above.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nThe script will error out because we are referencing variables that don’t exist: since we didn’t pass command-line arguments to the script, the $1 and $2 have not been set.\n\n\nbash printname.sh\n\n\nprintname.sh: line 4: $1: unbound variable\n\n\nThe script will run in its entirety and not throw any errors, because we are now using default Bash settings such that referencing non-existent variables does not throw an error. Of course, no names are printed either, since we didn’t specify any:\n\n\n\n\n\nbash printname.sh\n\nFirst name: \nLast name: \n\n\nThe set line should read:\n\n#set -ueo pipefail\n\n\nBecause we are quoting \"John Doe\", both names are passed as a single argument and both names end up in $1, the “first name”:\n\n\nbash printname.sh \"John Doe\"\n\nFirst name: John Doe\nLast name: \n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3 Descriptive variable names\nWhile you can use the $1-style placeholder variables throughout your script, I find it very useful to copy them to more descriptively named variables as follows:\n\n#!/bin/bash\nset -ueo pipefail\n\nfirst_name=$1\nlast_name=$2\n  \necho \"First name: $first_name\"\necho \"Last name: $last_name\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\nUsing descriptively named variables in your scripts has several advantages. It will make your script easier to understand for others and for yourself. It will also make it less likely that you make errors in your script in which you use the wrong variable in the wrong place.\n\n\n\n\n\n\nOther variables that are automatically available inside scripts\n\n\n\n\n$0 contains the script’s file name\n$# contains the number of command-line arguments passed\n\n\n\n\n\n\n\n\n\nExercise: a script to print a specific line\n\n\n\n\n\nWrite a script that prints a specific line (identified by line number) from a file.\n\nSave the script as line.sh\nStart with the shebang and set lines\nYour script takes two arguments: a file name ($1) and a line number ($2)\nCopy the $1 and $2 variables to descriptively named variables\nTo print a specific line, think how you might combine head and tail to do this. If you’re at a loss, feel free to check out the solution below.\nTest the script by printing line 4 from samples.txt.\n\n\n\n\n\n\n\nSolution: how to print a specific line number\n\n\n\n\n\nFor example, to print line 37 of samples.txt directly:\n\nhead -n 37 samples.txt | tail -n 1\n\nIn the script, you’ll have to use variables instead of 37 and samples.txt.\nHow this command works:\n\nhead -n 37 samples.txt will print the first 37 lines of samples.txt\nWe pipe those 37 lines into the tail command\nWe ask tail to just print the last line of its input, which will in this case be line 37 of the original input file.\n\n\n\n\n\n\n\n\n\n\nFull solution\n\n\n\n\n\n\n#!/bin/bash\nset -ueo pipefail\n  \ninput_file=$1\nline_nr=$2\n\nhead -n \"$line_nr\" \"$input_file\" | tail -n 1\n\nTo run the script and make it print the 4th line of samples.txt:\n\nbash line.sh samples.txt 4"
  },
  {
    "objectID": "sessions/shell-scripting.html#touching-up-scripts",
    "href": "sessions/shell-scripting.html#touching-up-scripts",
    "title": "Shell Scripting",
    "section": "4 Touching up scripts",
    "text": "4 Touching up scripts\n\n4.1 Report what’s happening\n \n\n\n\n\n\n\n\nKeyboard shortcut to run shell commands from the editor\n\n\n\nTo add a keyboard shortcut that will send code selected in the editor pane to the terminal (such that you don’t have to copy and paste):\n\nClick the      (bottom-left) => Keyboard Shortcuts.\nFind Terminal: Run Selected Text in Active Terminal, click on it, then add a shortcut, e.g. Ctrl+Enter."
  },
  {
    "objectID": "sessions/shell-scripting.html#touching-scripts-up",
    "href": "sessions/shell-scripting.html#touching-scripts-up",
    "title": "Shell Scripting",
    "section": "4 Touching scripts up",
    "text": "4 Touching scripts up\n\n4.1 Report what’s happening\nIt is often useful to have your scripts report what is going on. For instance, this can help with troubleshooting.2 Let’s try this for our line.sh script:\n\n#!/bin/bash\nset -ueo pipefail\n\necho \"Starting script $0\"\ndate # Print date & time to log & time running duration\n\nline_nr=\"$1\"\nfile=\"$2\"\n\necho \"Input file: $file\"\necho \"Requested line number: $line_nr\"\n\nhead -n \"$line_nr\" \"$file\" | tail -n 1\n\necho \"Done with script $0\"\ndate\n\n\n\n\n\nbash line.sh 2 shell-intro.qmd\n\nStarting script line.sh\nSun Aug 14 12:46:06 PM CEST 2022\nInput file: shell-intro.qmd\nRequested line number: 2\ntitle: \"The Unix Shell\"\nDone with script line.sh\n\n\n \n\n\n\n\n\n\n\nKeyboard shortcut to run shell commands from the editor\n\n\n\nTo add a keyboard shortcut that will send code selected in the editor pane to the terminal (such that you don’t have to copy and paste):\n\nClick the      (bottom-left) => Keyboard Shortcuts.\nFind Terminal: Run Selected Text in Active Terminal, click on it, then add a shortcut, e.g. Ctrl+Enter."
  },
  {
    "objectID": "sessions/shell-scripting.html#improving-our-scripts",
    "href": "sessions/shell-scripting.html#improving-our-scripts",
    "title": "Shell Scripting",
    "section": "4 Improving our scripts",
    "text": "4 Improving our scripts\n\n4.1 A script to serve as a starting point\nWe’ve learned that the head command prints the first lines of a file, whereas the tail command prints the last lines. Sometimes it’s nice to be able to quickly see both ends of a file, so let’s write a little script that can do that for us, as a starting point for the next few modifications.\n\n#!/bin/bash\nset -ueo pipefail\n\ninput_file=$1\n\nhead -n 2 \"$input_file\"\necho \"---\"\ntail -n 2 \"$input_file\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\nNext, let’s run the script:\n\nbash headtail.sh samples.txt\n\n\n\n4.2 Redirecting output to a file\nSo far, note that the output of our scripts was just printed to screen, e.g.:\n\nWith printnames.sh, we were simply echo-ing back the arguments passed to the script inside sentences.\nWith headtail.sh, we printed the first and last few lines of a file.\n\nAll this output was printed to screen because that is always the default output mode of Unix commands, and this works the same way regardless of whether those commands are run directly on the command line, or are run inside a script.\nAlong those same lines, we have already learned that we can “redirect” output to a file using > (write/overwrite) and >> (append) when we run shell commands &emdash; and this, too, works exactly the same inside a script.\n\n#!/bin/bash\nset -ueo pipefail\n\n\n\n4.3 Report what’s happening\nIt is often useful to have your scripts report what is going on. For instance, this can help with troubleshooting.3\nLet’s try this with our headtail.sh script. Such reporting is a bit more sensible when a script’s main output goes to a file rather than to scree\n\n#!/bin/bash\nset -ueo pipefail\n\necho \"Starting script $0\"\ndate # Print date & time to log & time running duration\n\nline_nr=\"$1\"\nfile=\"$2\"\n\necho \"Input file: $file\"\necho \"Requested line number: $line_nr\"\n\nhead -n \"$line_nr\" \"$file\" | tail -n 1\n\necho \"Done with script $0\"\ndate\n\n\n\n\n\nbash line.sh 2 printname.sh\n\nStarting script line.sh\nSun Aug 14 02:46:48 PM CEST 2022\nInput file: printname.sh\nRequested line number: 2\nset -ueo pipefail\nDone with script line.sh\n\n\n \n\n\n\n\n\n\n\nKeyboard shortcut to run shell commands from the editor\n\n\n\nTo add a keyboard shortcut that will send code selected in the editor pane to the terminal (such that you don’t have to copy and paste):\n\nClick the      (bottom-left) => Keyboard Shortcuts.\nFind Terminal: Run Selected Text in Active Terminal, click on it, then add a shortcut, e.g. Ctrl+Enter."
  },
  {
    "objectID": "sessions/shell-scripting.html#script-variations-and-improvements",
    "href": "sessions/shell-scripting.html#script-variations-and-improvements",
    "title": "Shell Scripting",
    "section": "4 Script variations and improvements",
    "text": "4 Script variations and improvements\n\n4.1 A script to serve as a starting point\nWe’ve learned that the head command prints the first lines of a file, whereas the tail command prints the last lines. Sometimes it’s nice to be able to quickly see both ends of a file, so let’s write a little script that can do that for us, as a starting point for the next few modifications.\nOpen a new file, save it as headtail.sh, and add the following code to it:\n\n#!/bin/bash\nset -ueo pipefail\n\ninput_file=$1\n\nhead -n 2 \"$input_file\"\necho \"---\"\ntail -n 2 \"$input_file\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\nNext, let’s run our headtail.sh script:\n\nbash headtail.sh samples.txt\n\n\n\n4.2 Redirecting output to a file\nSo far, the output of our scripts was printed to screen, e.g.:\n\nIn printnames.sh, we simply echo’d, inside sentences, the arguments passed to the script.\nIn headtail.sh, we printed the first and last few lines of a file.\n\nAll this output was printed to screen because that is the default output mode of Unix commands, and this works the same way regardless of whether those commands are run directly on the command line, or are run inside a script.\nAlong those same lines, we have already learned that we can “redirect” output to a file using > (write/overwrite) and >> (append) when we run shell commands — and this, too, works exactly the same way inside a script.\n\nWhen working with genomics data, we commonly have files as input, and new/modified files as output. So let’s practice with this and modify our headtail.sh script so that it writes output to a file.\nWe’ll make the following changes:\n\nWe will have the script accept a second argument: the output file name. Of course, we could also simply write the output to a predefined (“hardcoded”) file name such as out.txt, but in general, it’s better practice to keep this flexible via an argument.\nWe will redirect the output of our head, echo, and tail commands to the output file. We’ll have to append (>>) in the last two cases.\n\n\n#!/bin/bash\nset -ueo pipefail\n\ninput_file=$1\noutput_file=$2\n\nhead -n 2 \"$input_file\" > \"$output_file\"\necho \"---\" >> \"$output_file\"\ntail -n 2 \"$input_file\" >> \"$output_file\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\nNow we run the script again, this time also passing the name of an output file:\n\nbash headtail.sh samples.txt samples_headtail.txt\n\nThe script will no longer print any output to screen, and our output should instead be in samples_headtail.txt:\n\n# Check that the file exists and was just modified:\nls -lh samples_headtail.txt\n\n# Print the contents of the file to screen\ncat samples_headtail.txt\n\n\n\n4.3 Report what’s happening\nIt is often useful to have your scripts “report” or “log” what is going on. Let’s keep thinking about a script that has file(s) as the main output, but instead of having no output printed to screen at all, we’ll print some logging output to screen. For instance: what is the date and time, which arguments were passed to the script, what are the output files, and perhaps even summaries of the output. All of this can help with troubleshooting.3\nLet’s try this with our headtail.sh script.\n\n#!/bin/bash\nset -ueo pipefail\n\n## Process command-line arguments\ninput_file=$1\noutput_file=$2\n\n## Initial logging \necho \"Starting script $0\"           # Print name of script\ndate                                # Print date & time\necho \"Input file:   $input_file\"\necho \"Output file:  $output_file\" \necho                                # Print empty line to separate initial & final logging\n\n## Print the first and last two lines to a separate file\nhead -n 2 \"$input_file\" > \"$output_file\"\necho \"---\" >> \"$output_file\"\ntail -n 2 \"$input_file\" >> \"$output_file\"\n\n## Final logging\necho \"Listing the output file:\"\nls -lh \"$output_file\"\necho \"Done with script $0\"\ndate\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\nA couple of notes about the lines that were added to the script above:\n\nPrinting the date at the end of the script as well will allow you to check for how long the script ran, which can be informative for longer-running scripts.\nPrinting the input and output files (and the command-line arguments more generally) can be particularly useful for troubleshooting\nPrinting a “marker line” like Done with script, indicating that the end of the script was reached, is handy because due to our set settings, seeing this line printed means that no errors were encountered.\nBecause our script grew so much, I also added some comment headers like “Initial logging” to make the script easier to read, and such comments can be made more extensive to really explain what is being done.\n\n\n\n\nLet’s run the script again:\n\nbash headtail.sh printname.sh tmp.txt\n\nStarting script headtail.sh\nSun Aug 14 04:02:53 PM CEST 2022\nInput file:   printname.sh\nOutput file:  tmp.txt\n\nListing the output file:\n-rw-rw-r-- 1 jelmer jelmer 77 Aug 14 16:02 tmp.txt\nDone with script headtail.sh\nSun Aug 14 04:02:53 PM CEST 2022\n\n\nThe script printed some details for the output file, but not its contents (that would have worked here, but is usually not sensible when working with genomics data). Let’s take a look, though, to make sure the script worked:\n\ncat tmp.txt\n\n#!/bin/bash\nset -ueo pipefail\n---\necho \"First name: $1\"\necho \"Last name: $2\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe reporting (echo-ing) may have started to seem silly for our litle script, but fairly extensive reporting (as well as testing, which is outside the scope of this workshop) can be very useful — and will be eventually a time-saver.\nThis is especially true for long-running scripts, or scripts that you often reuse and perhaps share with others.\n\n\n \n\n\n\n\n\n\n\nKeyboard shortcut to run shell commands from the editor\n\n\n\nTo add a keyboard shortcut that will send code selected in the editor pane to the terminal (such that you don’t have to copy and paste):\n\nClick the      (bottom-left) => Keyboard Shortcuts.\nFind Terminal: Run Selected Text in Active Terminal, click on it, then add a shortcut, e.g. Ctrl+Enter."
  },
  {
    "objectID": "sessions/software.html",
    "href": "sessions/software.html",
    "title": "Using Software at OSC",
    "section": "",
    "text": "So far, we have only used commands that are available in any Unix shell. But to actually analyze genomics data sets, we also need to use specialized bioinformatics software.\nMost software that is already installed at OSC must nevertheless be “loaded” (“activated”) before we can use it; and if our software of choice is not installed, we have to do so ourselves. We will cover those topics in this module."
  },
  {
    "objectID": "sessions/software.html#running-command-line-programs",
    "href": "sessions/software.html#running-command-line-programs",
    "title": "Using Software at OSC",
    "section": "2 Running command-line programs",
    "text": "2 Running command-line programs\nAs pointed out in the introduction to the workshop, bioinformatics software (programs) that we use to analyze genomic data are typically run from the command line. That is, they have “command-line interfaces” (CLIs) rather than “graphical user interfaces” (GUIs), and are run using commands that are structurally very similar to how we’ve been using basic Unix commands.\nFor instance, we can run the program FastQC as follows, instructing it to process the FASTQ file sampleA.fastq.gz with default options:\n\nfastqc sampleA.fastq.gz\n\nSo, what we have learned in the previous modules can easily be applied to run command-line programs. But, we first need to load and/or install these programs.\n\n\n\n\n\n\nRunning inside a script or interactively\n\n\n\nLike any other command, we could in principle run the line of code above either in our interactive shell or from inside a script. In practice, it is better to do this in a script, especially at OSC, because:\n\nSuch programs typically take a while to run\nWe are not supposed to run processes that use significant resources on login nodes\nWe can run the same script simultaneously for different input files."
  },
  {
    "objectID": "sessions/software.html#software-at-osc-with-lmod-software-modules",
    "href": "sessions/software.html#software-at-osc-with-lmod-software-modules",
    "title": "Using Software at OSC",
    "section": "3 Software at OSC with Lmod (software modules)",
    "text": "3 Software at OSC with Lmod (software modules)\nOSC administrators manage software with the Lmod system. For us users, this means that while a lot of software has already been installed, most are only available to use after we explicitly load them. On the upside, this practice enables the use of different versions of the same software, and of mutually incompatible software on a single system.\nFor a list of software that has been installed at OSC, see:\nhttps://www.osc.edu/resources/available_software/software_list.\nYou can also search for available software (“modules”) in the shell:\n\nmodule spider lists modules that are installed.\nmodule avail lists modules that can be directly loaded, given the current environment (i.e., depending on which other software has been loaded.)\n\nSimply running module spider or module avail would spit out complete lists, but more usefully we can provide search terms as arguments to these commands:\n\nmodule spider python\n#>    Versions:\n#>      python/2.7-conda5.2\n#>      python/3.6-conda5.2\n#>      python/3.7-2019.10\n\nmodule avail python\n#> python/2.7-conda5.2  python/3.6-conda5.2 (D)  python/3.7-2019.10\n\nAll Lmod software functionality is accessed using module subcommands. For instance, we can use module load and module unload to, you guessed it, load and unload software:\n\n# Load a module:\nmodule load python              # Load default version\nmodule load python/3.7-2019.10  # Load a specific version\n\n# Unload a module:\nmodule unload python\n\nTo check which modules have been loaded (the list will include some modules that have been loaded automatically):\n\nmodule list"
  },
  {
    "objectID": "sessions/software.html#options-when-software-isnt-installed-at-osc",
    "href": "sessions/software.html#options-when-software-isnt-installed-at-osc",
    "title": "Using Software at OSC",
    "section": "4 Options when software isn’t installed at OSC",
    "text": "4 Options when software isn’t installed at OSC\nIt’s not too uncommon that software you need for your project is not installed at OSC, or that you need a more recent version of the software than those available.\nThe main alternatives available to you in such a case are to:\n\n“Manually” install the software yourself, which in the best case involves downloading a directly functioning binary (executable), but more commonly requires you to “compile” (build) the program.\nThis is sometimes straightforward but can also become extremely tricky, especially at OSC where you don’t have “administrator privileges”1 at OSC and will often have difficulties with “dependencies”2.\nSend an email to oschelp@osc.edu. They might be able to help you with your installation, or in case of commonly used software, might be willing to perform a system-wide installation (that is, making it available through module).\nUse the software environment manager conda, which creates environments with one or more software packages that are activated like in the module system.\nUse Apptainer / Singularity “containers”. Containers are like mini virtual machines and create isolated environments that can be exported and used anywhere, but require a bit more expertise and code to use than conda.\n\nThe latter two alternatives are useful not only at OSC, where they bypass issues with dependencies and administrator privileges, but more generally, for reproducible and portable environments.\nWith these, you can easily maintain distinct “environments” each with a different version of the same software, or with mutually incompatible software.\nBenefits:\n\n\n\n\n\n\nWhen to use containers instead of Conda\n\n\n\n\nCreate an isolated environment that can be exported and used anywhere.\nAllow you to run software that requires a different OS / OS version.\n\n\n\n\nNot much of a learning curve.\nLots of bioinformatics software is available as a Conda package."
  },
  {
    "objectID": "sessions/software.html#using-conda",
    "href": "sessions/software.html#using-conda",
    "title": "Using Software at OSC",
    "section": "5 Using conda",
    "text": "5 Using conda\nConda creates so-called environments in which you can install one or more software packages. As mentioned above, these environments are activated and deactivated in a similar manner as with the Lmod system – but the key difference is that we can create and manage these environments ourselves.\n\n\n\n\n\n\nWhat’s in an environment?\n\n\n\nOne environment per software, or one per project\nNote that even when you install a single program, many things are usually installed: dependencies\n\n\n\n5.1 Loading the (mini)conda module\nWhile it is also fairly straightforward to install conda for yourself 3, we will use OSC’s system-wide installation of conda in this workshop. Therefore, we first need to use a module load command to make it available:\n\n# (The most common installation of conda is actually called \"miniconda\")\nmodule load miniconda3\n\n\n\n5.2 One-time conda configuration\nWe will also do some one-time configuration, which will set the conda “channels” (basically, software repositories) that we want to use when we install software. This config also includes setting relative priorities among the channels, since one software package may be available from multiple channels.\nLike with module commands, conda commands consist of two parts, the conda command itself and a subcommand, such as config:\n\nconda config --add channels defaults     # Added first => lowest priority\nconda config --add channels bioconda\nconda config --add channels conda-forge  # Added last => highest priority\n\nLet’s check whether this configuration step worked:\n\nconda config --get channels\n\n\n\n5.3 Creating an environment for cutadapt\nTo practice using conda, we will now create a conda environment with the program cutadapt installed.\ncutadapt is a commonly used program to remove adapters or primers from sequence reads in FASTQ files; in particular, it is ubiquitous for primer removal in (e.g. 16S rRNA) microbiome metabarcoding studies. But there is no Lmod module on OSC for it, so if we want to use, our best option is to resort to conda.\nHere is the command to create a new environment and install cutadapt into that environment:\n\nconda create -y -n cutadapt -c bioconda cutadapt\n\nLet’s break the above command down:\n\ncreate is the conda subcommand to create a new environment.\n-y is a flag that prevents us from being asked to confirm installation.\nFollowing the -n option, we can specify the name of the environment, so -n cutadapt means that we want our environment to be called cutadapt. We can use whatever name we like for the environment, but of course a descriptive yet concise name is a good idea. Since we are making a single-program environment, it makes sense to simply name it after the program.\nFollowing the -c option, we can specify a channel from which we want to install, so -c bioconda indicates we want to use the bioconda channel. (Given that we’ve done some config above, this is not always necessary, but it can be good to be explicit.)\nThe cutadapt at the end of the line simply tells conda to install the package of that name. This is a “positional” argument to the command (note that there’s no option like -s before it): we put any software package(s) we want to install at the end of the command.\n\n\n\n\n\n\n\nSpecifying a version\n\n\n\nIf we want to be explicit about the version we want to install, we can add the version after = following the package name. We do that below, and we also include the version in the environment name:\n\nconda create -y -n cutadapt-4.1 -c bioconda cutadapt=4.1\n\n Let’s run the command above and see if we can install cutadapt\n\n\n\n\n5.4 Creating an environment for any program\nMinor variations on the conda create command above can be used to install almost any program for which is conda package is available.\nHowever, you may be wondering how we would know:\n\nWhether the program is available and what its conda package’s name is\nWhich versions are available\nWhich conda channel we should use\n\nADD…\n\n\n5.5 Activating conda environments\nWhereas we use the term “load” for Lmod modules, we use “activate” to the same effect for conda environments.\nOddly enough, the most foolproof way to activate a conda environment is to use source activate rather than the expected conda activate — for instance:\n\nsource activate cutadapt-4.1\n\n\n(cutadapt-4.1) [jelmer@pitzer-login03 PAS2250]$\n\n\n\n\n\n\n\nEnvironment indicator\n\n\n\nWhen we have an active conda environment, its name is conveniently displayed in our prompt, as depicted above.\n\n\nAfter we have activated the cutadapt environment, we should be able to actually use the program. To test this, we’ll again simply run it with a --help option:\n\ncutadapt --help\n\n\n\n\n5.6 Lines to add to your scripts\nWhile you’ll typically want to do installation interactively and only need to do to it once (see note below), you should always include the necessary code to load/activate your programs in your shell scripts.\nWhen your program is available as an Lmod module, this simply entails a module load call — e.g., for fastqc:\n\n#!/bin/bash\nset -ueo pipefail\n\nmodule load fastqc\n\nWhen your program is available in a conda environment, this entails a module load command to load conda itself, followed by a source activate command to load the relevant conda environment:\n\n#!/bin/bash\n\n## Load software\nmodule load miniconda3\nsource activate cutadapt-4.1\n\n## Strict/safe Bash settings \nset -ueo pipefail\n\n\n\n\n\n\n\nWarning\n\n\n\nWe’ve moved the set -ueo pipefail line below the source activate command, because the conda activation procedure may otherwise throw “unbound variable” errors.\n\n\n\n\n\n\n\n\nInstall once, load always\n\n\n\n\nProvided you don’t need to switch versions, you only need to install a program once. This is true also at OSC and also when using conda (your environments won’t disappear unless you delete them).\nIn every single “session” that you want to use a program via an Lmod module or conda environment, you …"
  },
  {
    "objectID": "sessions/software.html#setup",
    "href": "sessions/software.html#setup",
    "title": "Using Software at OSC",
    "section": "1 Setup",
    "text": "1 Setup\n\n\n\n\n\n\nStarting a VS Code session with an active terminal (click here)\n\n\n\n\n\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 3 in the box Number of hours\nEnter /fs/ess/scratch/PAS2250 in the box Working Directory\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\nOpen a terminal by clicking the “hamburger menu” way in the top left, and then Terminal > New Terminal.\nIn the terminal, type Bash and press Enter."
  },
  {
    "objectID": "sessions/software.html#software-at-osc-with-lmod",
    "href": "sessions/software.html#software-at-osc-with-lmod",
    "title": "Using Software at OSC",
    "section": "3 Software at OSC with Lmod",
    "text": "3 Software at OSC with Lmod\nOSC administrators manage software with the Lmod system of software modules. For us users, this means that even though a lot of software is installed, most of it can only be used after we explicitly load it.\n(That may seem like a drag, but on the upside, this practice enables the use of different versions of the same software, and of mutually incompatible software on a single system.)\n\n3.1 Checking for available software\nThe OSC website has a list of software that has been installed at OSC. You can also search for available software in the shell:\n\nmodule spider lists modules that are installed.\nmodule avail lists modules that can be directly loaded, given the current environment (i.e., depending on which other software has been loaded).\n\nSimply running module spider or module avail would spit out complete lists — more usefully, we can provide search terms as arguments to these commands:\n\nmodule spider python\n\n\n\n\n\npython:\n\n\n\n Versions:\n    python/2.7-conda5.2\n    python/3.6-conda5.2\n    python/3.7-2019.10\n\n\nmodule avail python\n\n\npython/2.7-conda5.2         python/3.6-conda5.2 (D)         python/3.7-2019.10\n\n\n\n\n\n\n\nTip\n\n\n\nThe (D) in the output above marks the default version of the program; that is, the version of the program that would be loaded if we don’t specify a version (see examples below).\n\n\n\n\n3.2 Loading software\nAll other Lmod software functionality is also accessed using module “subcommands” (we call module the command and e.g. spider the subcommand). For instance, to load and unload software:\n\n# Load a module:\nmodule load python              # Load the default version\nmodule load python/3.7-2019.10  # Load a specific version\n\n# Unload a module:\nmodule unload python\n\nTo check which modules have been loaded (the list will include modules that have been loaded automatically):\n\nmodule list\n\n\nCurrently Loaded Modules:\n    1) xalt/latest       2) gcc-compatibility/8.4.0       3) intel/19.0.5       4) mvapich2/2.3.3       5) modules/sp2020\n\n\n\n3.3 A practical example\nLet’s load a very commonly used bioinformatics program that we will also use in examples later on: FastQC. FastQC performs quality control (hence: “QC”) on FASTQ files.\nFirst, let’s test that we indeed cannot currently use fastqc by running fastqc with the --help flag:\n\nfastqc --help\n\n\nbash: fastqc: command not found\n\n\n\n\n\n\n\nHelp!\n\n\n\nA solid majority of command-line programs can be run with with a --help (and/or -h) flag, and this is perfect to try first, since it will tell use whether we can use the program, and if we can, we immediately get some usage information.\n\n\nNext, let’s check whether it is available at OSC, and if so, in which versions:\n\nmodule avail fastqc\n\n\nfastqc/0.11.8\n\nThere is only one version available (0.11.8), which means that module load fastqc and module load fastqc/0.11.8 would each load that same version.\n\n\n\n\n\n\nWhat might still be an argument to specify the version when we load the software?\n\n\n\n\n\nWhen we use it inside a script:\n\nThis would ensure that when we run the same script a year later, the same version would be used (assuming it hasn’t been removed) — otherwise, it’s possible a newer version would has been installed in the meantime, which might produce different results.\nIt will make it easy to see which version we used, which is something we typically report in papers.\n\n\n\n\nLet’s load the FastQC module:\n\nmodule load fastqc/0.11.8\n\nAfter we have loaded the module, we can retry our --help attempt:\n\nfastqc --help\n\n\n        FastQC - A high throughput sequence QC analysis tool\nSYNOPSIS\n    fastqc seqfile1 seqfile2 .. seqfileN\n\nfastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] \n       [-c contaminant file] seqfile1 .. seqfileN\n       \n[…and much more]"
  },
  {
    "objectID": "sessions/software.html#when-software-isnt-installed-at-osc",
    "href": "sessions/software.html#when-software-isnt-installed-at-osc",
    "title": "Using Software at OSC",
    "section": "4 When software isn’t installed at OSC",
    "text": "4 When software isn’t installed at OSC\nIt’s not too uncommon that software you need for your project is not installed at OSC, or that you need a more recent version of the software than those available. The main options available to you in such a case are to:\n\n“Manually” install the software, which in the best case involves downloading a directly functioning binary (executable), but more commonly requires you to “compile” (build) the program. This is sometimes straightforward but can also become extremely tricky, especially at OSC where you don’t have “administrator privileges”1 at OSC and will often have difficulties with “dependencies”2.\nSend an email to OSC Help. They might be able to help you with your installation, or in case of commonly used software, might be willing to perform a system-wide installation (that is, making it available through module).\nUse conda, which creates software environments that are activated like in the module system.\nUse Apptainer / Singularity “containers”. Containers are software environments that are more self-contained, akin to mini virtual machines.\n\nConda and containers are useful not only at OSC, where they bypass issues with dependencies and administrator privileges, but more generally, for reproducible and portable software environments. They also allow you to easily maintain distinct “environments”, each with a different version of the same software, or with mutually incompatible software.\nWe will teach conda here because it is easier to learn and use than containers, and because nearly all open-source bioinformatics software is available as a conda package.\n\n\n\n\n\n\nWhen to use containers instead of Conda\n\n\n\n\nIf you need to use software that requires a different Operating System (OS) or OS version than the one at OSC.\nIf you want or require even greater reproducibility and portability to create an isolated environment that can be exported and used anywhere."
  },
  {
    "objectID": "sessions/software.html#lines-to-add-to-your-scripts",
    "href": "sessions/software.html#lines-to-add-to-your-scripts",
    "title": "Using Software at OSC",
    "section": "6 Lines to add to your scripts",
    "text": "6 Lines to add to your scripts\nWhile you’ll typically want to do installation interactively and only need to do to it once (see note below), you should always include the necessary code to load/activate your programs in your shell scripts.\nWhen your program is available as an Lmod module, this simply entails a module load call — e.g., for fastqc:\n\n#!/bin/bash\nset -ueo pipefail\n\nmodule load fastqc\n\nWhen your program is available in a conda environment, this entails a module load call to load conda followed by a source activate\n\n\n\n\n\n\nInstall once, load always\n\n\n\n\nProvided you don’t need to switch versions, you only need to install a program once. This is true also at OSC and also when using conda (your environments won’t disappear unless you delete them).\nIn every single “session” that you want to use a program via an Lmod module or conda environment, you …"
  },
  {
    "objectID": "sessions/software.html#addendum-a-few-other-useful-conda-commands",
    "href": "sessions/software.html#addendum-a-few-other-useful-conda-commands",
    "title": "Using Software at OSC",
    "section": "6 Addendum: a few other useful conda commands",
    "text": "6 Addendum: a few other useful conda commands\n\nDeactivate the currently active conda environment:\n\nconda deactivate   \n\nActivate one environment and then “stack” an additional environment (a regular conda activate command would switch environments):\n\nsource activate cutadapt         # Now, the env \"cutadapt\" is active\nconda activate --stack multiqc   # Now, both \"cutadapt\" and \"multiqc\" are active\n\nRemove an environment entirely:\n\nconda env remove -n cutadapt\n\nList all your conda environments:\n\nconda env list\n\nList all packages (programs) installed in an environment:\n\nconda list -n cutadapt"
  },
  {
    "objectID": "sessions/slurm.html",
    "href": "sessions/slurm.html",
    "title": "Compute jobs with Slurm",
    "section": "",
    "text": "We have so far been working on login nodes at OSC, but in order to run some actual analyses, you will need access to compute nodes.\nAutomated scheduling software allows hundreds of people with different requirements to access compute nodes effectively and fairly. For this purpose, OSC uses the Slurm scheduler (Simple linux utility for resource management).\nA temporary reservation of (parts of) a compute node is called a compute job. What are the options to start a compute job at OSC?\nWhen running command-line programs for genomics analyses, non-interactive jobs are the most useful and will be the focus of this module. We’ll also touch on interactive shell jobs, which can occasionally be handy and are requested and managed in a very similar way to non-interactive jobs."
  },
  {
    "objectID": "sessions/slurm.html#setup",
    "href": "sessions/slurm.html#setup",
    "title": "Compute jobs with Slurm",
    "section": "1 Setup",
    "text": "1 Setup\n\n\n\n\n\n\nStarting a VS Code session with an active terminal (click here)\n\n\n\n\n\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 3 in the box Number of hours\nEnter /fs/ess/scratch/PAS2250 in the box Working Directory\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\nOpen a terminal by clicking the “hamburger menu” way in the top left, and then Terminal > New Terminal.\nIn the terminal, type Bash and press Enter."
  },
  {
    "objectID": "sessions/slurm.html#interactive-shell-jobs",
    "href": "sessions/slurm.html#interactive-shell-jobs",
    "title": "Compute jobs with Slurm",
    "section": "2 Interactive shell jobs",
    "text": "2 Interactive shell jobs\nInteractive shell jobs will grant you interactive shell access on a compute node. Working in an interactive shell job is operationally identical to working on a login node as we’ve been doing so far, but the difference is that it’s now okay to use significant computing resources. (How much and for how long depends on what you reserve.)\n\n2.1 Using srun\nA couple of different commands can be used to start an interactive shell job. I prefer the general srun command1, which we can use with the --pty /bin/bash option to get an interactive Bash shell.\nHowever, if we run that command without additional options, we get an error:\n\nsrun --pty /bin/bash\n\n\nsrun: error: ERROR: Job invalid: Must specify account for job\nsrun: error: Unable to allocate resources: Unspecified error\n\nAs the error message Must specify account for job tries to tell us, we need to indicate which OSC project (or as SLURM puts it, “account”) we want to use for this compute job. This is because an OSC project always has to be charged for the computing resources used during a compute job.\nTo specify the project/account, we can use the --account= option followed by the project number:\n\nsrun --account=PAS2250 --pty /bin/bash\n\n\nsrun: job 12431932 queued and waiting for resources\nsrun: job 12431932 has been allocated resources\n[…regular login info, such as quota, not shown…]\n[jelmer@p0133 PAS2250]$\n\nThere we go! First we got some Slurm scheduling info:\n\nInitially, the job is “queued”: that is, waiting to start.\nVery soon (usually!), the job has been “allocated resources”: that is, computing resources such as a compute node were found and reserved for the job.\n\nThen:\n\nThe job starts and because we’ve reserved an interactive shell job, this means that a new Bash shell is initiated: for that reason, we get to see our regular login info once again.\nMost importantly, we are no longer on a login node but on a compute node, as our prompt hints at: we switched from something like [jelmer@pitzer-login04 PAS2250]$ to the [jelmer@p0133 PAS2250]$ shown above.\nNote also that the job has a number (above: job 12431932): every compute job has such a unique identifier among all jobs by all users at OSC, and we can use this number to monitor and manage it. All of us will therefore see a different job number pop up.\n\n\n\n\n\n\n\nOSC projects\n\n\n\nDuring this workshop, we can all use the project PAS2250, which is actually a project that OSC has freely given me to introduce people to working at OSC. The project will still be charged but the credits on it were freely awarded.\nTo work on your own research project at OSC, you will either have to get your own project (typically, PIs get one for their lab or for a specific research project) or you can become an MCIC member and use the MCIC project.\n\n\n\n\n2.2 Compute job options\nThe --account= option is just one of out of many options we can use when reserving a compute job, but is the only one that always has to be specified (including for non-interactive jobs and for Interactive Apps).\nDefaults exist for all other options, such as the amount of time (1 hour) and the number of cores (1). These options are all specified in the same way for interactive and non-interactive jobs, and we’ll dive into them below.\n\n\n\n\n\n\nTip\n\n\n\nMany SLURM options have a long format (--account=PAS2250) and a short format (-A PAS2250), which can generally be used interchangeably. For clarity, we’ll try to stick to long format options during this workshop."
  },
  {
    "objectID": "sessions/slurm.html#non-interactive-jobs",
    "href": "sessions/slurm.html#non-interactive-jobs",
    "title": "Compute jobs with Slurm",
    "section": "3 Non-interactive jobs",
    "text": "3 Non-interactive jobs\nWhen requesting non-interactive jobs, we are asking the Slurm scheduler to run a script on a compute node. For this reason, we can also refer to it as “submitting a script (to the queue)”.\nIn contrast to interactive shell jobs, we stay in our current shell on a login node when submitting a script, and cannot really interact with the process on the compute node, other than:\n\nOutput from the script that would normally be printed to screen ends up in a file.\nWe can do things like monitoring whether the job is still running and cancelling the job, which will revoke the compute node reservation and stop the ongoing process.\n\nWhereas we used Slurm’s srun command to start an interactive shell job, we use its sbatch command to submit a script.\nRecall from the Bash scripting module that we can run a Bash script as follows:\n\nbash printnames.sh Jane Doe\n\nThe above command will run the script on our current node, which is typically a login node. To instead submit the script to the Slurm queue, we simply replace bash by sbatch:\n\nsbatch printnames.sh Jane Doe\n\nOf course we always have to specify the OSC account when submitting a compute job.\na minimal functional sbatch call would be:\n\nsbatch --account=PAS2250 printnames.sh Jane Doe\n\n\n\n\n\n\n\nsbatch options vs. script arguments\n\n\n\n\nsbatch [sbatch-options] myscript.sh [script-arguments]\n\n\n# No options/arguments for either:\nsbatch printnames.sh\n\nsbatch printnames.sh Jane Doe\n\nsbatch --account=PAS2250 printnames.sh\n\nsbatch --account=PAS2250 printnames.sh Jane Doe\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe script that we submit can be in different languages but typically (and in the examples in this workshop), they are shell (Bash) scripts.\n\n\n\n\n\n\n\n\nTip\n\n\n\nBoth interactive and non-interactive jobs start in the directory that they were submitted from: that is, your working directory will remain the same."
  },
  {
    "objectID": "info/about.html",
    "href": "info/about.html",
    "title": "General Workshop Info & Signing Up",
    "section": "",
    "text": "This workshop is geared towards people who would like to get started with analyzing genomic datasets.\nIt will be taught in-person with video-linking at the Wooster and Columbus Ohio State campuses, and it is also possible to join online through Zoom. We will have an instructor at each campus: Jelmer Poelstra from the Molecular and Cellular Imaging Center (MCIC) at the Wooster campus, and Mike Sovic from the Center for Applied Plant Sciences (CAPS) at the Columbus campus.\nThe workshop will be highly hands-on and take place across three afternoons:\nWed, Aug 17 - Fri, Aug 19, 2022.\n\nAnyone affiliated with The Ohio State University or Wooster USDA can attend\nAttendance is free\nNo prior experience with coding or genomic data is required\nYou will need to bring a laptop and don’t need to install anything prior to or during the workshop\nWe will work with example genomics data but if you have any, you are also welcome to bring your own data.\n\nSee below for information about the contents of the workshop and to sign up.\nFor questions, please email Jelmer."
  },
  {
    "objectID": "info/about.html#contents-of-the-workshop",
    "href": "info/about.html#contents-of-the-workshop",
    "title": "General Workshop Info & Signing Up",
    "section": "Contents of the workshop",
    "text": "Contents of the workshop\nThe focus of the workshop is on building general skills for analyzing genomics data, such as RNAseq, metabarcoding, metagenomic shotgun sequencing, or whole-genome sequencing. These skills boil down to the ability to write small shell scripts that run command-line programs and submit these scripts to a compute cluster – in our case, at the Ohio Supercomputer Center (OSC).\n\nTopics\n\nIntroduction to the Ohio Supercomputer Center (OSC)\nUsing the VS Code text editor at OSC\nIntroduction to the Unix shell (= the terminal / command line)\nBasics of shell scripts\nSoftware at OSC with modules & Conda\nSubmitting your scripts using the SLURM scheduler\nPutting it all together: practical examples of running analysis jobs at OSC\n\nThe modules will be a mixture of lectures that include “participatory live-coding” (with the instructor slowly demonstrating and participants expected to follow along for themselves) and exercises.\n\n\nSome more background\nCommand-line programs are preferred for many of the steps to analyze genomic sequencing data, such as those involving quality control, trimming or adapter removal, and assembly or mapping. Other features of such datasets are that they tend to contain a lot of data, and that many analysis steps can be done independently for each sample. It therefore pays off -or may be necessary- to run your analyses not on a laptop or desktop, but at a supercomputer like OSC.\nBeing able to run your analysis with command-line programs at OSC involves a number of skills that may seem overwhelming at first. Fortunately, learning the basics of these skills does not take a lot of time, and will enable you to be up-and-running with working on your own genomic data! Keep in mind that these days, excellent programs are available for almost any genomics analysis, so you do not need to be able to code it all up from scratch. You will just need to know how to efficiently run such programs, which is what this workshop aims to teach you."
  },
  {
    "objectID": "info/about.html#sign-up",
    "href": "info/about.html#sign-up",
    "title": "General Workshop Info & Signing Up",
    "section": "Sign up!",
    "text": "Sign up!\nTo apply to attend the workshop, please fill out the form below. There is no real selection procedure: we accept anyone who is at OSU/USDA and signs up before we have reached the maximum number of participants.\n\nLoading…"
  },
  {
    "objectID": "sessions/05-vars-loops.html",
    "href": "sessions/05-vars-loops.html",
    "title": "Variables and Loops",
    "section": "",
    "text": "In this module, we will cover a couple of topics that are good to know about before you start writing and running shell scripts:"
  },
  {
    "objectID": "sessions/05-vars-loops.html#setup",
    "href": "sessions/05-vars-loops.html#setup",
    "title": "Variables and Loops",
    "section": "1 Setup",
    "text": "1 Setup\n\n\n\n\n\n\nStarting a VS Code session with an active terminal (click here)\n\n\n\n\n\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 3 in the box Number of hours\nEnter /fs/ess/scratch/PAS2250 in the box Working Directory\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\nOpen a terminal by clicking the “hamburger menu” way in the top left, and then Terminal > New Terminal.\nIn the terminal, type Bash and press Enter."
  },
  {
    "objectID": "sessions/05-vars-loops.html#variables",
    "href": "sessions/05-vars-loops.html#variables",
    "title": "Variables and Loops",
    "section": "2 Variables",
    "text": "2 Variables\nIn programming, we use variables for things that:\n\nWe refer to repeatedly and/or\nAre subject to change.\n\nThese tend to be settings like the paths to input and output files, and parameter values for programs.\n\n2.1 Assigning and referencing variables\nTo assign a value to a variable in Bash (in short: to assign a variable), use the syntax variable=value. For example:\n\n# Assign the value \"low\" to the variable \"treatment\":\ntreatment=low\n\n# Assign the value \"200\" to the variable \"nlines\":\nnlines=200\n\n\n\n\n\n\n\nSpace-sensitive\n\n\n\nBe aware that there can be no spaces around the equals sign (=)!\n\n\nTo reference a variable (i.e., to access its value), you need to put a dollar sign $ in front of its name. We’ll use the echo command to review the values that our variables contain:\n\necho $treatment\n\n\n\nlow\n\n\n\necho $nlines\n\n\n\n200\n\n\nConveniently, we can directly use variables in lots of contexts, as if we had instead typed their values:\n\nls_options=\"-lh\"\n\nls $ls_options\n\ntotal 120K\n-rw-rw-r-- 1 jelmer jelmer 2.9K Aug 11 10:01 01-intro.qmd\n-rw-rw-r-- 1 jelmer jelmer  248 Aug  9 21:42 02-osc.qmd\n-rw-rw-r-- 1 jelmer jelmer  11K Aug 11 10:43 03-vscode.qmd\n-rw-rw-r-- 1 jelmer jelmer  218 Aug  9 21:43 04-shell.qmd\n-rw-rw-r-- 1 jelmer jelmer  13K Aug 15 13:21 05-vars-loops.qmd\n-rw-rw-r-- 1 jelmer jelmer  13K Aug 15 13:21 05-vars-loops.rmarkdown\n-rw-rw-r-- 1 jelmer jelmer  21K Aug 14 16:02 06-scripts.qmd\n-rw-rw-r-- 1 jelmer jelmer  17K Aug 14 22:50 07-software.qmd\n-rw-rw-r-- 1 jelmer jelmer 7.9K Aug 15 11:43 08-slurm.qmd\n-rw-rw-r-- 1 jelmer jelmer  232 Aug  9 21:42 09-examples.qmd\n-rw-rw-r-- 1 jelmer jelmer    0 Aug 15 13:21 18.txt\n-rw-rw-r-- 1 jelmer jelmer    0 Aug 15 13:21 Aug\ndrwxrwxr-x 2 jelmer jelmer 4.0K Aug 10 15:09 img\n-rw-rw-r-- 1 jelmer jelmer    0 Aug 15 13:21 README_2022-08-15.txt\n-rw-rw-r-- 1 jelmer jelmer    0 Aug 15 13:21 README2_Thu,\n-rw-rw-r-- 1 jelmer jelmer    0 Aug 15 13:21 README3_Thu, Aug 18.txt\ndrwxrwxr-x 2 jelmer jelmer 4.0K Aug 14 15:16 sandbox\n\n\n\ninput_file=04-shell.qmd\n\nls -lh $input_file \n\n-rw-rw-r-- 1 jelmer jelmer 218 Aug  9 21:43 04-shell.qmd\n\n\n\n\n2.2 Rules for naming variables\nVariable names:\n\nCan contain letters, numbers, and underscores\nCannot contain spaces, periods, or other special symbols\nCannot start with a number\n\n\n\n2.3 Command substitution\nIf you want to store the result of a command in a variable, you can use a construct called “command substitution” by wrapping the command inside $():\n\n# (date +%F will return the date in YYYY-MM-DD format)\ntoday=$(date +%F)\n\n# Create a file with our $today variable:\ntouch README_$today.txt\n\n# Check the name of our newly created file:\nls README_*\n\nREADME_2022-08-15.txt\n\n\n\n# Define a filename that we'll use in the next few commands:\ninput_file=shell-scripting.qmd\n\n# `wc -l` will count the number of lines\n# Using `<` (input redirection) is a trick to avoid the filename from being printed \nnlines=$(wc -l < $input_file)\n\n# We can directly use the variables in our quoted echo statement:\necho \"The file $input_file has $nlines lines\"\n\nbash: line 6: shell-scripting.qmd: No such file or directory\nThe file shell-scripting.qmd has  lines\n\n\nCommand substitution can for instance be useful when you want your script to report some results, or when a next step in the script depends on a previous result.\n\n\n2.4 Environment variables\nThere are also predefined variables in the Unix shell: that is, variables that exist in your environment by default. These so-called “environment variables” are always spelled in all-caps:\n\n# Environment variable $USER contains your user name \necho $USER\n\njelmer\n\n\n\n# Environment variable $HOME contains the path to your home directory\necho $HOME\n\n/home/jelmer\n\n\nEnvironment variables can provide useful information. We’ll see them again when we talk about the SLURM compute job scheduler.\n\n\n2.5 Quoting variables\nAbove, we learned that a variable name cannot contain spaces. But what happens if our variable’s value contains spaces?\nFirst off, when we try to assign the variable without quotes, we get an error:\n\ntoday=Thu, Aug 18\n\n\nAug: command not found\n\nBut it works when we quote (with double quotes, \"...\") the entire string that makes up the value:\n\ntoday=\"Thu, Aug 18\"\necho $today\n\nThu, Aug 18\n\n\nNow, let’s try to reference this variable:\n\ntouch README2_$today.txt\nls README2_*\n\n\n\nREADME2_Thu,\n\n\n\n\n\n\n\n\nWhat went wrong here? How many files were created?\n\n\n\n\n\nThe shell performed so-called field splitting using a space as a separator, splitting the value into three separate units – as a result, three files were created: README2_Thu, (listed above), as well as Aug and 18.txt.\nThe following code will list all these three files:\n\n# `ls -t` will sort by last-modified date, and `head -n 3` prints the top 3\n# Therefore, this will print the last 3 files that were created/modified\nls -t | head -n 3\n\n18.txt\nAug\nREADME2_Thu,\n\n\n\n\n\nSimilar to what we had to do when assigning the variable, our problems can be avoided by quoting the variable when we reference it:\n\ntouch README3_\"$today\".txt\nls README3_*\n\n\n\nREADME3_Thu, Aug 18.txt\n\n\nAnother issue we can run into when we don’t quote variables is that we can’t explicitly define where a variable name ends within a longer string of text:\n\necho README_$today_final.txt\n\n\n\nREADME_.txt\n\n\n\n\n\n\n\n\nWhat went wrong here?\n\n\n\n\n\n\nFollowing a $, the shell will stop interpreting characters as being part of the variable name only when it encounters a character that cannot be part of a variable name, such as a space or a period.\nSince variable names can contain underscores, it will look for the variable $today_final, which does not exist.\nImportantly, the shell does not error out when you reference a non-existing variable – it basically ignores it, such that README_$today_final.txt becomes README_.txt, as if we hadn’t referenced any variable.\n\n\n\n\nQuoting solves this issue, too:\n\necho README_\"$today\"_final.txt\n\n\n\nREADME_Thu, Aug 18_final.txt\n\n\n\n\n\n\n\n\nMore on quoting – and double vs. single quotes\n\n\n\n\n\nBy double-quoting a variable, we are essentially escaping (or “turning off”) the default special meaning of the space as a separator, and are asking the shell to interpret it as a literal space.\nSimilarly, we are escaping other “special characters”, such as globbing wildcards, with double quotes. Compare:\n\necho *\n\n01-intro.qmd 02-osc.qmd 03-vscode.qmd 04-shell.qmd 05-vars-loops.qmd 05-vars-loops.rmarkdown 06-scripts.qmd 07-software.qmd 08-slurm.qmd 09-examples.qmd 18.txt Aug img README_2022-08-15.txt README2_Thu, README3_Thu, Aug 18.txt sandbox\n\n\n\necho \"*\"\n\n*\n\n\nHowever, as we also saw above, double quotes do not turn off the special meaning of $ (i.e., denoting a string as a variable):\n\necho \"$today\"\n\n\n\nThu, Aug 18\n\n\n…but single quotes will:\n\necho '$today'\n\n$today\n\n\n\n\n\nAll in all, it is good practice to quote variables when you reference them: it never hurts, and avoids unexpected surprises."
  },
  {
    "objectID": "sessions/05-vars-loops.html#for-loops",
    "href": "sessions/05-vars-loops.html#for-loops",
    "title": "Variables and Loops",
    "section": "3 For loops",
    "text": "3 For loops\nLoops are a universal element of programming languages, and are extremely useful to repeat operations, such as when you want to run the same script or command for multiple files.\nHere, we’ll only cover what is by far the most common type of loop: the for loop.\nfor loops iterate over a collection, such as a list of files: that is, they allow you to perform one or more actions for each element in the collection, one element at a time.\n\n3.1 for loop syntax and mechanics\nLet’s see a first example, where our collection is just a very short list of numbers (1, 2, and 3):\n\nfor a_number in 1 2 3; do\n    echo \"In this iteration of the loop, the number is $a_number\"\n    echo \"--------\"\ndone\n\nIn this iteration of the loop, the number is 1\n--------\nIn this iteration of the loop, the number is 2\n--------\nIn this iteration of the loop, the number is 3\n--------\n\n\nfor loops contain the following mandatory keywords:\n\n\n\n\n\n\n\nKeyword\nPurpose\n\n\n\n\nfor\nAfter for, we set the variable name\n\n\nin\nAfter in, we specify the collection we are looping over\n\n\ndo\nAfter do, we have one ore more lines specifying what to do with each item\n\n\ndone\nTells the shell we are done with the loop\n\n\n\n\n\n\n\n\n\nWhy the semicolon ; before do?\n\n\n\n\n\nA semicolon separates two commands written on a single line – for instance, instead of:\n\nmkdir results\ncd results\n\n…you could equivalently type:\n\nmkdir results; cd results\n\nThe ; in the for loop syntax has the same function, and as such, an alternative way to format a for loop is:\n\nfor a_number in 1 2 3\ndo\n    echo \"In this iteration of the loop, the number is $a_number\"\ndone\n\nBut that’s one line longer and a bit awkwardly asymmetric.\n\n\n\nIt is important to realize that the loop runs sequentially for each item in the collection, and will therefore run as many times as there are items in the collection.\nThe following example, where we let the computer sleep for 1 second before printing the date and time with the date command, demonstrates that the loop is being executed sequentially:\n\nfor a_number in 1 2 3; do\n    echo \"In this iteration of the loop, the number is $a_number\"\n    #sleep 1s          # Let the computer sleep for 1 second\n    date              # Print the date and time\n    echo \"--------\"\ndone\n\nIn this iteration of the loop, the number is 1\nMon Aug 15 01:21:58 PM CEST 2022\n--------\nIn this iteration of the loop, the number is 2\nMon Aug 15 01:21:58 PM CEST 2022\n--------\nIn this iteration of the loop, the number is 3\nMon Aug 15 01:21:58 PM CEST 2022\n--------\n\n\nThe aspect that is perhaps most difficult to understand is that in each iteration of the loop, one element in the collection (in the example above, either 1, 2, or 3) is being assigned to the variable specified after for (in the example above, a_number).\nWhen we specify the collection “manually”, like we did above with numbers, we separate the elements by a space, as this example also shows:\n\nfor mushroom in morel destroying_angel eyelash_cup; do\n    echo \"$mushroom is an Ohio mushroom\"\ndone\n\nmorel is an Ohio mushroom\ndestroying_angel is an Ohio mushroom\neyelash_cup is an Ohio mushroom\n\n\n\n\n3.2 Looping over files with globbing\nIn practice, we rarely manually list the collection of items we want to loop over. Instead, we commonly loop over files directly using globbing:\n\n# We make sure we only select gzipped FASTQ files using the `*fastq.gz` glob\nfor fastq_file in data/raw/*fastq.gz; do\n    echo \"File $fastq_file has $(wc -l < $fastq_file) lines.\"\n    # More processing...\ndone\n\nIf needed, you can use your globbing / wild card skills to narrow down the file selection:\n\n# Perhaps we only want to select R1 files (forward reads): \nfor fastq_file in data/raw/*R1*fastq.gz; do\n    # Some file processing...\ndone\n\n# Or only filenames starting with A or B:\nfor fastq_file in data/raw/[AB]*fastq.gz; do\n    # Some file processing...\ndone\n\n\n\n\n\n\n\nAlternatives to looping with a glob\n\n\n\n\n\nWith genomics data, the routine of looping over an entire directory of files, or selections made with simple globbing patterns, should serve you very well.\nBut in some cases, you may want to iterate only over a specific list of filenames (or partial filenames such as sample IDs) that represent a complex selection.\n\nIf this is a short list, you could directly specify it in the loop:\n\nfor sample in A1 B6 D3; do\n    R1=data/fastq/\"$sample\"_R1.fastq.gz\n    R2=data/fastq/\"$sample\"_R2.fastq.gz\n    # Some file processing...\ndone\n\nIf it is a longer list, you could create a simple text file with one line per sample ID / filename, and use command substitution as follows:\n\nfor fastq_file in $(cat file_of_filenames.txt); do\n    # Some file processing...\ndone\n\n\n(In cases like this, Bash arrays (basically, variables that consist of multiple values, like a vector in R) or while loops may provide more elegant solutions, but those are outside the scope of this introduction.)"
  },
  {
    "objectID": "sessions/03-vscode.html",
    "href": "sessions/03-vscode.html",
    "title": "The VS Code Text Editor",
    "section": "",
    "text": "In this module, we will learn the basics of a fancy text editor called Visual Studio Code (VS Code for short). Conveniently, we can use a version of this editor (sometimes referred to as Code Server) in our browser via the OSC OnDemand website.\nWe will use VS Code throughout the workshop as practically a one-stop solution for our computing activities at OSC: accessing the Unix shell and writing scripts. This is also how I use this editor in my daily work.\nTo emphasize the additional functionality relative to basic text editors like Notepad and TextEdit, editors like VS Code are also referred to as “IDEs”: Integrated Development Environments. If you’ve ever worked with R, the RStudio program is another good example of an IDE."
  },
  {
    "objectID": "sessions/03-vscode.html#starting-vs-code-at-osc",
    "href": "sessions/03-vscode.html#starting-vs-code-at-osc",
    "title": "The VS Code Text Editor",
    "section": "1 Starting VS Code at OSC",
    "text": "1 Starting VS Code at OSC\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nEnter 3 in the box Number of hours\nEnter /fs/ess/scratch/PAS2250 in the box Working Directory\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\n\n\n\n\n\n\n\n\n\n\nVS Code runs on a login node\n\n\n\nIn the previous module, we’ve learned that all serious computation at OSC should be done not on login nodes but on compute nodes.\nStarting an RStudio session, for instance, requires filling out a similar form, and RStudio will subsequently run on a compute node and your selected OSC project will be charged.\nRunning VS Code is a slightly peculiar case: we do have to fill out a form and reserve a pre-specified number of hours (the session will actually stop working after the allotted time has passed), but we’re on a login node and are not being charged."
  },
  {
    "objectID": "sessions/03-vscode.html#getting-started-with-vs-code",
    "href": "sessions/03-vscode.html#getting-started-with-vs-code",
    "title": "The VS Code Text Editor",
    "section": "2 Getting started with VS Code",
    "text": "2 Getting started with VS Code\n\n\n\n\n2.1 Side bars\nThe narrow side bar on the far left has:\n\nA      (“hamburger menu” icon) in the top, which has most of the standard menu items that you often find in a top bar, like File.\nA      (cog wheel icon) in the bottom, through which you can mainly access settings.\nA bunch of icons in the middle that serve to switch between different options for the wide side bar (to the right of the narrow side bar), which can show:\n\nExplorer: File browser (and, e.g., an outline for the active file)\nSearch: To search recursively across all files in the active folder\nSource Control: To work with version control systems like Git (not used in this workshop)\nRun and Debug: For debugging your code (not used in this workshop)\nExtensions: To install extensions (we’ll install one later)\n\n\n\n\n2.2 Editor pane and Welcome document\nThe main part of the VS Code is the editor pane. Whenever you open VS Code, a tab with a Welcome document is automatically opened. This provides some help for beginners, but also, for example, an overview of recently opened folders.\nWe can also use the Welcome document to open a new text file by clicking New file below Start (alternatively, click      =>   File   =>   New File). We’ll work with files starting tomorrow, but if you want, you could already start a file with notes on the workshop now.\n\n\n\n\n\n\nRe-open the Welcome document\n\n\n\nIf you’ve closed the Welcome document but want it back, click      =>   Help   =>   Welcome.\n\n\n\n\n2.3 Terminal\nBy default, no terminal is open in VS Code – to do so, click the      => Terminal => New Terminal.\nIn the terminal, the prompt says Singularity>. This is because in OSC OnDemand, VS Code runs inside a Singularity container (for our purposes, it is not important what that means, exactly). To break out of the Singularity shell and get a regular Bash Unix shell, type bash and press Enter.\nIn the next module, Mike will teach us how to use the terminal."
  },
  {
    "objectID": "sessions/03-vscode.html#some-tips-and-tricks",
    "href": "sessions/03-vscode.html#some-tips-and-tricks",
    "title": "The VS Code Text Editor",
    "section": "3 Some tips and tricks",
    "text": "3 Some tips and tricks\n\n3.1 Making use of your screen’s real estate\nSince we are using VS Code inside a browser window, we are unfortunately losing some screen space. Make sure to maximize the browser window and if you have a bookmarks bar, try to hide it (for Chrome: Ctrl/⌘+Shift+B).\n\n\n3.2 Resizing panes\nYou can resize panes (the terminal, editor, and wide sidebar) by hovering your cursor over the borders and then dragging it.\n\n\n3.3 The Command Palette\nTo access all the menu options that are available in VS Code, the so-called “Command Palette” can be handy, especially if you know what you are looking for.\nTo access the Command Palette, click      and then Command Palette or press F1 (or Ctrl/⌘+Shift+P).\nFor a quick test, open the Command Palette and start typing “color theme”, and you’ll see the relevant options pop up.\n\n\n3.4 Color themes\nTo try out different color themes for the entire program, click      and then Color Theme. (I like “Quiet Light”.)"
  },
  {
    "objectID": "sessions/03-vscode.html#working-directory",
    "href": "sessions/03-vscode.html#working-directory",
    "title": "The VS Code Text Editor",
    "section": "4 Working directory",
    "text": "4 Working directory\nSetting a “working directory” means that you designate a folder on a computer as the starting point for your operations.\n\n\n\n\n\n\nFolder vs. directory\n\n\n\n“Folder” and “directory” mean the same thing – the latter is most commonly used in the context of the Unix Shell.\n\n\nVS Code has a concept of a working directory that is effective in all parts of the program: in the file explorer in the side bar, in the terminal, and when saving or opening files in the editor.\nIn this workshop, we’ll exclusively work within the folder /fs/ess/scratch/PAS2250 (you’ll make personal folders within there shortly). By opening this folder beforehand (we did this in the form on the OnDemand site), we make sure that VS Code always takes this folder as a starting point, which will make navigation and saving files much easier.\n\n\n\n\n\n\nTaking off where you were\n\n\n\nAdditionally, when you reopen a folder later, VS Code will to some extent resume where you were before! It will reopen the text files that you had open and if you had an active terminal, it will also open a terminal. This is very convenient, especially when you start working on multiple projects (different folders) in VS Code and switch between those.\n\n\n\n\n\n\n\n\nSwitching folders\n\n\n\nTo switch to a different folder from within VS Code, click      =>   File   =>   Open Folder."
  },
  {
    "objectID": "sessions/03-vscode.html#addendum-keyboard-shortcuts",
    "href": "sessions/03-vscode.html#addendum-keyboard-shortcuts",
    "title": "The VS Code Text Editor",
    "section": "5 Addendum: keyboard shortcuts",
    "text": "5 Addendum: keyboard shortcuts\nWorking with keyboard shortcuts (also called “keybindings”) for common operations can be a lot faster than using your mouse. Below are some useful ones for VS Code (for Mac, replace Ctrl by ⌘).\n\n\n\n\n\n\nKeyboard shortcut cheatsheet\n\n\n\nFor a single-page PDF overview of keyboard shortcuts for your operating system:      =>   Help   =>   Keyboard Shortcut Reference. (Or for direct links to these PDFs: Windows / Mac / Linux.)\n\n\n\nToggle the wide side bar: Ctrl+B\nOpen a terminal: Ctrl+` or Ctrl+Shift+C.\nToggle between the terminal and the editor pane: Ctrl+` and Ctrl+1.\nLine actions:\n\nCtrl+X / C will cut/copy the entire line where the cursor is, when nothing is selected (!)\nCtrl+Shift+K will delete a line\nAlt+⬆/⬇ will move lines up or down.\n\nMultiple cursors: Press & hold Ctrl+Shift, then ⬆/⬇ arrows to add cursors upwards or downwards.\nToggle line comment (“comment out” code, and removing those comment signs): Ctrl+/\nSplit the editor window vertically: Ctrl+\\ (See also the options in      View => Editor Layout)\n\n\n\n\n\n\n\nBrowser interference\n\n\n\nUnfortunately, some VS Code and terminal keyboard shortcuts don’t work in this setting where we are using VS Code inside a browser, because existing browser keyboard shortcuts take precedence.\nIf you end up using VS Code a lot in your work, it is therefore worth switching to your own installation of the program (see At-home bonus: local installation)"
  },
  {
    "objectID": "sessions/03-vscode.html#at-home-bonus-local-installation",
    "href": "sessions/03-vscode.html#at-home-bonus-local-installation",
    "title": "The VS Code Text Editor",
    "section": "6 At-home bonus: local installation",
    "text": "6 At-home bonus: local installation\nAnother nice feature of VS Code is that it is freely available for all operating systems (and even though it is made by Microsoft, it is also open source).\nTherefore, if you like the program, you can also install it on your own computer and do your local text editing / script writing in the same environment at OSC (it is also easy to install on OSU-managed computers, because it is available in the Self Service software installer).\nEven better, the program can be “tunneled into” OSC, so that your working directory for the entire program can be at OSC rather than on your local computer. This gives the same experience as using VS Code through OSC OnDemand, except that you’re not working witin a browser window, which has some advantages (also: no need to fill out a form, or to break out of the Singularity shell).\nFor installation and SSH-tunneling setup, see this page - TBA."
  },
  {
    "objectID": "sessions/01-intro.html#what-you-will-and-wont-learn",
    "href": "sessions/01-intro.html#what-you-will-and-wont-learn",
    "title": "Introduction to the Workshop",
    "section": "What you will and won’t learn",
    "text": "What you will and won’t learn\nThe focus of the workshop is on building some general skills for analyzing genomics data.\nCommand-line programs are preferred for many of the steps to analyze genomic sequencing data. Because such datasets tend to contain a lot of data, it is also preferable to run your analyses not on a laptop or desktop, but at a compute cluster like the Ohio Supercomputer Center (OSC).\nThese realities mean that in genomics, you need the following set of skills that you may not have been thought during your biology education:\n\nHaving a basic understanding of a compute cluster (supercomputer)\n\nAnd being able to:\n\nUse the Unix shell (work in a terminal)\nWrite small shell scripts\nSubmit scripts to a “queue” and monitor and manage the resulting compute jobs\nActivate and probably install software in a Linux environment where you don’t have “admin rights”\n\nWe will teach the basics of these skills during this workshop!\nIt may be useful to point out that given this focus, we will not teach you much if anything about:\n\nDetails of genomic data file types\nDetails of specific (genomic) analyses\nMaking biological inferences from your data"
  },
  {
    "objectID": "sessions/01-intro.html#mechanics-of-a-hybrid-workshop",
    "href": "sessions/01-intro.html#mechanics-of-a-hybrid-workshop",
    "title": "Introduction to the Workshop",
    "section": "Mechanics of a hybrid workshop",
    "text": "Mechanics of a hybrid workshop\nWe have a slightly complicated set up with people in-person in Wooster with one instructor, in-person in Columbus with another instructor, and some people via Zoom. Some notes:\n\nThis website has all the material that we will go through during each of the modules! See the links in the schedule as well as under the top bar menus.\nIn-person participants don’t need to connect to the Zoom call (but are of course allowed to connect, if they can better see the instructor’s screen that way).\nBecause we’re not all on Zoom, we’ll use this Google Doc to share links, inpromptu code that is not on the website, and non-urgent questions.\nWhenever you have a question, please feel free to interrupt and speak up, both in-person and on Zoom. Only if your question is not urgent and you don’t want to interrupt the flow, put it in the Google Doc or ask about it during a break."
  },
  {
    "objectID": "sessions/01-intro.html#personal-introductions",
    "href": "sessions/01-intro.html#personal-introductions",
    "title": "Introduction to the Workshop",
    "section": "Personal introductions",
    "text": "Personal introductions\n\nInstructors\n\nJelmer Poelstra, Molecular and Cellular Imaging Center (MCIC), Wooster\nMike Sovic, Center for Applied Plant Sciences (CAPS), Wooster\n\n\n\nYou!\nPlease very briefly introduce yourself – include your position, department, and why you wanted to go to this workshop.\n\nAdd figure(s) showing data types & previous experience"
  },
  {
    "objectID": "sessions/02-osc.html#tba",
    "href": "sessions/02-osc.html#tba",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "1 TBA",
    "text": "1 TBA"
  },
  {
    "objectID": "sessions/04-shell.html#tba",
    "href": "sessions/04-shell.html#tba",
    "title": "The Unix Shell",
    "section": "1 TBA",
    "text": "1 TBA"
  },
  {
    "objectID": "sessions/06-scripts.html",
    "href": "sessions/06-scripts.html",
    "title": "Shell Scripting",
    "section": "",
    "text": "Shell scripts (or to be slightly more species, Bash scripts) enable us to run sets of commands non-interactively. This is especially beneficial or necessary when a set of commands:\nScripts form the basis for analysis pipelines and if we code things cleverly, it should be straightforward to rerun much of our project workflow:"
  },
  {
    "objectID": "sessions/06-scripts.html#setup",
    "href": "sessions/06-scripts.html#setup",
    "title": "Shell Scripting",
    "section": "1 Setup",
    "text": "1 Setup\n\n\n\n\n\n\nStarting a VS Code session with an active terminal (click here)\n\n\n\n\n\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 3 in the box Number of hours\nEnter /fs/ess/scratch/PAS2250 in the box Working Directory\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\nOpen a terminal by clicking the “hamburger menu” way in the top left, and then Terminal > New Terminal.\nIn the terminal, type Bash and press Enter."
  },
  {
    "objectID": "sessions/06-scripts.html#script-header-lines-and-zombie-scripts",
    "href": "sessions/06-scripts.html#script-header-lines-and-zombie-scripts",
    "title": "Shell Scripting",
    "section": "2 Script header lines and zombie scripts",
    "text": "2 Script header lines and zombie scripts\n\n2.1 Shebang line\nUse a so-called “shebang” line as the first line of a script to indicate which language your script use. More specifically, this line tell the computer where to find the binary (executable) that will run your script.\nSuch a line starts with #!, basically marking it as a special type of comment. After that, we provide the location to the relevant program: in our case Bash, which is always located at /bin/bash on Linux and Mac computers.\n#!/bin/bash\nAdding a shebang line is good practice and is necessary when we want to submit our script to OSC’s SLURM queue, which we’ll do tomorrow.\nAnother line that is good practice to add to your Bash scripts changes some default settings to safer alternatives.\n\n\n2.2 Bash script settings\nTwo Bash default settings are bad ideas inside scripts.\nFirst, and as we’ve seen in the previous module, Bash does not complain when you reference a variable that does not exist (in other words, it does not consider that an error).\nIn scripts, this can lead to all sorts of downstream problems, because you probably tried to do something with an existing variable but made a typo. Even more problematically, it can lead to potentially very destructive file removal:\n\n# Using a variable, we try to remove some temporary files whose names start with tmp_\ntemp_prefix=\"temp_\"\nrm \"$tmp_prefix\"*     # DON'T TRY THIS!\n\n\n# Using a variable, we try to remove a temporary directory\ntempdir=output/tmp\nrm -rf $tmpdir/*      # DON'T TRY THIS!\n\n\n\n\n\n\n\nThe comments above specified the intent we had. What would have actually happened?\n\n\n\n\n\nIn the first example, rm \"$tmp_prefix\"* would have been interpreted as rm *, because the non-existent variable is simply ignored. Therefore, we would have removed all files in the current working directory.\nAlong similar lines, in the second example, rm -rf $tmpdir/* would have been interpreted as rm -rf /*. Horrifyingly, this would attempt to remove the entire filesystem (recall that a leading / in a path is a computer’s root directory).1\n\n\n\n\nSecond, a Bash script keeps running after encountering errors. That is, if an error is encountered when running line 2 of a script, any remaining lines in the script will nevertheless be executed.\nIn the best case, this is a waste of computer resources, and in worse cases, it can lead to all kinds of unintended consequences. Additionally, if your script prints a lot of output, and much of it appears to be okay, you might not notice an error somewhere in the middle; but this error which might still have led to completely wrong results downstream.\n\nThe following three settings will make your Bash scripts more robust and safer. With these settings, the script terminates, with an appropriate error message, if:\n\nset -u — An unset variable is referenced.\nset -e — Almost any error occurs.\nset -o pipefail — An error occurs in a shell “pipeline” (e.g., sort | uniq).\n\nWe can change all of these settings in one line in a script:\n\nset -u -e -o pipefail     # (Don't run in the terminal)\n\nOr even more concisely:\n\nset -ueo pipefail         # (Don't run in the terminal)\n\n\n\n2.3 Our header lines as a rudimentary script\nLet’s go ahead and start a script with the header lines that we have so far discussed.\n\nOpen a new file in the VS Code editor (     =>   File   =>   New File) and save it as printname.sh (shell scripts, including Bash scripts, most commonly have the extension .sh).\nType the following lines in that script (please actually type instead of copy-pasting):\n\n\n#!/bin/bash\nset -ueo pipefail\n\n# (Note: this is a partial script. Don't enter this directly in your terminal.)\n\nAlready now, we could run (execute) the script. One way of doing this is calling the bash command followed by the name of the script2:\n\nbash printname.sh\n\nDoing this won’t print anything to screen (or file). This makes sense because our script doesn’t have any output, and as we’ve seen before with Bash, no output can be a good sign because it means that no errors were encountered."
  },
  {
    "objectID": "sessions/06-scripts.html#command-line-arguments-for-scripts",
    "href": "sessions/06-scripts.html#command-line-arguments-for-scripts",
    "title": "Shell Scripting",
    "section": "3 Command-line arguments for scripts",
    "text": "3 Command-line arguments for scripts\n\n3.1 Calling a script with arguments\nWhen you call a script, you can pass it command-line arguments, such as a file to operate on.\nThis is much like when you provide commands like ls with arguments:\n\n# Run ls without arguments:\nls\n\n# Pass 1 filename as an argument to ls:\nls data/sampleA.fastq.gz\n\n# Pass 2 filenames as arguments to ls, separated by spaces:\nls data/sampleA.fastq.gz data/sampleB.fastq.gz\n\nLet’s see the same thing with our printname.sh script and a fictional script fastqc.sh (which would probably run the FastQC program – we’ll make such a script later):\n\n# Run scripts without any arguments:\nbash fastqc.sh                          # (Fictional script)\nbash printname.sh\n\n# Run scripts with 1 or 2 arguments:\nbash fastqc.sh data/sampleA.fastq.gz    # 1 argument, a filename\nbash printname.sh John Doe              # 2 arguments, strings representing names\n\nIn the next section, we’ll see what happens when we pass arguments on the command line (in short: command-line arguments) to a script.\n\n\n3.2 Placeholder variables\nInside the script, any command-line arguments are automatically assigned to placeholder variables.\nA first argument will be assigned to the variable $1, any second argument will be assigned to $2, any third argument will be assigned to $3, and so on.\n\n\n\n\n\n\nIn the calls to fastqc.sh and printname.sh above, which placeholder variables were created and what were there values?\n\n\n\n\n\nIn bash fastqc.sh data/sampleA.fastq.gz, a single argument, data/sampleA.fastq.gz, is passed to the script, and will be assigned to $1.\nIn bash printname.sh John Doe, two arguments are passed to the script: the first one (John) will be stored in $1, and the second one (Doe) in $2.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nArguments passed to a script are only assigned to placeholder varaibles; unless we explicitly include code in the script to do something with those variables, nothing else happens.\n\n\nLet’s add code to our script to “process” any first and last name that are passed to the script as command-line arguments. First, our small script will simply echo the placeholder variables, so that we can see what happens. We’ll add two echo commands to our printname.sh script, such that the script reads:\n\n#!/bin/bash\nset -ueo pipefail\n\necho \"First name: $1\"\necho \"Last name: $2\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\n\n\n\nNext, we’ll run the script, passing the arguments John and Doe:\n\nbash printname.sh John Doe\n\nFirst name: John\nLast name: Doe\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nIn each case below, think about what might happen before you run the script. If you didn’t make a successful predictions, try to figure out what happened instead.\n\nRun the script without passing arguments to it.\nDeactivate (“comment out”) the line with set settings by inserting a # as the first character. Then, run the script again without passing arguments to it.\nDouble-quote John Doe when you run the script, i.e. run bash printname.sh \"John Doe\"\nRemove the # you inserted in the script in step 2 above.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nThe script will error out because we are referencing variables that don’t exist: since we didn’t pass command-line arguments to the script, the $1 and $2 have not been set.\n\n\nbash printname.sh\n\n\nprintname.sh: line 4: $1: unbound variable\n\n\nThe script will run in its entirety and not throw any errors, because we are now using default Bash settings such that referencing non-existent variables does not throw an error. Of course, no names are printed either, since we didn’t specify any:\n\n\n\n\n\nbash printname.sh\n\nFirst name: \nLast name: \n\n\nThe set line should read:\n\n#set -ueo pipefail\n\n\nBecause we are quoting \"John Doe\", both names are passed as a single argument and both names end up in $1, the “first name”:\n\n\nbash printname.sh \"John Doe\"\n\nFirst name: John Doe\nLast name: \n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3 Descriptive variable names\nWhile you can use the $1-style placeholder variables throughout your script, I find it very useful to copy them to more descriptively named variables as follows:\n\n#!/bin/bash\nset -ueo pipefail\n\nfirst_name=$1\nlast_name=$2\n  \necho \"First name: $first_name\"\necho \"Last name: $last_name\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\nUsing descriptively named variables in your scripts has several advantages. It will make your script easier to understand for others and for yourself. It will also make it less likely that you make errors in your script in which you use the wrong variable in the wrong place.\n\n\n\n\n\n\nOther variables that are automatically available inside scripts\n\n\n\n\n$0 contains the script’s file name\n$# contains the number of command-line arguments passed\n\n\n\n\n\n\n\n\n\nExercise: a script to print a specific line\n\n\n\n\n\nWrite a script that prints a specific line (identified by line number) from a file.\n\nSave the script as line.sh\nStart with the shebang and set lines\nYour script takes two arguments: a file name ($1) and a line number ($2)\nCopy the $1 and $2 variables to descriptively named variables\nTo print a specific line, think how you might combine head and tail to do this. If you’re at a loss, feel free to check out the solution below.\nTest the script by printing line 4 from samples.txt.\n\n\n\n\n\n\n\nSolution: how to print a specific line number\n\n\n\n\n\nFor example, to print line 37 of samples.txt directly:\n\nhead -n 37 samples.txt | tail -n 1\n\nIn the script, you’ll have to use variables instead of 37 and samples.txt.\nHow this command works:\n\nhead -n 37 samples.txt will print the first 37 lines of samples.txt\nWe pipe those 37 lines into the tail command\nWe ask tail to just print the last line of its input, which will in this case be line 37 of the original input file.\n\n\n\n\n\n\n\n\n\n\nFull solution\n\n\n\n\n\n\n#!/bin/bash\nset -ueo pipefail\n  \ninput_file=$1\nline_nr=$2\n\nhead -n \"$line_nr\" \"$input_file\" | tail -n 1\n\nTo run the script and make it print the 4th line of samples.txt:\n\nbash line.sh samples.txt 4"
  },
  {
    "objectID": "sessions/06-scripts.html#script-variations-and-improvements",
    "href": "sessions/06-scripts.html#script-variations-and-improvements",
    "title": "Shell Scripting",
    "section": "4 Script variations and improvements",
    "text": "4 Script variations and improvements\n\n4.1 A script to serve as a starting point\nWe’ve learned that the head command prints the first lines of a file, whereas the tail command prints the last lines. Sometimes it’s nice to be able to quickly see both ends of a file, so let’s write a little script that can do that for us, as a starting point for the next few modifications.\nOpen a new file, save it as headtail.sh, and add the following code to it:\n\n#!/bin/bash\nset -ueo pipefail\n\ninput_file=$1\n\nhead -n 2 \"$input_file\"\necho \"---\"\ntail -n 2 \"$input_file\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\nNext, let’s run our headtail.sh script:\n\nbash headtail.sh samples.txt\n\n\n\n4.2 Redirecting output to a file\nSo far, the output of our scripts was printed to screen, e.g.:\n\nIn printnames.sh, we simply echo’d, inside sentences, the arguments passed to the script.\nIn headtail.sh, we printed the first and last few lines of a file.\n\nAll this output was printed to screen because that is the default output mode of Unix commands, and this works the same way regardless of whether those commands are run directly on the command line, or are run inside a script.\nAlong those same lines, we have already learned that we can “redirect” output to a file using > (write/overwrite) and >> (append) when we run shell commands — and this, too, works exactly the same way inside a script.\n\nWhen working with genomics data, we commonly have files as input, and new/modified files as output. So let’s practice with this and modify our headtail.sh script so that it writes output to a file.\nWe’ll make the following changes:\n\nWe will have the script accept a second argument: the output file name. Of course, we could also simply write the output to a predefined (“hardcoded”) file name such as out.txt, but in general, it’s better practice to keep this flexible via an argument.\nWe will redirect the output of our head, echo, and tail commands to the output file. We’ll have to append (>>) in the last two cases.\n\n\n#!/bin/bash\nset -ueo pipefail\n\ninput_file=$1\noutput_file=$2\n\nhead -n 2 \"$input_file\" > \"$output_file\"\necho \"---\" >> \"$output_file\"\ntail -n 2 \"$input_file\" >> \"$output_file\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\nNow we run the script again, this time also passing the name of an output file:\n\nbash headtail.sh samples.txt samples_headtail.txt\n\nThe script will no longer print any output to screen, and our output should instead be in samples_headtail.txt:\n\n# Check that the file exists and was just modified:\nls -lh samples_headtail.txt\n\n# Print the contents of the file to screen\ncat samples_headtail.txt\n\n\n\n4.3 Report what’s happening\nIt is often useful to have your scripts “report” or “log” what is going on. Let’s keep thinking about a script that has file(s) as the main output, but instead of having no output printed to screen at all, we’ll print some logging output to screen. For instance: what is the date and time, which arguments were passed to the script, what are the output files, and perhaps even summaries of the output. All of this can help with troubleshooting.3\nLet’s try this with our headtail.sh script.\n\n#!/bin/bash\nset -ueo pipefail\n\n## Process command-line arguments\ninput_file=$1\noutput_file=$2\n\n## Initial logging \necho \"Starting script $0\"           # Print name of script\ndate                                # Print date & time\necho \"Input file:   $input_file\"\necho \"Output file:  $output_file\" \necho                                # Print empty line to separate initial & final logging\n\n## Print the first and last two lines to a separate file\nhead -n 2 \"$input_file\" > \"$output_file\"\necho \"---\" >> \"$output_file\"\ntail -n 2 \"$input_file\" >> \"$output_file\"\n\n## Final logging\necho \"Listing the output file:\"\nls -lh \"$output_file\"\necho \"Done with script $0\"\ndate\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\nA couple of notes about the lines that were added to the script above:\n\nPrinting the date at the end of the script as well will allow you to check for how long the script ran, which can be informative for longer-running scripts.\nPrinting the input and output files (and the command-line arguments more generally) can be particularly useful for troubleshooting\nPrinting a “marker line” like Done with script, indicating that the end of the script was reached, is handy because due to our set settings, seeing this line printed means that no errors were encountered.\nBecause our script grew so much, I also added some comment headers like “Initial logging” to make the script easier to read, and such comments can be made more extensive to really explain what is being done.\n\n\n\n\nLet’s run the script again:\n\nbash headtail.sh printname.sh tmp.txt\n\nStarting script headtail.sh\nMon Aug 15 01:08:06 PM CEST 2022\nInput file:   printname.sh\nOutput file:  tmp.txt\n\nListing the output file:\n-rw-rw-r-- 1 jelmer jelmer 77 Aug 15 13:08 tmp.txt\nDone with script headtail.sh\nMon Aug 15 01:08:06 PM CEST 2022\n\n\nThe script printed some details for the output file, but not its contents (that would have worked here, but is usually not sensible when working with genomics data). Let’s take a look, though, to make sure the script worked:\n\ncat tmp.txt\n\n#!/bin/bash\nset -ueo pipefail\n---\necho \"First name: $1\"\necho \"Last name: $2\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe reporting (echo-ing) may have started to seem silly for our litle script, but fairly extensive reporting (as well as testing, which is outside the scope of this workshop) can be very useful — and will be eventually a time-saver.\nThis is especially true for long-running scripts, or scripts that you often reuse and perhaps share with others.\n\n\n \n\n\n\n\n\n\n\nKeyboard shortcut to run shell commands from the editor\n\n\n\nTo add a keyboard shortcut that will send code selected in the editor pane to the terminal (such that you don’t have to copy and paste):\n\nClick the      (bottom-left) => Keyboard Shortcuts.\nFind Terminal: Run Selected Text in Active Terminal, click on it, then add a shortcut, e.g. Ctrl+Enter."
  },
  {
    "objectID": "sessions/07-software.html",
    "href": "sessions/07-software.html",
    "title": "Using Software at OSC",
    "section": "",
    "text": "So far, we have only used commands that are available in any Unix shell. But to actually analyze genomics data sets, we also need to use specialized bioinformatics software.\nMost software that is already installed at OSC must nevertheless be “loaded” (“activated”) before we can use it; and if our software of choice is not installed, we have to do so ourselves. We will cover those topics in this module."
  },
  {
    "objectID": "sessions/07-software.html#setup",
    "href": "sessions/07-software.html#setup",
    "title": "Using Software at OSC",
    "section": "1 Setup",
    "text": "1 Setup\n\n\n\n\n\n\nStarting a VS Code session with an active terminal (click here)\n\n\n\n\n\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 3 in the box Number of hours\nEnter /fs/ess/scratch/PAS2250 in the box Working Directory\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\nOpen a terminal by clicking the “hamburger menu” way in the top left, and then Terminal > New Terminal.\nIn the terminal, type Bash and press Enter."
  },
  {
    "objectID": "sessions/07-software.html#running-command-line-programs",
    "href": "sessions/07-software.html#running-command-line-programs",
    "title": "Using Software at OSC",
    "section": "2 Running command-line programs",
    "text": "2 Running command-line programs\nAs pointed out in the introduction to the workshop, bioinformatics software (programs) that we use to analyze genomic data are typically run from the command line. That is, they have “command-line interfaces” (CLIs) rather than “graphical user interfaces” (GUIs), and are run using commands that are structurally very similar to how we’ve been using basic Unix commands.\nFor instance, we can run the program FastQC as follows, instructing it to process the FASTQ file sampleA.fastq.gz with default options:\n\nfastqc sampleA.fastq.gz\n\nSo, what we have learned in the previous modules can easily be applied to run command-line programs. But, we first need to load and/or install these programs.\n\n\n\n\n\n\nRunning inside a script or interactively\n\n\n\nLike any other command, we could in principle run the line of code above either in our interactive shell or from inside a script. In practice, it is better to do this in a script, especially at OSC, because:\n\nSuch programs typically take a while to run\nWe are not supposed to run processes that use significant resources on login nodes\nWe can run the same script simultaneously for different input files."
  },
  {
    "objectID": "sessions/07-software.html#software-at-osc-with-lmod",
    "href": "sessions/07-software.html#software-at-osc-with-lmod",
    "title": "Using Software at OSC",
    "section": "3 Software at OSC with Lmod",
    "text": "3 Software at OSC with Lmod\nOSC administrators manage software with the Lmod system of software modules. For us users, this means that even though a lot of software is installed, most of it can only be used after we explicitly load it.\n(That may seem like a drag, but on the upside, this practice enables the use of different versions of the same software, and of mutually incompatible software on a single system.)\n\n3.1 Checking for available software\nThe OSC website has a list of software that has been installed at OSC. You can also search for available software in the shell:\n\nmodule spider lists modules that are installed.\nmodule avail lists modules that can be directly loaded, given the current environment (i.e., depending on which other software has been loaded).\n\nSimply running module spider or module avail would spit out complete lists — more usefully, we can provide search terms as arguments to these commands:\n\nmodule spider python\n\n\n\n\n\npython:\n\n\n\n Versions:\n    python/2.7-conda5.2\n    python/3.6-conda5.2\n    python/3.7-2019.10\n\n\nmodule avail python\n\n\npython/2.7-conda5.2         python/3.6-conda5.2 (D)         python/3.7-2019.10\n\n\n\n\n\n\n\nTip\n\n\n\nThe (D) in the output above marks the default version of the program; that is, the version of the program that would be loaded if we don’t specify a version (see examples below).\n\n\n\n\n3.2 Loading software\nAll other Lmod software functionality is also accessed using module “subcommands” (we call module the command and e.g. spider the subcommand). For instance, to load and unload software:\n\n# Load a module:\nmodule load python              # Load the default version\nmodule load python/3.7-2019.10  # Load a specific version\n\n# Unload a module:\nmodule unload python\n\nTo check which modules have been loaded (the list will include modules that have been loaded automatically):\n\nmodule list\n\n\nCurrently Loaded Modules:\n    1) xalt/latest       2) gcc-compatibility/8.4.0       3) intel/19.0.5       4) mvapich2/2.3.3       5) modules/sp2020\n\n\n\n3.3 A practical example\nLet’s load a very commonly used bioinformatics program that we will also use in examples later on: FastQC. FastQC performs quality control (hence: “QC”) on FASTQ files.\nFirst, let’s test that we indeed cannot currently use fastqc by running fastqc with the --help flag:\n\nfastqc --help\n\n\nbash: fastqc: command not found\n\n\n\n\n\n\n\nHelp!\n\n\n\nA solid majority of command-line programs can be run with with a --help (and/or -h) flag, and this is perfect to try first, since it will tell use whether we can use the program, and if we can, we immediately get some usage information.\n\n\nNext, let’s check whether it is available at OSC, and if so, in which versions:\n\nmodule avail fastqc\n\n\nfastqc/0.11.8\n\nThere is only one version available (0.11.8), which means that module load fastqc and module load fastqc/0.11.8 would each load that same version.\n\n\n\n\n\n\nWhat might still be an argument to specify the version when we load the software?\n\n\n\n\n\nWhen we use it inside a script:\n\nThis would ensure that when we run the same script a year later, the same version would be used (assuming it hasn’t been removed) — otherwise, it’s possible a newer version would has been installed in the meantime, which might produce different results.\nIt will make it easy to see which version we used, which is something we typically report in papers.\n\n\n\n\nLet’s load the FastQC module:\n\nmodule load fastqc/0.11.8\n\nAfter we have loaded the module, we can retry our --help attempt:\n\nfastqc --help\n\n\n        FastQC - A high throughput sequence QC analysis tool\nSYNOPSIS\n    fastqc seqfile1 seqfile2 .. seqfileN\n\nfastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] \n       [-c contaminant file] seqfile1 .. seqfileN\n       \n[…and much more]"
  },
  {
    "objectID": "sessions/07-software.html#when-software-isnt-installed-at-osc",
    "href": "sessions/07-software.html#when-software-isnt-installed-at-osc",
    "title": "Using Software at OSC",
    "section": "4 When software isn’t installed at OSC",
    "text": "4 When software isn’t installed at OSC\nIt’s not too uncommon that software you need for your project is not installed at OSC, or that you need a more recent version of the software than those available. The main options available to you in such a case are to:\n\n“Manually” install the software, which in the best case involves downloading a directly functioning binary (executable), but more commonly requires you to “compile” (build) the program. This is sometimes straightforward but can also become extremely tricky, especially at OSC where you don’t have “administrator privileges”1 at OSC and will often have difficulties with “dependencies”2.\nSend an email to OSC Help. They might be able to help you with your installation, or in case of commonly used software, might be willing to perform a system-wide installation (that is, making it available through module).\nUse conda, which creates software environments that are activated like in the module system.\nUse Apptainer / Singularity “containers”. Containers are software environments that are more self-contained, akin to mini virtual machines.\n\nConda and containers are useful not only at OSC, where they bypass issues with dependencies and administrator privileges, but more generally, for reproducible and portable software environments. They also allow you to easily maintain distinct “environments”, each with a different version of the same software, or with mutually incompatible software.\nWe will teach conda here because it is easier to learn and use than containers, and because nearly all open-source bioinformatics software is available as a conda package.\n\n\n\n\n\n\nWhen to use containers instead of Conda\n\n\n\n\nIf you need to use software that requires a different Operating System (OS) or OS version than the one at OSC.\nIf you want or require even greater reproducibility and portability to create an isolated environment that can be exported and used anywhere."
  },
  {
    "objectID": "sessions/07-software.html#using-conda",
    "href": "sessions/07-software.html#using-conda",
    "title": "Using Software at OSC",
    "section": "5 Using conda",
    "text": "5 Using conda\nConda creates so-called environments in which you can install one or more software packages. As mentioned above, these environments are activated and deactivated in a similar manner as with the Lmod system – but the key difference is that we can create and manage these environments ourselves.\n\n\n\n\n\n\nWhat’s in an environment?\n\n\n\nOne environment per software, or one per project\nNote that even when you install a single program, many things are usually installed: dependencies\n\n\n\n5.1 Loading the (mini)conda module\nWhile it is also fairly straightforward to install conda for yourself 3, we will use OSC’s system-wide installation of conda in this workshop. Therefore, we first need to use a module load command to make it available:\n\n# (The most common installation of conda is actually called \"miniconda\")\nmodule load miniconda3\n\n\n\n5.2 One-time conda configuration\nWe will also do some one-time configuration, which will set the conda “channels” (basically, software repositories) that we want to use when we install software. This config also includes setting relative priorities among the channels, since one software package may be available from multiple channels.\nLike with module commands, conda commands consist of two parts, the conda command itself and a subcommand, such as config:\n\nconda config --add channels defaults     # Added first => lowest priority\nconda config --add channels bioconda\nconda config --add channels conda-forge  # Added last => highest priority\n\nLet’s check whether this configuration step worked:\n\nconda config --get channels\n\n\n\n5.3 Creating an environment for cutadapt\nTo practice using conda, we will now create a conda environment with the program cutadapt installed.\ncutadapt is a commonly used program to remove adapters or primers from sequence reads in FASTQ files; in particular, it is ubiquitous for primer removal in (e.g. 16S rRNA) microbiome metabarcoding studies. But there is no Lmod module on OSC for it, so if we want to use, our best option is to resort to conda.\nHere is the command to create a new environment and install cutadapt into that environment:\n\nconda create -y -n cutadapt -c bioconda cutadapt\n\nLet’s break the above command down:\n\ncreate is the conda subcommand to create a new environment.\n-y is a flag that prevents us from being asked to confirm installation.\nFollowing the -n option, we can specify the name of the environment, so -n cutadapt means that we want our environment to be called cutadapt. We can use whatever name we like for the environment, but of course a descriptive yet concise name is a good idea. Since we are making a single-program environment, it makes sense to simply name it after the program.\nFollowing the -c option, we can specify a channel from which we want to install, so -c bioconda indicates we want to use the bioconda channel. (Given that we’ve done some config above, this is not always necessary, but it can be good to be explicit.)\nThe cutadapt at the end of the line simply tells conda to install the package of that name. This is a “positional” argument to the command (note that there’s no option like -s before it): we put any software package(s) we want to install at the end of the command.\n\n\n\n\n\n\n\nSpecifying a version\n\n\n\nIf we want to be explicit about the version we want to install, we can add the version after = following the package name. We do that below, and we also include the version in the environment name:\n\nconda create -y -n cutadapt-4.1 -c bioconda cutadapt=4.1\n\n Let’s run the command above and see if we can install cutadapt\n\n\n\n\n5.4 Creating an environment for any program\nMinor variations on the conda create command above can be used to install almost any program for which is conda package is available.\nHowever, you may be wondering how we would know:\n\nWhether the program is available and what its conda package’s name is\nWhich versions are available\nWhich conda channel we should use\n\nADD…\n\n\n5.5 Activating conda environments\nWhereas we use the term “load” for Lmod modules, we use “activate” to the same effect for conda environments.\nOddly enough, the most foolproof way to activate a conda environment is to use source activate rather than the expected conda activate — for instance:\n\nsource activate cutadapt-4.1\n\n\n(cutadapt-4.1) [jelmer@pitzer-login03 PAS2250]$\n\n\n\n\n\n\n\nEnvironment indicator\n\n\n\nWhen we have an active conda environment, its name is conveniently displayed in our prompt, as depicted above.\n\n\nAfter we have activated the cutadapt environment, we should be able to actually use the program. To test this, we’ll again simply run it with a --help option:\n\ncutadapt --help\n\n\n\n\n5.6 Lines to add to your scripts\nWhile you’ll typically want to do installation interactively and only need to do to it once (see note below), you should always include the necessary code to load/activate your programs in your shell scripts.\nWhen your program is available as an Lmod module, this simply entails a module load call — e.g., for fastqc:\n\n#!/bin/bash\nset -ueo pipefail\n\nmodule load fastqc\n\nWhen your program is available in a conda environment, this entails a module load command to load conda itself, followed by a source activate command to load the relevant conda environment:\n\n#!/bin/bash\n\n## Load software\nmodule load miniconda3\nsource activate cutadapt-4.1\n\n## Strict/safe Bash settings \nset -ueo pipefail\n\n\n\n\n\n\n\nWarning\n\n\n\nWe’ve moved the set -ueo pipefail line below the source activate command, because the conda activation procedure may otherwise throw “unbound variable” errors.\n\n\n\n\n\n\n\n\nInstall once, load always\n\n\n\n\nProvided you don’t need to switch versions, you only need to install a program once. This is true also at OSC and also when using conda (your environments won’t disappear unless you delete them).\nIn every single “session” that you want to use a program via an Lmod module or conda environment, you …"
  },
  {
    "objectID": "sessions/07-software.html#addendum-a-few-other-useful-conda-commands",
    "href": "sessions/07-software.html#addendum-a-few-other-useful-conda-commands",
    "title": "Using Software at OSC",
    "section": "6 Addendum: a few other useful conda commands",
    "text": "6 Addendum: a few other useful conda commands\n\nDeactivate the currently active conda environment:\n\nconda deactivate   \n\nActivate one environment and then “stack” an additional environment (a regular conda activate command would switch environments):\n\nsource activate cutadapt         # Now, the env \"cutadapt\" is active\nconda activate --stack multiqc   # Now, both \"cutadapt\" and \"multiqc\" are active\n\nRemove an environment entirely:\n\nconda env remove -n cutadapt\n\nList all your conda environments:\n\nconda env list\n\nList all packages (programs) installed in an environment:\n\nconda list -n cutadapt"
  },
  {
    "objectID": "sessions/08-slurm.html",
    "href": "sessions/08-slurm.html",
    "title": "Compute Jobs with Slurm",
    "section": "",
    "text": "We have so far been working on login nodes at OSC, but in order to run some actual analyses, you will need access to compute nodes.\nAutomated scheduling software allows hundreds of people with different requirements to access compute nodes effectively and fairly. For this purpose, OSC uses the Slurm scheduler (Simple linux utility for resource management).\nA temporary reservation of (parts of) a compute node is called a compute job. What are the options to start a compute job at OSC?\nWhen running command-line programs for genomics analyses, batch jobs are the most useful and will be the focus of this module. We’ll also touch on interactive shell jobs, which can occasionally be handy and are requested and managed in a very similar way to batch jobs."
  },
  {
    "objectID": "sessions/08-slurm.html#setup",
    "href": "sessions/08-slurm.html#setup",
    "title": "Compute Jobs with Slurm",
    "section": "1 Setup",
    "text": "1 Setup\n\n\n\n\n\n\nStarting a VS Code session with an active terminal (click here)\n\n\n\n\n\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 3 in the box Number of hours\nEnter /fs/ess/scratch/PAS2250 in the box Working Directory\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\nOpen a terminal by clicking the “hamburger menu” way in the top left, and then Terminal > New Terminal.\nIn the terminal, type Bash and press Enter."
  },
  {
    "objectID": "sessions/08-slurm.html#interactive-shell-jobs",
    "href": "sessions/08-slurm.html#interactive-shell-jobs",
    "title": "Compute Jobs with Slurm",
    "section": "2 Interactive shell jobs",
    "text": "2 Interactive shell jobs\nInteractive shell jobs will grant you interactive shell access on a compute node. Working in an interactive shell job is operationally identical to working on a login node as we’ve been doing so far, but the difference is that it’s now okay to use significant computing resources. (How much and for how long depends on what you reserve.)\n\n2.1 Using srun\nA couple of different commands can be used to start an interactive shell job. I prefer the general srun command1, which we can use with the --pty /bin/bash option to get an interactive Bash shell.\nHowever, if we run that command without additional options, we get an error:\n\nsrun --pty /bin/bash\n\n\nsrun: error: ERROR: Job invalid: Must specify account for job\nsrun: error: Unable to allocate resources: Unspecified error\n\nAs the error message Must specify account for job tries to tell us, we need to indicate which OSC project (or as SLURM puts it, “account”) we want to use for this compute job. This is because an OSC project always has to be charged for the computing resources used during a compute job.\nTo specify the project/account, we can use the --account= option followed by the project number:\n\nsrun --account=PAS2250 --pty /bin/bash\n\n\nsrun: job 12431932 queued and waiting for resources\nsrun: job 12431932 has been allocated resources\n[…regular login info, such as quota, not shown…]\n[jelmer@p0133 PAS2250]$\n\nThere we go! First we got some Slurm scheduling info:\n\nInitially, the job is “queued”: that is, waiting to start.\nVery soon (usually!), the job has been “allocated resources”: that is, computing resources such as a compute node were found and reserved for the job.\n\nThen:\n\nThe job starts and because we’ve reserved an interactive shell job, this means that a new Bash shell is initiated: for that reason, we get to see our regular login info once again.\nMost importantly, we are no longer on a login node but on a compute node, as our prompt hints at: we switched from something like [jelmer@pitzer-login04 PAS2250]$ to the [jelmer@p0133 PAS2250]$ shown above.\nNote also that the job has a number (above: job 12431932): every compute job has such a unique identifier among all jobs by all users at OSC, and we can use this number to monitor and manage it. All of us will therefore see a different job number pop up.\n\n\n\n\n\n\n\nOSC projects\n\n\n\nDuring this workshop, we can all use the project PAS2250, which is actually a project that OSC has freely given me to introduce people to working at OSC. The project will still be charged but the credits on it were freely awarded.\nTo work on your own research project at OSC, you will either have to get your own project (typically, PIs get one for their lab or for a specific research project) or you can become an MCIC member and use the MCIC project.\n\n\n\n\n2.2 Compute job options\nThe --account= option is just one of out of many options we can use when reserving a compute job, but is the only one that always has to be specified (including for batch jobs and for Interactive Apps).\nDefaults exist for all other options, such as the amount of time (1 hour) and the number of cores (1). These options are all specified in the same way for interactive and batch jobs, and we’ll dive into them below.\n\n\n\n\n\n\nTip\n\n\n\nMany SLURM options have a long format (--account=PAS2250) and a short format (-A PAS2250), which can generally be used interchangeably. For clarity, we’ll try to stick to long format options during this workshop."
  },
  {
    "objectID": "sessions/08-slurm.html#non-interactive-jobs",
    "href": "sessions/08-slurm.html#non-interactive-jobs",
    "title": "Compute Jobs with Slurm",
    "section": "3 Non-interactive jobs",
    "text": "3 Non-interactive jobs\nWhen requesting non-interactive jobs, we are asking the Slurm scheduler to run a script on a compute node. For this reason, we can also refer to it as “submitting a script (to the queue)”.\nIn contrast to interactive shell jobs, we stay in our current shell on a login node when submitting a script, and cannot really interact with the process on the compute node, other than:\n\nOutput from the script that would normally be printed to screen ends up in a file.\nWe can do things like monitoring whether the job is still running and cancelling the job, which will revoke the compute node reservation and stop the ongoing process.\n\nWhereas we used Slurm’s srun command to start an interactive shell job, we use its sbatch command to submit a script.\nRecall from the Bash scripting module that we can run a Bash script as follows:\n\nbash printnames.sh Jane Doe\n\nThe above command will run the script on our current node, which is typically a login node. To instead submit the script to the Slurm queue, we simply replace bash by sbatch:\n\nsbatch printnames.sh Jane Doe\n\nOf course we always have to specify the OSC account when submitting a compute job.\na minimal functional sbatch call would be:\n\nsbatch --account=PAS2250 printnames.sh Jane Doe\n\n\n\n\n\n\n\nsbatch options vs. script arguments\n\n\n\n\nsbatch [sbatch-options] myscript.sh [script-arguments]\n\n\n# No options/arguments for either:\nsbatch printnames.sh\n\nsbatch printnames.sh Jane Doe\n\nsbatch --account=PAS2250 printnames.sh\n\nsbatch --account=PAS2250 printnames.sh Jane Doe\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe script that we submit can be in different languages but typically (and in the examples in this workshop), they are shell (Bash) scripts.\n\n\n\n\n\n\n\n\nTip\n\n\n\nBoth interactive and non-interactive jobs start in the directory that they were submitted from: that is, your working directory will remain the same."
  },
  {
    "objectID": "sessions/09-examples.html#tba",
    "href": "sessions/09-examples.html#tba",
    "title": "Example Compute Jobs",
    "section": "1 TBA",
    "text": "1 TBA"
  },
  {
    "objectID": "sessions/08-slurm.html#batch-jobs",
    "href": "sessions/08-slurm.html#batch-jobs",
    "title": "Compute Jobs with Slurm",
    "section": "3 Batch jobs",
    "text": "3 Batch jobs\nWhen requesting batch jobs, we are asking the Slurm scheduler to run a script on a compute node. For this reason, we can also refer to it as “submitting a script (to the queue)”.\nIn contrast to interactive shell jobs, we stay in our current shell on a login node when submitting a script, and cannot really interact with the process on the compute node, other than:\n\nOutput from the script that would normally be printed to screen ends up in a file.\nWe can do things like monitoring whether the job is still running and cancelling the job, which will revoke the compute node reservation and stop the ongoing process.\n\n\n\n\n\n\n\nTip\n\n\n\nThe script that we submit can be in different languages but typically, including in all examples in this workshop, they are shell (Bash) scripts.\n\n\n\n3.1 The sbatch command\nWhereas we used Slurm’s srun command to start an interactive shell job, we use its sbatch command to submit a script.\nRecall from the Bash scripting module that we can run a Bash script as follows:\n\nbash printname.sh Jane Doe\n\nFirst name: Jane\nLast name: Doe\n\n\n\n\n\n\n\n\nCan’t find the printname.sh script?\n\n\n\n\n\n\nOpen a new file in the VS Code editor (     =>   File   =>   New File) and save it as printname.sh\nCopy the code below into the script:\n\n\n#!/bin/bash\nset -ueo pipefail\n\nfirst_name=$1\nlast_name=$2\n  \necho \"First name: $first_name\"\necho \"Last name: $last_name\"\n\n\n\n\nThe above command ran the script on our current node, a login node. To instead submit the script to the Slurm queue, we would start by simply replacing bash by sbatch:\n\nsbatch printname.sh Jane Doe\n\n\nsrun: error: ERROR: Job invalid: Must specify account for job\nsrun: error: Unable to allocate resources: Unspecified error\n\nAs we’ve learned, we always have to specify the OSC account when submitting a compute job. Conveniently, we can also specify Slurm/sbatch options inside our script, but first, let’s add the --account option on the command line:\n\nsbatch --account=PAS2250 printname.sh Jane Doe\n\n\nSubmitted batch job 12431935\n\n\n\n\n\n\n\nsbatch options vs. script arguments\n\n\n\n\nsbatch [sbatch-options] myscript.sh [script-arguments]\n\n\nsbatch printname.sh                             # No options/arguments for either\nsbatch printname.sh Jane Doe                    # Script arguments but no sbatch option\nsbatch --account=PAS2250 printname.sh           # sbatch option but no script arguments\nsbatch --account=PAS2250 printname.sh Jane Doe  # Both sbatch option and script arguments\n\n\n\n\n\n3.2 Adding sbatch options in scripts\nInstead of specifying Slurm/sbatch options on the command-line when we submit the script, we can also add these options inside the script.\nThis is handy because even though we have so far only seen the account= option, you often want to specify several options. That would lead to very long sbatch commands. Additionally, it can be practical to store a script’s typical Slurm options along with the script itself.\nWe add the options in the script using another type of special comment line akin to the shebang line, marked by #SBATCH. The equivalent of adding --account=PAS2250 after sbatch on the command line, is a line in a script that reads #SBATCH --account=PAS2250.\nJust like the shebang line, the #SBATCH line(s) should be at the top of the script. Let’s add one to the printname.sh script, such that the first few lines read:\n\n#!/bin/bash\n\n#SBATCH --account=PAS2250\n\nset -ueo pipefail\n\nAfter having added this to the script, we can successfully run our earlier sbatch command without options:\n\nsbatch printname.sh Jane Doe\n\n\nSubmitted batch job 12431942\n\n\n\n\n\n\n\nRunning a script with #SBATCH in other contexts\n\n\n\nBecause the #SBATCH lines are special comment lines, they will simply be ignored and not throw any errors when you run a script that contains them in other contexts: when not running them as a batch job at OSC, or even when running them on a computer without Slurm installed.\n\n\n\n\n\n\n\n\nTip\n\n\n\nAny sbatch options provided on the command-line will override the equivalent options provided in the script itself.\n\n\n\n\n3.3 Other common sbatch options\n\n\n3.4 Where does the output go?\n\n\n\n\n\n\nTip\n\n\n\nBoth interactive and batch jobs start in the directory that they were submitted from: that is, your working directory will remain the same."
  },
  {
    "objectID": "sessions/08-slurm.html#intro-to-batch-jobs",
    "href": "sessions/08-slurm.html#intro-to-batch-jobs",
    "title": "Compute Jobs with Slurm",
    "section": "3 Intro to batch jobs",
    "text": "3 Intro to batch jobs\nWhen requesting batch jobs, we are asking the Slurm scheduler to run a script on a compute node. For this reason, we can also refer to it as “submitting a script (to the queue)”.\nIn contrast to interactive shell jobs, we stay in our current shell on a login node when submitting a script, and cannot really interact with the process on the compute node, other than:\n\nOutput from the script that would normally be printed to screen ends up in a file.\nWe can do things like monitoring whether the job is still running and cancelling the job, which will revoke the compute node reservation and stop the ongoing process.\n\n\n\n\n\n\n\nTip\n\n\n\nThe script that we submit can be in different languages but typically, including in all examples in this workshop, they are shell (Bash) scripts.\n\n\n\n3.1 The sbatch command\nWhereas we used Slurm’s srun command to start an interactive shell job, we use its sbatch command to submit a script.\nRecall from the Bash scripting module that we can run a Bash script as follows:\n\nbash printname.sh Jane Doe\n\nFirst name: Jane\nLast name: Doe\n\n\n\n\n\n\n\n\nCan’t find the printname.sh script?\n\n\n\n\n\n\nOpen a new file in the VS Code editor (     =>   File   =>   New File) and save it as printname.sh\nCopy the code below into the script:\n\n\n#!/bin/bash\nset -ueo pipefail\n\nfirst_name=$1\nlast_name=$2\n  \necho \"First name: $first_name\"\necho \"Last name: $last_name\"\n\n\n\n\nThe above command ran the script on our current node, a login node. To instead submit the script to the Slurm queue, we would start by simply replacing bash by sbatch:\n\nsbatch printname.sh Jane Doe\n\n\nsrun: error: ERROR: Job invalid: Must specify account for job\nsrun: error: Unable to allocate resources: Unspecified error\n\nAs we’ve learned, we always have to specify the OSC account when submitting a compute job. Conveniently, we can also specify Slurm/sbatch options inside our script, but first, let’s add the --account option on the command line:\n\nsbatch --account=PAS2250 printname.sh Jane Doe\n\n\nSubmitted batch job 12431935\n\n\n\n\n\n\n\nsbatch options vs. script arguments\n\n\n\n\nsbatch [sbatch-options] myscript.sh [script-arguments]\n\n\nsbatch printname.sh                             # No options/arguments for either\nsbatch printname.sh Jane Doe                    # Script arguments but no sbatch option\nsbatch --account=PAS2250 printname.sh           # sbatch option but no script arguments\nsbatch --account=PAS2250 printname.sh Jane Doe  # Both sbatch option and script arguments\n\n\n\n\n\n3.2 Adding sbatch options in scripts\nInstead of specifying Slurm/sbatch options on the command-line when we submit the script, we can also add these options inside the script.\nThis is handy because even though we have so far only seen the account= option, you often want to specify several options. That would lead to very long sbatch commands. Additionally, it can be practical to store a script’s typical Slurm options along with the script itself.\nWe add the options in the script using another type of special comment line akin to the shebang line, marked by #SBATCH. The equivalent of adding --account=PAS2250 after sbatch on the command line, is a line in a script that reads #SBATCH --account=PAS2250.\nJust like the shebang line, the #SBATCH line(s) should be at the top of the script. Let’s add one to the printname.sh script, such that the first few lines read:\n\n#!/bin/bash\n\n#SBATCH --account=PAS2250\n\nset -ueo pipefail\n\nAfter having added this to the script, we can successfully run our earlier sbatch command without options:\n\nsbatch printname.sh Jane Doe\n\n\nSubmitted batch job 12431942\n\n\n\n\n\n\n\nRunning a script with #SBATCH in other contexts\n\n\n\nBecause the #SBATCH lines are special comment lines, they will simply be ignored and not throw any errors when you run a script that contains them in other contexts: when not running them as a batch job at OSC, or even when running them on a computer without Slurm installed.\n\n\n\n\n\n\n\n\nTip\n\n\n\nAny sbatch options provided on the command-line will override the equivalent options provided in the script itself.\n\n\n\n\n3.3 Where does the output go?\n\n\n\n\n\n\nTip\n\n\n\nBoth interactive and batch jobs start in the directory that they were submitted from: that is, your working directory will remain the same."
  },
  {
    "objectID": "sessions/08-slurm.html#other-common-sbatch-options",
    "href": "sessions/08-slurm.html#other-common-sbatch-options",
    "title": "Compute Jobs with Slurm",
    "section": "4 Other common sbatch options",
    "text": "4 Other common sbatch options\n\n4.1 --time: Time limit (“wall time”).\n\nYour job gets killed as soon as it hits the time limit!\nWall time means…\nYou will only be charged for the time your job actually used.\nIn general, shorter jobs are likely to start running sooner\nThe default is 1 hour. Acceptable time formats include:\n\nminutes\nhours:minutes:seconds\ndays-hours\n\n\n\n#!/bin/bash\n#SBATCH --time=1:00:00\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are uncertain about the time your job will take, ask for (much) more time than you think you will need.\n\n\n\n\n\n\n\n\nWalltime limits\n\n\n\nFor single-node jobs, up to 168 hours (7 days) can be requested. If that’s not enough, you can request access to the longserial queue for jobs of up to 336 hours (14 days)."
  }
]