[
  {
    "objectID": "info/glossary.html",
    "href": "info/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Directory\nSyntax\nString\npseudocode\nbinary (executable)\ndependency"
  },
  {
    "objectID": "info/info.html#locations-and-links",
    "href": "info/info.html#locations-and-links",
    "title": "Practical Workshop Information",
    "section": "Locations and links",
    "text": "Locations and links\n\nColumbus: Aronoff Laboratory, room 104 (instructor: Mike Sovic)\nWooster: Selby Hall, room 203 (instructor: Jelmer Poelstra)\nZoom: email us for the link! (instructor: Jelmer Poelstra)\nGoogle Doc for sharing links and code, and for non-urgent questions"
  },
  {
    "objectID": "info/info.html#computer-setup",
    "href": "info/info.html#computer-setup",
    "title": "Practical Workshop Information",
    "section": "Computer Setup",
    "text": "Computer Setup\n\nYour computer\nSince we will be working entirely at the Ohio Supercomputer Center (OSC), and will be doing so through our internet browsers:\n\nYou won’t need to install anything\nAny operating system will work\nYou won’t need an especially powerful machine (though browsers, especially in combination with Zoom, can use their fair share of memory).\n\nIf you’re attending in person, you will need to bring a laptop. You can watch the presentation on a big screen in the room, which will make it easier to code along (see below). You won’t need to connect to the Zoom call.\nIf you’re attending via Zoom, we would recommend a two-monitor setup. This is because much of the time, you need to be able to simultaneously see the instructor’s screen via Zoom as well as your own browser window.\n\n\nOSC account and project\nTo work with OSC resources, we need access to an “OSC project”. We will be using the project PAS2250 during the workshop, and all participants will be added to that project. If you don’t yet have a personal OSC account, you will receive an invitation to create one when you’ve been added to the project.\n\n\nGoogle Doc\nWe’ll use this Google Doc for sharing links and code, and for non-urgent questions."
  },
  {
    "objectID": "info/info.html#miscellaneous-info",
    "href": "info/info.html#miscellaneous-info",
    "title": "Practical Workshop Information",
    "section": "Miscellaneous info",
    "text": "Miscellaneous info\n\nExpect to participate!\nThe modules will be a mixture of lectures that include “participatory live-coding” (also called “code-along”; with the instructor slowly demonstrating and participants expected to follow along for themselves) and small single-person exercises (we won’t be doing breakout rooms / groups). Therefore, be prepared to actively participate during much of the workshop!\n\n\nExample data\nWe will mainly use a set of FASTQ files from a published RNAseq experiment as example data. (It may be worth emphasizing that the exact data type matters relatively little for the purposes of our workshop, since we focus on foundational skills and not specific genomic analyses.)\nIf you have any genomic data of your own, you can bring it along and you should be able to experiment a bit with it during our second session on Friday afternoon. If this is a large dataset (say, >10GB), uploading it to OSC will take some time. You could try to start this after Wednesday’s sessions, when you’ve had some background on this. Alternatively, you can contact the instructors about this prior to the workshop.\n\n\nParticipants\nWe’re expecting up to 14 people in Wooster, 14 in Columbus, and 6 via Zoom."
  },
  {
    "objectID": "info/about.html",
    "href": "info/about.html",
    "title": "General Workshop Info & Signing Up",
    "section": "",
    "text": "This workshop is geared towards people who would like to get started with analyzing genomic datasets.\nIt will be taught in-person with video-linking at the Wooster and Columbus Ohio State campuses, and it is also possible to join online through Zoom.\nWe will have an instructor at each campus: Jelmer Poelstra from the Molecular and Cellular Imaging Center (MCIC) at the Wooster campus, and Mike Sovic from the Center for Applied Plant Sciences (CAPS) at the Columbus campus. We will also have two graduate student TAs: Menuka Bhandari (Center for Food Animal Health) and Camila Perdoncini Carvalho (Plant Pathology).\nThe workshop will be highly hands-on and take place across three afternoons:\nWed, Aug 17 - Fri, Aug 19, 2022.\n\nAnyone affiliated with The Ohio State University or Wooster USDA can attend\nAttendance is free\nNo prior experience with coding or genomic data is required\nYou will need to bring a laptop and don’t need to install anything prior to or during the workshop\nWe will work with example genomics data but if you have any, you are also welcome to bring your own data.\n\nSee below for information about the contents of the workshop and to sign up.\nFor questions, please email Jelmer."
  },
  {
    "objectID": "info/about.html#contents-of-the-workshop",
    "href": "info/about.html#contents-of-the-workshop",
    "title": "General Workshop Info & Signing Up",
    "section": "Contents of the workshop",
    "text": "Contents of the workshop\nThe focus of the workshop is on building general skills for analyzing genomics data, such as RNAseq, metabarcoding, metagenomic shotgun sequencing, or whole-genome sequencing. These skills boil down to the ability to write small shell scripts that run command-line programs and submit these scripts to a compute cluster – in our case, at the Ohio Supercomputer Center (OSC).\n\nTopics\n\nIntroduction to the Ohio Supercomputer Center (OSC)\nUsing the VS Code text editor at OSC\nIntroduction to the Unix shell (= the terminal / command line)\nBasics of shell scripts\nSoftware at OSC with modules & Conda\nSubmitting your scripts using the SLURM scheduler\nPutting it all together: practical examples of running analysis jobs at OSC\n\nThe modules will be a mixture of lectures that include “participatory live-coding” (with the instructor slowly demonstrating and participants expected to follow along for themselves) and exercises.\n\n\nSome more background\nCommand-line programs are preferred for many of the steps to analyze genomic sequencing data, such as those involving quality control, trimming or adapter removal, and assembly or mapping. Other features of such datasets are that they tend to contain a lot of data, and that many analysis steps can be done independently for each sample. It therefore pays off -or may be necessary- to run your analyses not on a laptop or desktop, but at a supercomputer like OSC.\nBeing able to run your analysis with command-line programs at OSC involves a number of skills that may seem overwhelming at first. Fortunately, learning the basics of these skills does not take a lot of time, and will enable you to be up-and-running with working on your own genomic data! Keep in mind that these days, excellent programs are available for almost any genomics analysis, so you do not need to be able to code it all up from scratch. You will just need to know how to efficiently run such programs, which is what this workshop aims to teach you."
  },
  {
    "objectID": "info/about.html#sign-up",
    "href": "info/about.html#sign-up",
    "title": "General Workshop Info & Signing Up",
    "section": "Sign up!",
    "text": "Sign up!\nTo apply to attend the workshop, please fill out the form below. There is no real selection procedure: we accept anyone who is at OSU/USDA and signs up before we have reached the maximum number of participants.\n\nLoading…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Command line basics  for genomic analysis at OSC",
    "section": "",
    "text": "Schedule\n\n\n\nDay\nModule\nInstructor\nTime\n\n\n\n\nDay 1\n1. Introduction to the Workshop\nMike / Jelmer\nWed 1:00 - 1:15 pm\n\n\n\n2. The Ohio Supercomputer Center (OSC)\nMike\nWed 1:15 - 2:00 pm\n\n\n\n3. The VS Code Text Editor\nJelmer\nWed 2:00 - 2:15 pm\n\n\n\n4. The Unix Shell (& FASTQ files)\nMike\nWed 2:30 - 4:30 pm\n\n\nDay 2\n5. Variables, Globbing and Loops\nJelmer\nThu 12:00 - 1:00 pm\n\n\n\n6. Shell Scripting\nJelmer\nThu 1:15 - 2:30 pm\n\n\nDay 3\n7. Using Software at OSC\nJelmer\nFri 1:00 - 1:45 pm\n\n\n\n8. Compute Jobs with Slurm\nJelmer\nFri 1:45 - 3:00 pm\n\n\n\n9. Batch Jobs in Practice\nMike / Jelmer\nFri 3:15 - 4:30 pm\n\n\n\n\n\n\n\n\n\nThe schedule is approximate\n\n\n\nStart and end times for every day will be respected, but individual modules may take shorter or longer than indicated below. The instructors will be available for additional questions from about 15 minutes before we start and for about 30 minutes after we end each day."
  },
  {
    "objectID": "modules/08-slurm.html",
    "href": "modules/08-slurm.html",
    "title": "Compute Jobs with Slurm",
    "section": "",
    "text": "We have so far been working on login nodes at OSC, but in order to run some actual analyses, you will need access to compute nodes.\nAutomated scheduling software allows hundreds of people with different requirements to access compute nodes effectively and fairly. For this purpose, OSC uses the Slurm scheduler (Simple linux utility for resource management).\nA temporary reservation of resources on compute nodes is called a compute job. What are the options to start a compute job at OSC?\nWhen running command-line programs for genomics analyses, batch jobs are the most useful and will be the focus of this module. We’ll also touch on interactive shell jobs, which can occasionally be handy and are requested and managed in a very similar way to batch jobs."
  },
  {
    "objectID": "modules/08-slurm.html#setup",
    "href": "modules/08-slurm.html#setup",
    "title": "Compute Jobs with Slurm",
    "section": "1 Setup",
    "text": "1 Setup\n\n\n\n\n\n\nStarting a VS Code session with an active terminal (click here)\n\n\n\n\n\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 4 or more in the box Number of hours\nTo avoid having to switch folders within VS Code, enter /fs/ess/scratch/PAS2250/participants/<your-folder> in the box Working Directory (replace <your-folder> by the actual name of your folder).\nClick Launch.\n\nOn the next page, once the top bar of the box is green and says Runnning, click Connect to VS Code.\nOpen a terminal:    => Terminal => New Terminal.\nIn the terminal, type bash and press Enter.\nType pwd in the termain to check you are in /fs/ess/scratch/PAS2250.\nIf not, click    =>   File   =>   Open Folder and enter /fs/ess/scratch/PAS2250/<your-folder>."
  },
  {
    "objectID": "modules/08-slurm.html#interactive-shell-jobs",
    "href": "modules/08-slurm.html#interactive-shell-jobs",
    "title": "Compute Jobs with Slurm",
    "section": "2 Interactive shell jobs",
    "text": "2 Interactive shell jobs\nInteractive shell jobs will grant you interactive shell access on a compute node. Working in an interactive shell job is operationally identical to working on a login node as we’ve been doing so far, but the difference is that it’s now okay to use significant computing resources. (How much and for how long depends on what you reserve.)\n\n2.1 Using srun\nA couple of different commands can be used to start an interactive shell job. I prefer the general srun command1, which we can use with --pty /bin/bash added to get an interactive Bash shell.\nHowever, if we run that command without additional options, we get an error:\n\nsrun --pty /bin/bash\n\n\nsrun: error: ERROR: Job invalid: Must specify account for job\nsrun: error: Unable to allocate resources: Unspecified error\n\nAs the error message Must specify account for job tries to tell us, we need to indicate which OSC project (or as SLURM puts it, “account”) we want to use for this compute job. This is because an OSC project always has to be charged for the computing resources used during a compute job.\nTo specify the project/account, we can use the --account= option followed by the project number:\n\nsrun --account=PAS2250 --pty /bin/bash\n\n\nsrun: job 12431932 queued and waiting for resources\nsrun: job 12431932 has been allocated resources\n[…regular login info, such as quota, not shown…]\n[jelmer@p0133 PAS2250]$\n\nThere we go! First some Slurm scheduling info was printed to screen:\n\nInitially, the job is “queued”: that is, waiting to start.\nVery soon (usually!), the job is “allocated resources”: that is, computing resources such as a compute node are reserved for the job.\n\nThen:\n\nThe job starts and because we’ve reserved an interactive shell job, a new Bash shell is initiated: for that reason, we get to see our regular login info once again.\nMost importantly, we are no longer on a login node but on a compute node, as our prompt hints at: we switched from something like [jelmer@pitzer-login04 PAS2250]$ to the [jelmer@p0133 PAS2250]$ shown above.\nNote also that the job has a number (above: job 12431932): every compute job has such a unique identifier among all jobs by all users at OSC, and we can use this number to monitor and manage it. All of us will therefore see a different job number pop up.\n\n\n\n\n\n\n\nThe working directory stays the same\n\n\n\nBatch jobs start in the directory that they were submitted from: that is, your working directory remains the same.\n\n\n\n\n\n2.2 Compute job options\nThe --account= option is just one of out of many options we can use when reserving a compute job, but is the only one that always has to be specified (including for batch jobs and for Interactive Apps).\nDefaults exist for all other options, such as the amount of time (1 hour) and the number of cores (1). These options are all specified in the same way for interactive and batch jobs, and we’ll dive into them below.\n\n\n\n\n\n\nQueueing times\n\n\n\nThe “bigger” (more time, more cores, more memory) our job is, the more likely it is that our job will be pending for an appreciable amount of time.\nSmaller jobs (requesting up to a few hours and cores) will almost always start running nearly instantly. Even big jobs (requesting a day or more, 10 or more cores) will often do so, but during busy times, you might have to wait for a while. That said, the only times I’ve had to wait for more than an hour or so was when I was requesting jobs with very large memory requirements (100s of GBs), which have to be submitted to a separate queue/“partition”."
  },
  {
    "objectID": "modules/08-slurm.html#intro-to-batch-jobs",
    "href": "modules/08-slurm.html#intro-to-batch-jobs",
    "title": "Compute Jobs with Slurm",
    "section": "3 Intro to batch jobs",
    "text": "3 Intro to batch jobs\nWhen requesting batch jobs, we are asking the Slurm scheduler to run a script on a compute node.\nIn contrast to interactive shell jobs, we stay in our current shell when submitting a script, and the script will run on a compute node “out of sight”. Also, as we’ll discuss in more detail below:\n\nOutput from the script that would normally be printed to screen ends up in a file (!).\nDespite not being on the same node as our job, we can do things like monitoring whether the job is already/still running, and cancelling the job.\n\n\n\n\n\n\n\nScripts in other languages\n\n\n\nThe script that we submit can be in different languages but typically, including in all examples in this workshop, they are shell (Bash) scripts.\n\n\n\n\n3.1 The sbatch command\nWhereas we used Slurm’s srun command to start an interactive shell job, we use its sbatch command to submit a batch job. Recall from the Bash scripting module that we can run a Bash script as follows:\n\nbash scripts/printname.sh Jane Doe\n\nFirst name: Jane\nLast name: Doe\n\n\n\n\n\n\n\n\nCan’t find yesterday’s printname.sh script?\n\n\n\n\n\n\nOpen a new file in the VS Code editor (     =>   File   =>   New File) and save it as printname.sh\nCopy the code below into the script:\n\n\n#!/bin/bash\nset -ueo pipefail\n\nfirst_name=$1\nlast_name=$2\n  \necho \"First name: $first_name\"\necho \"Last name: $last_name\"\n\n\n\n\nThe above command ran the script on our current node, a login node. To instead submit the script to the Slurm queue, we would start by simply replacing bash by sbatch:\n\nsbatch scripts/printname.sh Jane Doe\n\n\nsrun: error: ERROR: Job invalid: Must specify account for job\nsrun: error: Unable to allocate resources: Unspecified error\n\nAs we’ve learned, we always have to specify the OSC account when submitting a compute job. Conveniently, we can also specify Slurm/sbatch options inside our script, but first, let’s add the --account option on the command line:\n\nsbatch --account=PAS2250 scripts/printname.sh Jane Doe\n\n\nSubmitted batch job 12431935\n\n\n\n\n\n\n\nsbatch options and script arguments\n\n\n\n\n\nNote that we can use sbatch options and script arguments in one command, in the following order:\n\nsbatch [sbatch-options] myscript.sh [script-arguments]\n\nBut both of these are optional:\n\nsbatch printname.sh                             # No options/arguments for either\nsbatch printname.sh Jane Doe                    # Script arguments but no sbatch option\nsbatch --account=PAS2250 printname.sh           # sbatch option but no script arguments\nsbatch --account=PAS2250 printname.sh Jane Doe  # Both sbatch option and script arguments\n\n\n\n\n\n\n\n3.2 Adding sbatch options in scripts\nInstead of specifying Slurm/sbatch options on the command-line when we submit the script, we can also add these options inside the script.\nThis is handy because even though we have so far only seen the account= option, you often want to specify several options. That would lead to very long sbatch commands. Additionally, it can be practical to store a script’s typical Slurm options along with the script itself.\nWe add the options in the script using another type of special comment line akin to the shebang line, marked by #SBATCH. The equivalent of adding --account=PAS2250 after sbatch on the command line is a line in a script that reads #SBATCH --account=PAS2250.\nJust like the shebang line, the #SBATCH line(s) should be at the top of the script. Let’s add one such line to the printname.sh script, such that the first few lines read:\n\n#!/bin/bash\n#SBATCH --account=PAS2250\n\nset -ueo pipefail\n\nAfter having added this to the script, we can run our earlier sbatch command without options:\n\nsbatch printname.sh Jane Doe\n\n\nSubmitted batch job 12431942\n\nAfter we submit the batch job, we immediately get our prompt back. Everything else (job queuing and running) will happen out of our immediate view. This allows us to submit many jobs at the same time — we don’t have to wait for other jobs to finish (or even to start).\n\n\n\n\n\n\nsbatch option precedence\n\n\n\nAny sbatch option provided on the command line will override the equivalent option provided inside the script. This is sensible: we can provide “defaults” inside the script, and change one or more of those when needed on the command line.\n\n\n\n\n\n\n\n\nRunning a script with #SBATCH in other contexts\n\n\n\nBecause #SBATCH lines are special comment lines, they will simply be ignored and not throw any errors when you run a script that contains them in other contexts: when not running them as a batch job at OSC, or even when running them on a computer without Slurm installed.\n\n\n\n\n\n3.3 Where does the output go?\nAbove, we saw that when we ran the printname.sh script directly, its output was printed to the screen, whereas when we submitted it as a batch job, all that was sprinted to screen was Submitted batch job 12431942. So where did our output go?\nOur output ended up in a file called slurm-12431942.out: that is, slurm-<job-number>.out. Since each job number is unique to a given job, your file would have a different number in its name. We might call this type of file a Slurm log file.\n\n\n\n\n\n\nAny idea why we might not want batch job output printed to screen, even if we could?\n\n\n\n\n\nThe power of submitting batch jobs is that you can submit many at once — e.g. one per sample, running the same script. If the output from all those scripts ends up on your screen, things become a big mess, and you have no lasting record of what happened.\n\n\n\nLet’s take a look at the contents of the Slurm log file with the cat command:\n\ncat slurm-12431942.out\n\n\nFirst name: Jane\nLast name: Doe\n\nThis file simply contains the output that we saw printed to screen before — nothing more and nothing less.\nIt’s important to conceptually distinguish two broad types of output that a script may have:\n\nOutput that is printed to screen when we directly run a script, such as what was produced by our echo statements, by any errors that may occur, and possibly by a program that we run in the script.2 As we saw, this output ends up in the Slurm log file when we submit the script as a batch job.\nOutput that we redirect to a file (> myfile.txt) or output that a program we run in the script writes to file(s). This type of output will always end up in those very same files regardless of whether we run the script directly or as a batch job."
  },
  {
    "objectID": "modules/08-slurm.html#monitoring-batch-and-other-compute-jobs",
    "href": "modules/08-slurm.html#monitoring-batch-and-other-compute-jobs",
    "title": "Compute Jobs with Slurm",
    "section": "4 Monitoring batch (and other compute) jobs",
    "text": "4 Monitoring batch (and other compute) jobs\n\n4.1 A sleepy script for practice\nLet’s use the following short script to practice monitoring and managing batch and other compute jobs.\nOpen a new file in the VS Code editor (     =>   File   =>   New File) and save it as scripts/sleep.sh, then copy the following into it:\n\n#!/bin/bash\n#SBATCH --account=PAS2250\n\necho \"I will sleep for 30 seconds\" > sleep.txt\nsleep 30s\necho \"I'm awake!\"\n\n\n\nOn Your Own: Batch job output recap\nIf you submit the script as a batch job using sbatch scripts/sleep.sh:\n\nHow many output files will this batch job produce?\nWhat will be in it/them?\nIn which directory will the file(s) appear?\nIn terms of output, what would have been different if we had run the script directly, i.e. using the command bash scripts/sleep.sh?\n\nYou can test your predictions by running the script, if you want.\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nThe script will produce 2 files\nThey will contain:\n\nsleep.txt: I will sleep for 30 seconds\nslurm-<job-number>.out: I'm awake!\n\nBoth files will end up in your current working directory.\nIf we had run the script directly, slept.txt would have been the same, but All done! would have been printed to screen.\n\n\n\n\n\n\n\n4.2 Checking the status of our batch job\nAfter we submit a job, it may be initially be queued (or pending), before the Slurm scheduler finds a “slot” for our job. Then, the job will start running, and at some point it will stop running, either because the script ran into and error or because it ran to completion.\nHow can we check the status of our batch job? We can do so using the Slurm command squeue:\n\nsqueue -u $USER -l\n\nIn the command above:\n\nOur user name is specified with the -u option (otherwise we would see everyone’s jobs) —\nWe use the environment variable $USER, which is a variable that’s always available and contains your user name, so that the very same code will work for everyone (you can also simply type your user name if that’s shorter or easier).\nWe’ve added the -l option to get more verbose output.\n\nLet’s try that — first we submit the script:\n\nsbatch scripts/sleep.sh\n\n\nSubmitted batch job 12431945\n\nWe may be able to catch the STATE being PENDING before the job starts:\nsqueue -u $USER -l\n# Fri Aug 19 07:23:19 2022\n#              JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n#           12520046 serial-40 sleep.sh   jelmer  PENDING       0:00   1:00:00      1 (None)\nBut soon enough it should say RUNNING in the STATE column:\nsqueue -u $USER -l\n# Fri Aug 19 07:23:45 2022\n#              JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n#           12520046 condo-osu sleep.sh   jelmer  RUNNING       0:12   1:00:00      1 p0133\nThe script should finish after 30 seconds (sleep 30s…), and after that, the squeue output will only show the header line with column names:\n\nsqueue -u $USER -l\n# Fri Aug 19 07:24:18 2022\n#              JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON) \n\nOnce a job has finished running, it disappears from the squeue listing. So, the output above means that we have no running (or pending) jobs.\nBut we need to check our output file(s) to see if our script ran successfully!\n\ncat sleep.txt\n\n\nI will sleep for 30 seconds\n\n\ncat slurm-12520046.out\n\n\nI’m awake!\n\n\n\n\n4.3 Cancelling jobs (and other monitoring/managing commands)\nSometimes, you want to cancel one or more jobs, because you realize you made a mistake in the script or you used the wrong input files. You can do so using scancel:\n\nscancel 2979968        # Cancel job number 2979968\nscancel -u $USER       # Cancel all your jobs\n\n\n\n\n\n\n\nAt-home reading: Other commands and options\n\n\n\n\n\n\nCheck only a specific job by specifying the job ID, e.g 2979968:\n\nsqueue -j 2979968\n\nOnly show running (not pending) jobs:\n\nsqueue -u $USER -t RUNNING\n\nUpdate Slurm directives for a job that has already been submitted:\n\nscontrol update job=<jobID> timeLimit=5:00:00\n\nHold and release a pending (queued) job, e.g. when needing to update input file before it starts running:\n\nscontrol hold <jobID>        # Job won't start running until released\nscontrol release <jobID>     # Job is free to start\n\nYou can see more details about any running or finished jobs, including the amount of time it ran for:\n\nscontrol show job 2526085   # For job 2526085\n\n# UserId=jelmer(33227) GroupId=PAS0471(3773) MCS_label=N/A\n# Priority=200005206 Nice=0 Account=pas0471 QOS=pitzer-default\n# JobState=RUNNING Reason=None Dependency=(null)\n# Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n# RunTime=00:02:00 TimeLimit=01:00:00 TimeMin=N/A\n# SubmitTime=2020-12-14T14:32:44 EligibleTime=2020-12-14T14:32:44\n# AccrueTime=2020-12-14T14:32:44\n# StartTime=2020-12-14T14:32:47 EndTime=2020-12-14T15:32:47 Deadline=N/A\n# SuspendTime=None SecsPreSuspend=0 LastSchedEval=2020-12-14T14:32:47\n# Partition=serial-40core AllocNode:Sid=pitzer-login01:57954\n# [...]"
  },
  {
    "objectID": "modules/08-slurm.html#common-sbatch-options",
    "href": "modules/08-slurm.html#common-sbatch-options",
    "title": "Compute Jobs with Slurm",
    "section": "5 Common sbatch options",
    "text": "5 Common sbatch options\n\n\n\n\n\n\nLong and and short option format\n\n\n\nMany SLURM options have a long format (--account=PAS2250) and a short format (-A PAS2250), which can generally be used interchangeably. For clarity, we’ll stick to long format options during this workshop.\n\n\n\n5.1 --account: The OSC project\nAs seen above. Always specify the project when submitting a batch job.\n\n\n5.2 --time: Time limit (“wall time”)\nSpecify the maximum amount of time your job will run for. Wall time is a term meant to distinguish it from, say “core hours”: if a job runs for 2 hour and used 8 cores, the wall time was 2 hours and the number of core hours was 2 x 8 = 16.\n\nYour job gets killed as soon as it hits the specified time limit!\nYou will only be charged for the time your job actually used.\nThe default time limit is 1 hour. Acceptable time formats include:\n\nminutes\nhours:minutes:seconds\ndays-hours\n\nFor single-node jobs, up to 168 hours (7 days) can be requested. If that’s not enough, you can request access to the longserial queue for jobs of up to 336 hours (14 days).\n\n\n#!/bin/bash\n#SBATCH --time=1:00:00\n\n\n\n\n\n\n\nAsk for more time\n\n\n\nIf you are uncertain about the time your job will take, ask for (much) more time than you think you will need. This is because queuing times are generally good at OSC and you won’t be charged for reserved-but-not-used time.\n\n\n\n\n5.3 --mem: RAM memory\nSpecify a maximum amount of RAM (Random Access Memory) that your job can use.\n\nThe default unit is MB (MegaBytes) — append G for GB.\nThe default amount is 4 GB per core that you reserve\nLike with the time limit, your job gets killed when it hits the memory limit.\n\n#!/bin/bash\n#SBATCH --mem=20G\n\n\n\n\n\n\nDefault memory limits usually work\n\n\n\nIt is not that common to hit the memory limit, so I usually don’t specify it — unless the program reports needing lots of memory, or I got “out-of-memory” errors when trying to run the script before.\n\n\n\n\n5.4 Cores (& nodes and tasks)\nSpecify the number of nodes (≈ computers), cores, or “tasks” (processes). These are separate but related options, and this is where things can get confusing!\n\nSlurm for the most part uses “core” and “CPU” interchangeably3. More generally, “thread” is also commonly used interchangeably with core/CPU4.\n\n\nRunning a program that uses multiple threads/cores/CPUs (“multi-threading”) is common. In such cases, specify the number of threads/cores/CPUs n with --cpus-per-task=n (and keep --nodes and --ntasks at their defaults of 1).\nThe program you’re running may have an argument like --cores or --threads, which you should then set to n as well.\n\n\n\n\n\n\n\nUncommon cases\n\n\n\n\nOnly ask for >1 node when a program is parallelized with e.g. “MPI”, which is uncommon in bioinformatics.\nFor jobs with multiple processes (tasks), use --ntasks=n or --ntasks-per-node=n.\n\n\n\n\n\n\n\n\n\n\n\n\nResource/use\nshort\nlong\ndefault\n\n\n\n\nNr. of cores/CPUs/threads (per task)\n-c 1\n--cpus-per-task=1\n1\n\n\nNr. of “tasks” (processes)\n-n 1\n--ntasks=1\n1\n\n\nNr. of tasks per node\n-\n--ntasks-per-node=1\n1\n\n\nNr. of nodes\n-N 1\n--nodes=1\n1\n\n\n\n\n#!/bin/bash\n#SBATCH --cpus-per-task=2\n\n\n\n5.5 --output: Slurm log files\nAs we saw above, by default, all output from a script that would normally5 be printed to screen will end up in a Slurm log file when we submit the script as a batch job. This file will be created in the directory from which you submitted the script, and will be called slurm-<job-number>.out, e.g. slurm-12431942.out.\nBut it is possible to change the name of this file. For instance, it can be useful to include the name of the program that the script runs, so that it’s easier to recognize this file later.\nWe can do this with the --output option, e.g. --output=slurm-fastqc.out if we were running FastQC.\nHowever, you’ll generally want to keep the batch job number in the file name too6. Since we won’t know the batch job number in advance, we need a trick here and that is to use %j, which represents the batch job number:\n\n#!/bin/bash\n#SBATCH --output=slurm-fastqc-%j.out\n\n\n\n\n\n\n\nAt-home reading: stdout and stderr\n\n\n\n\n\nBy default, two output streams “standard output” (stdout) and “standard error” (stderr) are printed to screen and therefore also both end up in the same Slurm log file, but it is possible to separate them into different files.\nBecause stderr, as you might have guessed, often contains error messages, it could be useful to have those in a separate file. You can make that happen with the --error option, e.g. --error=slurm-fastqc-%j.err.\nHowever, reality is more messy: some programs print their main output not to a file but to standard out, and their logging output, errors and regular messages alike, to standard error. Yet other programs use stdout or stderr for all messages.\nI therefore usually only specify --output, such that both streams end up in that file."
  },
  {
    "objectID": "modules/08-slurm.html#addendum-table-with-sbatch-options",
    "href": "modules/08-slurm.html#addendum-table-with-sbatch-options",
    "title": "Compute Jobs with Slurm",
    "section": "6 Addendum: Table with sbatch options",
    "text": "6 Addendum: Table with sbatch options\nThis includes all the discussed options, and a couple more useful ones:\n\n\n\n\n\n\n\n\n\nResource/use\nshort\nlong\ndefault\n\n\n\n\nProject to be billed\n-A PAS0471\n--account=PAS0471\nN/A\n\n\nTime limit\n-t 4:00:00\n--time=4:00:00\n1:00:00\n\n\nNr of nodes\n-N 1\n--nodes=1\n1\n\n\nNr of cores\n-c 1\n--cpus-per-task=1\n1\n\n\nNr of “tasks” (processes)\n-n 1\n--ntasks=1\n1\n\n\nNr of tasks per node\n-\n--ntasks-per-node\n1\n\n\nMemory limit per node\n-\n--mem=4G\n(4G)\n\n\nLog output file (%j = job number)\n-o\n--output=slurm-fastqc-%j.out\n\n\n\nError output (stderr)\n-e\n--error=slurm-fastqc-%j.err\n\n\n\nJob name (displayed in the queue)\n-\n--job-name=fastqc\n\n\n\nPartition (=queue type)\n-\n--partition=longserial  --partition=hugemem\n\n\n\nGet email when job starts, ends, fails,  or all of the above\n-\n--mail-type=START  --mail-type=END  --mail-type=FAIL  --mail-type=ALL\n\n\n\nLet job begin at/after specific time\n-\n--begin=2021-02-01T12:00:00\n\n\n\nLet job begin after other job is done\n-\n--dependency=afterany:123456"
  },
  {
    "objectID": "modules/06-scripts.html",
    "href": "modules/06-scripts.html",
    "title": "Shell Scripting",
    "section": "",
    "text": "Shell scripts (or to be slightly more precise, Bash scripts) enable us to run sets of commands non-interactively. This is especially beneficial or necessary when a set of commands:\nScripts form the basis for analysis pipelines and if we code things cleverly, it should be straightforward to rerun much of our project workflow:"
  },
  {
    "objectID": "modules/06-scripts.html#setup",
    "href": "modules/06-scripts.html#setup",
    "title": "Shell Scripting",
    "section": "1 Setup",
    "text": "1 Setup\n\n\n\n\n\n\nStarting a VS Code session with an active terminal (click here)\n\n\n\n\n\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 4 or more in the box Number of hours\nTo avoid having to switch folders within VS Code, enter /fs/ess/scratch/PAS2250/participants/<your-folder> in the box Working Directory (replace <your-folder> by the actual name of your folder).\nClick Launch.\n\nOn the next page, once the top bar of the box is green and says Runnning, click Connect to VS Code.\nOpen a terminal:    => Terminal => New Terminal.\nIn the terminal, type bash and press Enter.\nType pwd in the termain to check you are in /fs/ess/scratch/PAS2250.\nIf not, click    =>   File   =>   Open Folder and enter /fs/ess/scratch/PAS2250/<your-folder>."
  },
  {
    "objectID": "modules/06-scripts.html#script-header-lines-and-zombie-scripts",
    "href": "modules/06-scripts.html#script-header-lines-and-zombie-scripts",
    "title": "Shell Scripting",
    "section": "2 Script header lines and zombie scripts",
    "text": "2 Script header lines and zombie scripts\n\n2.1 Shebang line\nWe use a so-called “shebang” line as the first line of a script to indicate which language our script uses. More specifically, this line tell the computer where to find the binary (executable) that will run our script.\nSuch a line starts with #!, basically marking it as a special type of comment. After that, we provide the location to the relevant program: in our case Bash, which is located at /bin/bash on Linux and Mac computers.\n#!/bin/bash\nAdding a shebang line is good practice in general, and is necessary when we want to submit our script to OSC’s Slurm queue, which we’ll do tomorrow.\n\n\n\n2.2 Bash script settings\nAnother line that is good practice to add to your Bash scripts changes some default settings to safer alternatives. The following two Bash default settings are bad ideas inside scripts:\nFirst, and as we’ve seen in the previous module, Bash does not complain when you reference a variable that does not exist (in other words, it does not consider that an error).\nIn scripts, this can lead to all sorts of downstream problems, because you very likely tried and failed to do something with an actual variable. Even more problematically, it can lead to potentially very destructive file removal:\n\n# Using a variable, we try to remove some temporary files whose names start with tmp_\ntemp_prefix=\"temp_\"\nrm \"$tmp_prefix\"*     # DON'T TRY THIS!\n\n\n# Using a variable, we try to remove a temporary directory\ntempdir=output/tmp\nrm -rf $tmpdir/*      # DON'T TRY THIS!\n\n\n\n\n\n\n\nThe comments above specified the intent we had. What would have actually happened?\n\n\n\n\n\nIn both examples, there is a similar typo: temp vs. tmp, which means that we are referencing a (likely) non-existent variable.\n\nIn the first example, rm \"$tmp_prefix\"* would have been interpreted as rm *, because the non-existent variable is simply ignored. Therefore, we would have removed all files in the current working directory.\nIn the second example, along similar lines, rm -rf $tmpdir/* would have been interpreted as rm -rf /*. Horrifyingly, this would attempt to remove the entire filesystem (recall that a leading / in a path is a computer’s root directory).1 (-r makes the removal recursive and -f makes forces removal).\n\n\n\n\n\nSecond, a Bash script keeps running after encountering errors. That is, if an error is encountered when running line 2 of a script, any remaining lines in the script will nevertheless be executed.\nIn the best case, this is a waste of computer resources, and in worse cases, it can lead to all kinds of unintended consequences. Additionally, if your script prints a lot of output, you might not notice an error somewhere in the middle if it doesn’t produce more errors downstream. But the downstream results from what we at that point might call a “zombie script” may still be completely wrong.\n\nThe following three settings will make your Bash scripts more robust and safer. With these settings, the script terminates, with an appropriate error message, if:\n\nset -u — An unset (non-existent) variable is referenced.\nset -e — Almost any error occurs.\nset -o pipefail — An error occurs in a shell “pipeline” (e.g., sort | uniq).\n\nWe can change all of these settings in one line in a script:\n\nset -u -e -o pipefail     # (For in a script - don't run in the terminal)\n\nOr even more concisely:\n\nset -ueo pipefail         # (For in a script - don't run in the terminal)\n\n\n\n\n2.3 Our header lines as a rudimentary script\nLet’s go ahead and start a script with the header lines that we have so far discussed.\n\nInside your personal directory within /fs/ess/scratch/PAS2250/participants, make a directory called scripts and one called sandbox (e.g. mkdir scripts sandbox, or use the VS Code menus.\nOpen a new file in the VS Code editor (     =>   File   =>   New File) and save it as printname.sh within the newly created scripts dir.\n\n\n\n\n\n\nShell scripts, including Bash scripts, most commonly have the extension .sh\n\n\n\n\n\n\nType the following lines in that script (not in your terminal!):\n\n#!/bin/bash\nset -ueo pipefail\n\n\nAlready now, we could run (execute) the script. One way of doing this is calling the bash command followed by the name of the script2:\n\nbash scripts/printname.sh\n\nDoing this won’t print anything to screen (or file). Since our script doesn’t have any output, that makes sense — no output can be a good sign, because it means that no errors were encountered."
  },
  {
    "objectID": "modules/06-scripts.html#command-line-arguments-for-scripts",
    "href": "modules/06-scripts.html#command-line-arguments-for-scripts",
    "title": "Shell Scripting",
    "section": "3 Command-line arguments for scripts",
    "text": "3 Command-line arguments for scripts\n\n3.1 Calling a script with arguments\nWhen you call a script, you can pass it command-line arguments, such as a file to operate on.\nThis is much like when you provide a command like ls with arguments:\n\n# Run ls without arguments:\nls\n\n# Pass 1 filename as an argument to ls:\nls data/sampleA.fastq.gz\n\n# Pass 2 filenames as arguments to ls, separated by spaces:\nls data/sampleA.fastq.gz data/sampleB.fastq.gz\n\nLet’s see what this would look like with our printname.sh script and a fictional script fastqc.sh:\n\n# Run scripts without any arguments:\nbash fastqc.sh                            # (Fictional script)\nbash scripts/printname.sh\n\n# Run scripts with 1 or 2 arguments:\nbash fastqc.sh data/sampleA.fastq.gz      # 1 argument, a filename\nbash scripts/printname.sh John Doe        # 2 arguments, strings representing names\n\nIn the next section, we’ll see what happens when we pass arguments to a script on the command line.\n\n\n\n3.2 Placeholder variables\nInside the script, any command-line arguments are automatically available in placeholder variables.\nA first argument will be assigned to the variable $1, any second argument will be assigned to $2, any third argument will be assigned to $3, and so on.\n\n\n\n\n\n\nIn the calls to fastqc.sh and printname.sh above, what are the placeholder variables and their values?\n\n\n\n\n\nIn bash fastqc.sh data/sampleA.fastq.gz, a single argument, data/sampleA.fastq.gz, is passed to the script, and will be assigned to $1.\nIn bash scripts/printname.sh John Doe, two arguments are passed to the script: the first one (John) will be stored in $1, and the second one (Doe) in $2.\n\n\n\n\n\n\n\n\n\nPlaceholder variables are not automagically used\n\n\n\nArguments passed to a script are merely made available in placeholder variables — unless we explicitly include code in the script to do something with those variables, nothing else happens.\n\n\nLet’s add code to our printname.sh script to “process” any first and last name that are passed to the script as command-line arguments. For now, our script will simply echo the placeholder variables, so that we can see what happens:\n\n#!/bin/bash\nset -ueo pipefail\n\necho \"First name: $1\"\necho \"Last name: $2\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\n\n\n\nNext, we’ll run the script, passing the arguments John and Doe:\n\nbash scripts/printname.sh John Doe\n\nFirst name: John\nLast name: Doe\n\n\n\n\nOn Your Own: Command-line arguments\nIn each case below, think about what might happen before you run the script. Then, run it, and if you didn’t make a successful prediction, try to figure out what happened instead.\n\nRun the script (scripts/printname.sh) without passing arguments to it.\nDeactivate (“comment out”) the line with set settings by inserting a # as the first character. Then, run the script again without passing arguments to it.\nDouble-quote John Doe when you run the script, i.e. run bash scripts/printname.sh \"John Doe\"\n\nTo get back to where we were, remove the # you inserted in the script in step 2 above.\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nThe script will error out because we are referencing variables that don’t exist: since we didn’t pass command-line arguments to the script, the $1 and $2 have not been set.\n\n\nbash scripts/printname.sh\n\n\nprintname.sh: line 4: $1: unbound variable\n\n\nThe script will run in its entirety and not throw any errors, because we are now using default Bash settings such that referencing non-existent variables does not throw an error. Of course, no names are printed either, since we didn’t specify any:\n\n\n\n\n\nbash scripts/printname.sh\n\nFirst name: \nLast name: \n\n\nBeing commented out, the set line should read:\n\n#set -ueo pipefail\n\n\nBecause we are quoting \"John Doe\", both names are passed as a single argument and both names end up in $1, the “first name”:\n\n\nbash scripts/printname.sh \"John Doe\"\n\nFirst name: John Doe\nLast name: \n\n\n\n\n\n\n\n\n\n\n\n3.3 Descriptive variable names\nWhile you can use the $1-style placeholder variables throughout your script, I find it very useful to copy them to more descriptively named variables as follows:\n\n#!/bin/bash\nset -ueo pipefail\n\nfirst_name=$1\nlast_name=$2\n  \necho \"First name: $first_name\"\necho \"Last name: $last_name\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\nUsing descriptively named variables in your scripts has several advantages. It will make your script easier to understand for others and for yourself. It will also make it less likely that you make errors in your script in which you use the wrong variable in the wrong place.\n\n\n\n\n\n\nOther variables that are automatically available inside scripts\n\n\n\n\n$0 contains the script’s file name\n$# contains the number of command-line arguments passed\n\n\n\n\n\nOn Your Own: A script to print a specific line\nWrite a script that prints a specific line (identified by line number) from a file.\n\nOpen a new file and save it as scripts/printline.sh\nStart with the shebang and set lines\nYour script takes two arguments: a file name ($1) and a line number ($2)\nCopy the $1 and $2 variables to descriptively named variables\nTo print a specific line, think how you might combine head and tail to do this. If you’re at a loss, feel free to check out the top solution box.\nTest the script by printing line 4 from data/meta/meta.tsv.\n\n\n\n\n\n\n\nSolution: how to print a specific line number\n\n\n\n\n\nFor example, to print line 4 of data/meta/meta.tsv directly:\n\nhead -n 4 data/meta/meta.tsv | tail -n 1\n\nJust note that in the script, you’ll be using variables instead of the “hardcode values” 4 and data/meta/meta.tsv.\nHow this command works:\n\nhead -n 4 data/meta/meta.tsv will print the first 4 lines of data/meta/meta.tsv\nWe pipe those 4 lines into the tail command\nWe ask tail to just print the last line of its input, which will in this case be line 4 of the original input file.\n\n\n\n\n\n\n\n\n\n\nFull solution\n\n\n\n\n\n\n#!/bin/bash\nset -ueo pipefail\n  \ninput_file=$1\nline_nr=$2\n\nhead -n \"$line_nr\" \"$input_file\" | tail -n 1\n\n\n\n\nTo run the script and make it print the 4th line of meta.tsv:\n\nbash scripts/printline.sh data/meta/meta.tsv 4\n\nSRR7609471  beach   control 3   40982374    78.70"
  },
  {
    "objectID": "modules/06-scripts.html#script-variations-and-improvements",
    "href": "modules/06-scripts.html#script-variations-and-improvements",
    "title": "Shell Scripting",
    "section": "4 Script variations and improvements",
    "text": "4 Script variations and improvements\n\n4.1 A script to serve as a starting point\nWe’ve learned that the head command prints the first lines of a file, whereas the tail command prints the last lines. Sometimes it’s nice to be able to quickly see both ends of a file, so let’s write a little script that can do that for us, as a starting point for the next few modifications.\nOpen a new file, save it as scripts/headtail.sh, and add the following code to it:\n\n#!/bin/bash\nset -ueo pipefail\n\ninput_file=$1\n\nhead -n 2 \"$input_file\"\necho \"---\"\ntail -n 2 \"$input_file\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\n\n\n\nNext, let’s run our headtail.sh script:\n\nbash scripts/headtail.sh data/meta/meta.tsv\n\naccession   location    treatment   replicate   nreads_raw  pct_mapped\nSRR7609473  beach   control 1   45285752    76.01\n---\nSRR7609467  inland  treatment   2   47303936    79.25\nSRR7609474  inland  treatment   3   55728624    78.80\n\n\n\n\n\n4.2 Redirecting output to a file\nSo far, the output of our scripts was printed to screen, e.g.:\n\nIn printnames.sh, we simply echo’d, inside sentences, the arguments passed to the script.\nIn headtail.sh, we printed the first and last few lines of a file.\n\nAll this output was printed to screen because that is the default output mode of Unix commands, and this works the same way regardless of whether those commands are run directly on the command line, or are run inside a script.\nAlong those same lines, we have already learned that we can “redirect” output to a file using > (write/overwrite) and >> (append) when we run shell commands — and this, too, works exactly the same way inside a script.\n\nWhen working with genomics data, we commonly have files as input, and new/modified files as output. Let’s practice with this and modify our headtail.sh script so that it writes output to a file.\nWe’ll make the following changes:\n\nWe will have the script accept a second argument: the output file name3.\nWe will redirect the output of our head, echo, and tail commands to the output file. We’ll have to append (using >>) in the last two commands.\n\n\n#!/bin/bash\nset -ueo pipefail\n\ninput_file=$1\noutput_file=$2\n\nhead -n 2 \"$input_file\" > \"$output_file\"\necho \"---\" >> \"$output_file\"\ntail -n 2 \"$input_file\" >> \"$output_file\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\n\n\n\nNow we run the script again, this time also passing the name of an output file:\n\nbash scripts/headtail.sh data/meta/meta.tsv sandbox/samples_headtail.txt\n\nThe script will no longer print any output to screen, and our output should instead be in sandbox/samples_headtail.txt:\n\n# Check that the file exists and was just modified:\nls -lh sandbox/samples_headtail.txt\n\n-rw-rw-r-- 1 jelmer jelmer 197 Aug 19 23:00 sandbox/samples_headtail.txt\n\n\n\n# Print the contents of the file to screen\ncat sandbox/samples_headtail.txt\n\naccession   location    treatment   replicate   nreads_raw  pct_mapped\nSRR7609473  beach   control 1   45285752    76.01\n---\nSRR7609467  inland  treatment   2   47303936    79.25\nSRR7609474  inland  treatment   3   55728624    78.80\n\n\n\n\n\n4.3 Report what’s happening\nIt is often useful to have your scripts “report” or “log” what is going on. Let’s keep thinking about a script that has file(s) as the main output, but instead of having no output printed to screen at all, we’ll print some logging output to screen. For instance:\n\nWhat is the date and time\nWhich arguments were passed to the script\nWhat are the output files\nPerhaps even summaries of the output.\n\nAll of this can help with troubleshooting and record-keeping.4 Let’s try this with our headtail.sh script.\n\n#!/bin/bash\nset -ueo pipefail\n\n## Copy placeholder variables\ninput_file=$1\noutput_file=$2\n\n## Initial logging \necho \"Starting script $0\"           # Print name of script\ndate                                # Print date & time\necho \"Input file:   $input_file\"\necho \"Output file:  $output_file\" \necho                                # Print empty line to separate initial & final logging\n\n## Print the first and last two lines to a separate file\nhead -n 2 \"$input_file\" > \"$output_file\"\necho \"---\" >> \"$output_file\"\ntail -n 2 \"$input_file\" >> \"$output_file\"\n\n## Final logging\necho \"Listing the output file:\"\nls -lh \"$output_file\"\necho \"Done with script $0\"\ndate\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\nA couple of notes about the lines that were added to the script above:\n\nPrinting the date at the end of the script as well will allow you to check for how long the script ran, which can be informative for longer-running scripts.\nPrinting the input and output files (and the command-line arguments more generally) can be particularly useful for troubleshooting\nWe printed a “marker line” like Done with script, indicating that the end of the script was reached. This is handy due to our set settings: seeing this line printed means that no errors were encountered.\nI also added some comment headers like “Initial logging” to make the script easier to read, and such comments can be made more extensive to really explain what is being done.\n\n\n\n\nLet’s run the script again:\n\nbash scripts/headtail.sh data/meta/meta.tsv sandbox/tmp.txt\n\nStarting script scripts/headtail.sh\nFri Aug 19 11:00:45 PM CEST 2022\nInput file:   data/meta/meta.tsv\nOutput file:  sandbox/tmp.txt\n\nListing the output file:\n-rw-rw-r-- 1 jelmer jelmer 197 Aug 19 23:00 sandbox/tmp.txt\nDone with script scripts/headtail.sh\nFri Aug 19 11:00:45 PM CEST 2022\n\n\nThe script printed some details for the output file, but not its contents (that would have worked here, but is usually not sensible when working with genomics data). Let’s take a look, though, to make sure the script worked:\n\ncat sandbox/tmp.txt      # \"cat\" prints all of a file's contents\n\naccession   location    treatment   replicate   nreads_raw  pct_mapped\nSRR7609473  beach   control 1   45285752    76.01\n---\nSRR7609467  inland  treatment   2   47303936    79.25\nSRR7609474  inland  treatment   3   55728624    78.80\n\n\n\n\n\n\n\n\necho, echo\n\n\n\nThe extensive reporting (echo-ing) may have seemed silly for our little script, but fairly extensive reporting (as well as testing, but that’s outside the scope of this workshop) can be very useful — and will be eventually a time-saver.\nThis is especially true for long-running scripts, or scripts that you often reuse and perhaps share with others.\n\n\n\n\nOn Your Own: A fanciful script\nModify your printline.sh script to:\n\nRedirect output to a file\nThis output file should not be “hardcoded” in the script, but its name should be passed as an argument to the script, like we did above with headtail.sh\nAdd a bit of reporting — echo statements, date, etc, along the lines of what we did above with headtail.sh\nAdd some comments to describe what the code in the script is doing\n\n\n\n\n\n\n\nThe original printline.sh script\n\n\n\n\n\n\n\n\n#!/bin/bash\nset -ueo pipefail\n  \ninput_file=$1\nline_nr=$2\n\nhead -n \"$line_nr\" \"$input_file\" | tail -n 1\n\n\n\n\n\n\n\n\n\n\n\n(One possible) solution\n\n\n\n\n\n\n\n\n\n#!/bin/bash\nset -ueo pipefail\n\n## Copy placeholder variables\ninput_file=$1\noutput_file=$2\nline_nr=$3\n\n## Initial logging \necho \"Starting script $0\"           # Print name of script\ndate                                # Print date & time\necho \"Input file:   $input_file\"\necho \"Output file:  $output_file\"\necho \"Line number:  $line_nr\"\necho                                # Print empty line to separate initial & final logging\n\n## Print 1 specific line from the input file and redirect to an output file\nhead -n \"$line_nr\" \"$input_file\" | tail -n 1 > $output_file\n\n## Final logging\necho \"Listing the output file:\"\nls -lh \"$output_file\"\necho \"Done with script $0\"\ndate\n\nTo run the script with the additional argument:\n\nbash scripts/printline.sh data/meta/meta.tsv sandbox/meta_line.tsv 4\n\nStarting script scripts/printline.sh\nFri Aug 19 11:00:45 PM CEST 2022\nInput file:   data/meta/meta.tsv\nOutput file:  sandbox/meta_line.tsv\nLine number:  4\n\nListing the output file:\n-rw-rw-r-- 1 jelmer jelmer 42 Aug 19 23:00 sandbox/meta_line.tsv\nDone with script scripts/printline.sh\nFri Aug 19 11:00:45 PM CEST 2022"
  },
  {
    "objectID": "modules/09-examples.html",
    "href": "modules/09-examples.html",
    "title": "Batch Jobs in Practice",
    "section": "",
    "text": "So far, we have covered all the building blocks to be able to run command-line programs at OSC:\nWith these skills, it’s relatively straightforward to create and submit scripts to run most command-line programs that can analyze our genomics data.\nOf course, how straightforward this exactly is depends on the ease of use of the programs you need to run, but that will be true in general whenever you learn a new approach and the associated software."
  },
  {
    "objectID": "modules/09-examples.html#setup",
    "href": "modules/09-examples.html#setup",
    "title": "Batch Jobs in Practice",
    "section": "1 Setup",
    "text": "1 Setup\n\n\n\n\n\n\nStarting a VS Code session with an active terminal (click here)\n\n\n\n\n\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 4 or more in the box Number of hours\nTo avoid having to switch folders within VS Code, enter /fs/ess/scratch/PAS2250/participants/<your-folder> in the box Working Directory (replace <your-folder> by the actual name of your folder).\nClick Launch.\n\nOn the next page, once the top bar of the box is green and says Runnning, click Connect to VS Code.\nOpen a terminal:    => Terminal => New Terminal.\nIn the terminal, type bash and press Enter.\nType pwd in the termain to check you are in /fs/ess/scratch/PAS2250.\nIf not, click    =>   File   =>   Open Folder and enter /fs/ess/scratch/PAS2250/<your-folder>."
  },
  {
    "objectID": "modules/09-examples.html#worked-example-part-i-a-script-to-run-fastqc",
    "href": "modules/09-examples.html#worked-example-part-i-a-script-to-run-fastqc",
    "title": "Batch Jobs in Practice",
    "section": "2 Worked example, part I: A script to run FastQC",
    "text": "2 Worked example, part I: A script to run FastQC\n\n2.1 FastQC: A program for quality control of FASTQ files\nFastQC is perhaps the most ubiquitous genomics software. It produces visualizations and assessments of FASTQ files for statistics such as per-base quality (below) and adapter content. Running FastQC should, at least for Illumina data, almost always be the first analysis step after receiving your sequences.\nFor each FASTQ file, FastQC outputs an HTML file that you can open in your browser and which has about a dozen graphs showing different QC metrics. The most important one is the per-base quality score graph shown below.\n\n\n\n\n\n\nA FastQC quality score graph for decent-quality reads\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA FastQC quality score graph for poor-quality reads\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2 FastQC syntax\nTo analyze one optionally gzipped FASTQ file with FastQC, the syntax is simply:\n\nfastqc <fastq-file>\n\nOr if we wanted to specify the output directory (otherwise, output files end up in the current working directory):\n\nfastqc --outdir=<output-dir> <fastq-file>\n\nFor instance, if we wanted output files to go to the directory results/fastqc and wanted the program to analyze the file data/fastq/SRR7609467.fastq.gz, a functional command would like like this:\n\nfastqc --outdir=results/fastqc data/fastq/SRR7609467.fastq.gz\n\n\n\n\n\n\n\nFastQC’s output file names are automatically determined\n\n\n\nWe can specify the output directory, but not the actual file names, which will be automatically determined by FastQC based on the input file name.\nFor one FASTQ file, it will output one HTML file and one ZIP archive. The latter contains files with the summary statistics that were computed and on which the figures are based — we generally don’t need to look at that.\n\n\n\n\n\n2.3 A basic FastQC script\nHere is what a basic script to run FastQC could look like:\n\n#!/bin/bash\n\n## Bash strict settings\nset -euo pipefail\n\n## Copy the placeholder variables\ninput_file=\"$1\"\noutput_dir=\"$2\" \n\n## Run FastQC\nfastqc --outdir=\"$output_dir\" \"$input_file\"\n\nBut we’ll add a few things to to run this script smoothly as a batch job at OSC:\n\nWe load the relevant OSC module:\n\nmodule load fastqc/0.11.8\n\nWe add a few sbatch options:\n\n#SBATCH --account=PAS2250\n#SBATCH --output=slurm-fastqc-%j.out\n\n\nWe’ll also add a few echo statements to report what’s going on, and use a trick we haven’t yet seen — creating the output directory but only if it doesn’t yet exist:\n\nmkdir -p \"$output_dir\"\n\n\n\n\n\n\n\nThe -p option for mkdir\n\n\n\n\n\nUsing the -p option does two things at once for us, both of which are necessary for a foolproof inclusion of this command in a script:\n\nIt will enable mkdir to create multiple levels of directories at once: by default, mkdir errors out if the parent directory/directories of the specified directory don’t yet exist.\n\nmkdir newdir1/newdir2\n#> mkdir: cannot create directory ‘newdir1/newdir2’: No such file or directory\n\n\nmkdir -p newdir1/newdir2    # This successfully creates both directories\n\nIf the directory already exists, it won’t do anything and won’t return an error (which would lead the script to abort at that point with our set settings).\n\nmkdir newdir1/newdir2\n#> mkdir: cannot create directory ‘newdir1/newdir2’: File exists\n\n\nmkdir -p newdir1/newdir2   # This does nothing since the dirs already exist\n\n\n\n\n\nOur script now looks as follows:\n\n\n\n\n\n\nClick here to see the script\n\n\n\n\n\n\n#!/bin/bash\n#SBATCH --account=PAS2250\n#SBATCH --output=slurm-fastqc-%j.out\n  \n## Bash strict settings\nset -euo pipefail\n\n## Load the software\nmodule load fastqc\n\n## Copy the placeholder variables\ninput_file=\"$1\"\noutput_dir=\"$2\" \n\n## Initial reporting\necho \"Starting FastQC script\"\ndate\necho \"Input FASTQ file:   $input_file\"\necho \"Output dir:         $output_dir\"\necho\n\n## Create the output dir if needed\nmkdir -p \"$output_dir\"\n\n## Run FastQC\nfastqc --outdir=\"$output_dir\" \"$input_file\"\n\n## Final reporting\necho\necho \"Listing output files:\"\nls -lh \"$output_dir\"\n\necho\necho \"Done with FastQC script\"\ndate\n\n\n\n\nNotice that this script is very similar to our toy scripts from yesterday and today: mostly standard (“boilerplate”) code with just a single command to run our program of interest.\nTherefore, you can adopt this script as a template for scripts that run other command-line programs, and will generally only need minor modifications!\n\n\n\n\n\n\nKeep your scripts simple – use one program in a script\n\n\n\n\n\nIn general, it is a good idea to keep your scripts simple and run one program per script.\nOnce you get the hang of it, it may seem appealing to string a number of programs together in a single script, so that it’s easier to rerun everything — but that will often end up leading to more difficulties than convenience.\nTo really tie your full set of analyses together in an actual workflow / pipeline, you will want to start using a workflow management system like Snakemake or NextFlow.\n\n\n\n\n\n\n2.4 Submitting our FastQC script as a batch job\nOpen a new file in VS Code (     =>   File   =>   New File) and save it as fastqc.sh within your scripts/ directory. Paste in the code above and save the file.\nThen, we submit the script:\n\nsbatch scripts/fastqc.sh data/fastq/SRR7609467.fastq.gz results/fastqc\n\n\nSubmitted batch job 12521308\n\n\n\n\n\n\n\nOnce again: Where does our output go?\n\n\n\n\n\n\nOutput that would have been printed to screen if we had run the script directly: in the Slurm log file slurm-fastqc-<job-nr>.out\nFastQC’s main output files (HTML ZIP): to the output directory we specified.\n\n\n\n\nLet’s take a look at the queue:\n\nsqueue -u $USER\n# Fri Aug 19 10:38:16 2022\n#              JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n#           12521308 serial-40 fastqc.s   jelmer  PENDING       0:00   1:00:00      1 (None)\n\nOnce it’s no longer in the list produced by squeue, it’s done. Then, let’s check the Slurm log file1:\n\ncat slurm-fastqc-12521308.out    # You'll have a different number in the file name\n\n\n\n\n\n\n\nClick to see the contents of the Slurm log file\n\n\n\n\n\n\ncat misc/slurm-fastqc-12521308.out    # You'll have a different number in the file name\n\nStarting FastQC script\nFri Aug 19 10:39:52 EDT 2022\nInput FASTQ file:   data/fastq/SRR7609467.fastq.gz\nOutput dir:         results/fastqc\n\nStarted analysis of SRR7609467.fastq.gz\nApprox 5% complete for SRR7609467.fastq.gz\nApprox 10% complete for SRR7609467.fastq.gz\nApprox 15% complete for SRR7609467.fastq.gz\nApprox 20% complete for SRR7609467.fastq.gz\nApprox 25% complete for SRR7609467.fastq.gz\nApprox 30% complete for SRR7609467.fastq.gz\nApprox 35% complete for SRR7609467.fastq.gz\nApprox 40% complete for SRR7609467.fastq.gz\nApprox 45% complete for SRR7609467.fastq.gz\nApprox 50% complete for SRR7609467.fastq.gz\nApprox 55% complete for SRR7609467.fastq.gz\nApprox 60% complete for SRR7609467.fastq.gz\nApprox 65% complete for SRR7609467.fastq.gz\nApprox 70% complete for SRR7609467.fastq.gz\nApprox 75% complete for SRR7609467.fastq.gz\nApprox 80% complete for SRR7609467.fastq.gz\nApprox 85% complete for SRR7609467.fastq.gz\nApprox 90% complete for SRR7609467.fastq.gz\nApprox 95% complete for SRR7609467.fastq.gz\nAnalysis complete for SRR7609467.fastq.gz\n\nListing output files:\ntotal 16K\n-rw-r--r-- 1 jelmer PAS0471 224K Aug 19 10:39 SRR7609467_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 233K Aug 19 10:39 SRR7609467_fastqc.zip\n\nDone with FastQC script\nFri Aug 19 10:39:58 EDT 2022\n\n\n\n\n\nOur script already listed the output files, but let’s take a look at those too, and do so in the VS Code file browser in the side bar. To actually view FastQC’s HTML output file, we unfortunately need to download it with this older version of VS Code that’s installed at OSC — but the ability to download files from here is a nice one!"
  },
  {
    "objectID": "modules/09-examples.html#worked-example-part-ii-a-loop-in-a-workflow-script",
    "href": "modules/09-examples.html#worked-example-part-ii-a-loop-in-a-workflow-script",
    "title": "Batch Jobs in Practice",
    "section": "3 Worked example, part II: A loop in a workflow script",
    "text": "3 Worked example, part II: A loop in a workflow script\n\n3.1 A “workflow” file\nSo far, we’ve been typing our commands to run or submit scripts directly in the terminal. But it’s better to directly save these sorts of commands.\nTherefore, we will now create a new file for the purpose of documenting the steps that we are taking, and the scripts that we are submitting. You can think of this file as your analysis lab notebook2.\nIt’s easiest to also save this as a shell script (.sh) extension, even though it is not at all like the other scripts we’ve made, which are meant to be run/submitted in their entirety.\n\n\n\n\n\n\nNot like the other scripts\n\n\n\n\n\nOnce we’ve added multiple batch job steps, and the input of say step 2 depends on the output of step 1, we won’t be able to just run the script as is. This is because all the jobs would then be submitted at the same time, and step 2 would likely start running before step 1 is finished.\nThere are some possibilities with sbatch to make batch jobs wait on each other (e.g. the --dependency option), but this gets tricky quickly. As also mentioned above, if you want a fully automatically rerunnable workflow / pipeline, you should consider using a workflow management system like Snakemake or NextFlow.\n\n\n\nSo let’s go ahead and open a new text file, and save it as workflow.sh.\n\n\n\n3.2 Looping over all our files\nThe script that we wrote above will run FastQC for a single FASTQ file. Now, we will write a loop that iterates over all of our FASTQ files (only 8 in this case, but could be 100s just the same), and submits a batch job for each of them.\nLet’s type the following into our workflow.sh script, and then copy-and-paste it into the terminal to run the loop:\n\nfor fastq_file in data/fastq/*fastq.gz; do\n    sbatch scripts/fastqc.sh \"$fastq_file\" results/fastqc\ndone\n\n\nSubmitted batch job 2451089\nSubmitted batch job 2451090\nSubmitted batch job 2451091\nSubmitted batch job 2451092\nSubmitted batch job 2451093\nSubmitted batch job 2451094\nSubmitted batch job 2451095\nSubmitted batch job 2451096\n\n\n\nOn Your Own: Check if everything went well\n\nUse squeue to monitor your jobs.\nTake a look at the Slurm log files while the jobs are running and/or after the jobs are finished. A nice trick when you have many log files to check, is to use tail with a wildcard:\n\ntail slurm-fastqc*\n\nTake a look at the FastQC output files: are you seeing 12 HTML files?"
  },
  {
    "objectID": "modules/09-examples.html#adapting-our-scripting-workflow-to-run-other-command-line-programs",
    "href": "modules/09-examples.html#adapting-our-scripting-workflow-to-run-other-command-line-programs",
    "title": "Batch Jobs in Practice",
    "section": "4 Adapting our scripting workflow to run other command-line programs",
    "text": "4 Adapting our scripting workflow to run other command-line programs\n\nOn Your Own: Run another program\nUsing the techniques you’ve learned in this workshop, and especially, using our FastQC script as a template, try to run another command-line genomics program.\nWe below, we provide basically complete command-lines for three programs: MultiQC (summarizing FastQC output into one file), Trimmomatic (quality trimming and removing adapaters), and STAR (mapping files to a reference genome).\nYou can also try another program that you’ve been wanting to use.\n\n\n Commands to load/install and run other software:\n\n\n4.1 MultiQC\nMultiQC is a very useful program that can summarize QC and logging output from many other programs, such as FastQC, trimming software and read mapping software.\nThat means if you have sequenced 50 samples with paired-end reads, you don’t need to wade through 100 FASTQ HTML files to see if each is of decent quality — MultiQC will summarize all that output in nice, interactive figures in a single HTML file!\nHere, we’ll assume you want to run it on the FastQC output, which simply means using your FastQC output directory as the input directory for MultiQC.\n\nInstall\nMultiQC needs to be installed using an unusual 2-3 step procedure (one of the very few programs that can’t be installed in one go with conda):\n\nconda create -n multiqc python=3.7\nsource activate multiqc\nconda install -c bioconda -c conda-forge multiqc\n\n\n\n\n\n\n\nFailed to install? Using other people’s conda environments\n\n\n\n\n\nIf your MultiQC installation fails (this is a tricky one, with very many dependencies!), you can also use mine, by putting these line in your script:\n\nmodule load miniconda3\nsource activate /fs/project/PAS0471/jelmer/conda/multiqc-1.12\n\n\n\n\n\n\nRun\nYou would run MultiQC once for all files (no loop!) and with FastQC output as your input, as follows:\n\n# Copy the placeholder variables\ninput_dir=$1     # Directory where your FastQC output is stored\noutput_dir=$2    # Output dir of your choosing, e.g. result/multiqc\n\n# Activate the conda environment\nmodule load miniconda3\nsource activate multiqc\n\n# Run MultiQC\nmultiqc \"$input_dir\" -o \"$output_dir\"\n\n\n\n\n\n4.2 Trimmomatic\nTrimmomatic is a commonly-used program to both quality-trim FASTQ data and to remove adapters from the sequences.\n\nLoad the OSC module\n\nmodule load trimmomatic/0.38\n\n\n\nRun\nTo run Trimmomatic for one FASTQ file (=> loop needed like with FastQC):\n\n# Load the module\nmodule load trimmomatic/0.38\n\n# Copy the placeholder variables\ninput_fastq=$1    # One of our \"raw\" FASTQ files in data/fastq\noutput_fastq=$2   # Output file directory and name to your choosing\n\n# We provide you with a file that has all common Illumina adapters:\nadapter_file=/fs/ess/scratch/PAS2250/jelmer/mcic-scripts/trim/adapters.fa\n\n# Run Trimmomatic\ntrimmomatic PE \\\n  \"$input_fastq\" \"$output_fastq\" \\\n  ILLUMINACLIP:\"$adapter_file\":2:30:10 \\\n  LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36\n\n\n\n\n\n\n\nAvoid long lines with \\\n\n\n\nThe \\ in the Trimmomatic command above simply allow us to continue a single command one a new line, so we don’t get extremely long lines!\n\n\n\n\n\n\n4.3 STAR\n\n4.3.1 Load the OSC module\n\nmodule load gnu/10.3.0\nmodule load star/2.7.9a\n\n\n\n4.3.2 Index the genome\nFirst, we need to unzip the FASTA reference genome file:\n\ngunzip reference/Pvul.fa.gz\n\n\n#SBATCH --cpus-per-task=8\n\n# Load the module\nmodule load gnu/10.3.0\nmodule load star/2.7.9a\n\n# Copy the placeholder variables\nreference_fasta=$1       # Pvul.fa reference genome FASTA file\nindex_dir=$2             # Output dir with the genome index\n\n# Run STAR to index the reference genome\nSTAR --runMode genomeGenerate \\\n     --genomeDir \"$index_dir\" \\\n     --genomeFastaFiles \"$reference_fasta\" \\\n     --runThreadN \"$SLURM_CPUS_PER_TASK\"\n\n\n\n4.3.3 Map\n\n#SBATCH --cpus-per-task=8\n\n# Load the module\nmodule load star/2.7.9a\n\n# Copy the placeholder variables\nfastq_file=$1         # A FASTQ file to map to the reference genome\nindex_dir=$2          # Dir with the genome index (created in indexing script)\n\n# Extract a sample ID from the filename!\nsample_id=$(basename \"$fastq_file\" .fastq.gz)\n\n# Run STAR to map the FASTQ file\nSTAR \\\n  --runThreadN \"$SLURM_CPUS_PER_TASK\" \\\n  --genomeDir \"$index_dir\" \\\n  --readFilesIn $fastq_file \\\n  --readFilesCommand zcat \\\n  --outFileNamePrefix \"$outdir\"/\"$sample_id\" \\\n  --outSAMtype BAM Unsorted SortedByCoordinate\n\n\n\n\n\n\n\n\n\n\nKeyboard shortcut to run shell commands from the editor\n\n\n\nTo add a keyboard shortcut that will send code selected in the editor pane to the terminal (such that you don’t have to copy and paste):\n\nClick the      (bottom-left) => Keyboard Shortcuts.\nFind Terminal: Run Selected Text in Active Terminal, click on it, then add a shortcut, e.g. Ctrl+Enter."
  },
  {
    "objectID": "modules/02-osc.html#the-ohio-supercomputer-center-osc",
    "href": "modules/02-osc.html#the-ohio-supercomputer-center-osc",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "1 The Ohio Supercomputer Center (OSC)",
    "text": "1 The Ohio Supercomputer Center (OSC)\nThe Ohio Supercomputer Center is a resource provided by the State of Ohio (not The Ohio State University), so it’s available for use by any person/entity in Ohio including anyone at OSU. Further, academic folks like us are able to use it at heavily subsidized (=cheap!) rates. Physically, it’s located in Columbus - not on campus, but close by.\nWe’ll get you introduced to OSC in this session, and of course, you’ll get experience working with OSC resources as we go though the workshop, but we can’t cover everything here. The good news though is that OSC has lots of good support/training materials online. You can find them at the OSC website."
  },
  {
    "objectID": "modules/02-osc.html#what-is-a-supercomputer",
    "href": "modules/02-osc.html#what-is-a-supercomputer",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "2 What is a supercomputer?",
    "text": "2 What is a supercomputer?\nWe’ll start with the basics here - for our purposes, a supercomputer is basically a bunch of smaller computers connected together, allowing for computing jobs that require more resources than any one of the individual computers can alone provide."
  },
  {
    "objectID": "modules/02-osc.html#some-terminology",
    "href": "modules/02-osc.html#some-terminology",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "3 Some Terminology",
    "text": "3 Some Terminology\nThere are some terms that you’ll almost certainly hear (and maybe have used before yourself) when working with a supercomputer - either at OSC or elsewhere. Let’s think about how these terms relate to each other…\n\nSupercomputer\nNode\nCluster\nCore\nProcessor\n\n\n\n\nThere’s flexibility when using a supercomputer at OSC - the smallest job you can run would use a single processor, or core. Or you could run a slightly bigger job that uses say 10 cores that are all within one physical node. Or you could run an even bigger job that uses 100 cores that exist across 4 nodes by connecting those nodes together so their resources are shared (no single node at OSC has 100 cores)."
  },
  {
    "objectID": "modules/02-osc.html#the-structure-of-a-supercomputer",
    "href": "modules/02-osc.html#the-structure-of-a-supercomputer",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "4 The Structure Of A Supercomputer",
    "text": "4 The Structure Of A Supercomputer\nWe’re going to think about a supercomputer has having three main parts…\n\nLogin Node(s)\nCompute Nodes\nFile Storage System\n\n\n\n\n\n4.1 File Storage\nThere are 3 main areas/file systems where you can store files at OSC…\n\nHome Directories: /users/\nProject Directories: /fs/project/ OR /fs/ess/project/\nScratch Directories: /fs/scratch/ OR /fs/ess/scratch/\n\n\n\n\n\n4.1.1 Home Directory\nWhen you initially get an account with OSC, a HOME directory is created for you, named with your OSC username. This directory will always be within /users/. What subfolder it exists in depends on what project you were initially associated with. For example, the first project I was associated with at OSC was PAS0656. I don’t work on that project anymore, but the location of my HOME directory remains /users/PAS0656/osu6672 (note osu6672 is my OSC username).\nThis HOME directory provides you with up to 500 GB of storage (or up to 1,000,000 files). This is considered permanent storage, so it is well backed-up.\n\n\n4.1.2 Project Directories\nProject directories can’t be set up by just anyone, as the storage there has to be paid for. Typically, here at the University, Project directories are set up by PI’s. Project directories offer flexibility in terms of the amount of storage available (requested at the time of set-up, and then easily adjustable after that), and also in terms of who can access files in the directory.\nUnlike your HOME directory, where typically only you will have access to the files it contains, Projects are meant to be available to a specified group of people - say the members of a lab all working on a project. Like HOME directories, these are also backed up routinely. Project directories are located inside either /fs/project/ or /fs/ess/.\n\n\n4.1.3 Scratch Storage\nThe Scratch file system is meant to provide temporary storage space. Every Project directory has a corresponding Scratch directory. One advantage to Scratch space is that it is effectively unlimited. However, it’s not backed up, and files that are unmodified for a specific amount of time are automatically deleted.\n\n\n\n4.2 Login Nodes\n\n\n\nLogin nodes are set aside as an initial landing spot for everyone who logs in to a supercomputer. These are meant to be used to do things like organize your files, create scripts that specify future compute jobs, etc.\nThey are NOT meant for doing any serious computing on. Resources on the login nodes are limited and shared by everyone who logs in. Attempting large computing efforts on these nodes risks taxing the resources (i.e. RAM) and bogging things down for everyone. There are checks built in that limit what you are able to do on the login nodes (i.e. jobs running for longer than 20 min will be killed), but it’s best to just not push it at all. Any bigger computing jobs are better sent to the compute cluster.\n\n\n4.3 Compute Cluster\n\n\n\nThe compute cluster is really the powerhouse of the supercomputer, and is where you run your compute jobs. The details in terms of the size of the compute cluster itself, and the sizes of individual nodes contributing to the cluster (i.e. number of cores available on each node, amount of RAM available, etc) depends on which supercomputer you’re connected to, but regardless, you’ll have access to much more computing power than you’ll get on your local system.\nEach time you send a compute job to the compute cluster, you also make a request for the number of resources the job will need - specifically, the number of nodes, cores, and how long the job will run. As there are jobs being sent by different users all the time, there is software called a job scheduler that considers each request and assigns the necessary resources to the job as they are available. We’ll talk about that more later in the workshop.\nAll these parts are connected together to create a supercomputer…"
  },
  {
    "objectID": "modules/02-osc.html#supercomputers-at-osc",
    "href": "modules/02-osc.html#supercomputers-at-osc",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "5 Supercomputers at OSC",
    "text": "5 Supercomputers at OSC\nOSC technially has 3 separate supercomputers…\n\nRuby\nOwens\nPitzer\n\nRuby is reserved for specific uses, so the two you’ll interact with are Owens and Pitzer. This is what Owens looks like (and Pitzer is similar)…"
  },
  {
    "objectID": "modules/02-osc.html#connecting-to-a-supercomputer",
    "href": "modules/02-osc.html#connecting-to-a-supercomputer",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "6 Connecting to a Supercomputer",
    "text": "6 Connecting to a Supercomputer\nWhether you’re working on a supercomputer at OSC or elsewhere, you’ll connect to it via a connection between your personal computer and the remote computer. Sometimes this connection is a one-way connection, in which you’re working exclusively on the remote computer, and in other cases it can be a two-way connection. The two-way connection scenario allows you to transfer files between your local computer and the remote system (supercomputer).\n\n6.1 ssh\nA one-way connection that allows you to connect to and work on a remote supercomputer is usually achieved through secure shell - often abbreviated and referred to as ssh.\n\n\n\nThe two most common ways of connecting to OSC by way of ssh are though a Terminal or the OnDemand system.\n\n6.1.1 Terminal\nTraditionally if you’re on a Unix-based system (Mac or Linux), you can open the Terminal application and connect through ssh with something like this…\n\n\n\nUnfortunately, while this can also be done on a Windows-based system, it requires an extra piece of software (an ssh client - typically one called putty), as the ssh command isn’t available by default in Windows like it is on Unix-based systems.\nThe good news for us is that OSC provides an alternative way to interact with their supercomputers: the OnDemand system.\n\n\n6.1.2 OnDemand\nOnDemand allows you to access OSC resources through a web browser - it not only provides ssh access, but other forms of access too, including the ability to upload and download files between the remote and local computer. The OSC OnDemand starting point can be accessed at https://ondemand.osc.edu. After logging in, you should see a page similar to the one below:\n\n\n\nWe’re going to focus on some of the options in the blue bar along the top. Let’s start with Files. Hovering over this gives a list of directories you have access to. If you’re account is new, you might only have one or two. I’m associated with quite a few different projects, so I have a bunch. I’ll select my HOME directory.\n\n\n\nHere I can see a list of directories and files that are in my HOME directory.\n\n\n\nThere are also a series of useful buttons across the top. One note about the Upload/Download buttons here - these are great in many cases, but likely won’t be a good option for especially large files. Other options, such as Globus, are available for these (more info on Globus here).\nMore generally, this general interface is somewhat unique at OSC. Not all supercomputers will have this convenient GUI interface (the OnDemand system) that allows you to perform tasks like creating new folders, moving files, etc. in a point-and-click manner.\nHaving these is certainly nice, but they only go so far. Using a supercomputer effectively is likely to require interacting with the system through a command line approach as opposed to a point-and-click interface. One option available for doing that is under the Clusters option in the blue top bar:\n\n\n\nHere I’m selecting shell access to the Pitzer supercomputer, which will open a new browser tab looking like this:\n\n\n\nThis shell system works just like the Terminal program we briefly came across earlier. Basically, there’s a prompt (at the bottom of the image above) where you can enter a command for the supercomputer to execute. This is a common way to interact with the supercomputer.\nHowever, we’re going to check out one more option - a program called VS Code, which gives us the same basic shell access, but also includes some additional features that we might find useful later on. We can access VS Code by selecting Code Server under the Interactive Apps option on the OnDemand homepage (near the bottom of the list).\n\n\n\nIn this case, we need to provide a maximum time to keep the App open. The default is 6 hours. That will be fine for our purposes - you could even go a bit less, but remember that if you select 2 hours, you’ll be kicked off at the 2-hour mark, so usually better to err on the side of overestimating here.\n\n\n\nClick on Launch at the bottom and this will send your request to run the App for a maximum of the number of hours you chose. Once it’s ready, you’ll get a screen that looks like…\n\n\n\nWhen it’s available, click on the blue Connect to VS Code button and you should see the following Welcome screen…\n\n\n\nJelmer will talk about VS Code in the next section."
  },
  {
    "objectID": "modules/04-shell.html#the-unix-shellcommand-line-computing",
    "href": "modules/04-shell.html#the-unix-shellcommand-line-computing",
    "title": "The Unix Shell",
    "section": "1 The Unix Shell/Command Line Computing",
    "text": "1 The Unix Shell/Command Line Computing\nMany of the things you typically do by pointing and clicking can alternatively be done with command line approaches. The Unix shell allows you to interact with the supercomputer in a command-line environment. The Code Server/VS Code software Jelmer just introduced is one of several methods available for accessing the Unix shell, and the one we’ll use through the workshop. Now that we have the platform for interacting with the supercomputer, we’ll dive into command line computing."
  },
  {
    "objectID": "modules/04-shell.html#overview",
    "href": "modules/04-shell.html#overview",
    "title": "The Unix Shell",
    "section": "2 Overview",
    "text": "2 Overview\nWorking effectively on a remote supercomputer requires doing command-line computing. But there are more advantages to doing command line computing than just allowing you to work on a supercomputer.\n\n2.1 Some Terminology\nWe’re going to focus on the practice of doing command line computing here, and not get too bogged down in to details of terminology, but let’s highlight a few terms you’re likely to run across (and hear during the workshop).\n\nCommand Line\nShell\nTerminal\nConsole\nBash\n\nWhile it might not fly for a computer science class, for day-to-day bioinformatics, you’ll probably hear all these terms used somewhat interchangably. Basically, we’re talking about the process of interacting with your computer by giving it commands as opposed to the point-and-click way you’re likely more familiar with.\n\n\n2.2 Advantages to Command Line Computing\n\nInteract with a remote computer\nWork efficiently with large files\nAchieve reproducibility in research\nPerform general computing tasks more efficiently\n\n\n\n2.3 Structure Of Command Line Expressions\nWe’ll think of command line expressions as having 3 main parts. Don’t worry about following along here - there will be plenty of chances to try this out shortly. For now, just treat this as a demonstration and focus on these 3 components…\n\nCommands\nOptions or Arguments\nOutput\n\n\n2.3.1 Command Line Commands\nThe prompt indicates the shell is ready for a command.\n\n\n\nLet’s start with the ls command.\n\n\n\n\n\n\nHere, we’ve given a command, ls, and the shell has returned some output – in this case, listing the contents of the current directory. It has also returned another prompt, meaning it’s ready for another command.\nNow we’ll run the same command again, but this time we’ll add in an option -l (a dash followed by a lowercase L). Options allow you to modify the behavior of a command.\n\n\n\n\n\n\nNotice that the same four items are returned, but this time, they’re printed in a different format, and additional information is included.\nLet’s try adding one more option/argument – -h.\n\n\n\n\n\n\nCan you pick out what -h did to modify the output? Note the difference in the format of the column reporting the sizes of the items listed.\n\n\n\n2.4 Commonly-Used Commands\nBelow are some commands I find myself using quite a bit, grouped into some general categories based on their primary function…\n\nGetting Info About a Command\n\nman: get help (manual) for a command\n\nNavigating in the Terminal\n\npwd: returns (prints) your working directory\ncd: change working directory\n\nViewing Files\n\nless: view contents of a file\nhead: preview contents of a file\ntail: print the last lines of a file\ncat: print contents of file to screen\n\nManaging/Organizing Files\n\nls: list contents of directory\nmkdir: create a new directory\nrm: remove/delete a file or directory\ncp: copy files/directories to a new location\nmv: move files/directories to a new location\n\nWorking With Compressed Files\n\ngzip: compress a file with gzip compression\ngunzip: uncompress a gzip file\nzcat/gzcat: print the uncompressed contents of a compressed file to the screen\nunzip: uncompress a zip file\n\nAssessing Files\n\ndiff: print differences in two text files\nmd5: get and md5 checksum for a file\ngrep: search a text file for lines containing a pattern of text\nwc: return number of lines, words, characters in a file\n\nEditing Files\n\nsed\nawk\nsort\ntr\nuniq\n\nObtaining/Sharing Files\n\ncurl: download a file from online\nsftp: transfer files between a local and remote computer\nwget: download a file from online\n\nFeatures\n\nTab completion\nCommand History (up arrow)\nCtrl+r\nCtrl+c\n\nSpecial Notation\n\n|\n~\n.\n..\n$PATH\n$HOME\n\nWildcards\n\n*\n?\n[]\n^\n\n\nWhile it’s not an exhaustive list, getting a grasp on some of the commands and features above will go a long way in allowing you to start to work at command line. We won’t get to all of them in this session, but we’ll explore quite a few on this list."
  },
  {
    "objectID": "modules/04-shell.html#practice-with-common-commands",
    "href": "modules/04-shell.html#practice-with-common-commands",
    "title": "The Unix Shell",
    "section": "3 Practice with Common Commands",
    "text": "3 Practice with Common Commands\nHere we’ll start practicing with some common command-line commands.\n\n\n\n\n\n\nWorking Directories\n\n\n\nBefore we start with our first command, we should talk about directory paths and working directories. All the files on a computer exist within a hierarchical system of directories (folders). When working at command line, you are always “in” one of these directories. The directory you’re “in” at any given time is referred to as your working directory.\nIt’s useful to know what this is at any given time, and there’s a command that tells you: pwd. This brings us to our first command.\n\n\n\n3.1 pwd\nThe pwd command prints the absolute path of your current working directory.\n\n\n\n\n\n\nThe default working directory when you log on to OSC is your HOME directory. You’ll likely make lots of use of this directory if you work at OSC, but for the workshop, we’re all going to work in the scratch directory associated with the project set up for the workshop. So we’ll move to that directory next.\n\n\n3.2 cd\ncd allows you to change your working directory to any directory you have permission to access on the computer you’re working on. And this is a great place to introduce Tab Completion, which you really should get in the habit of using…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbsolute Paths vs Relative Paths\n\n\n\nWhat we’ve used here is an absolute (full) path. If you want to change directories, the cd command needs to know where you want to move to. As we just saw, we can give it that information by providing the absolute path to the directory on the computer we want to move to (starting with the root directory, which is indicated by the first / in the path /fs/ess/scratch/PAS2250 above).\nProviding absolute paths will always work, but it’s often a bit more typing than we need (or want) to do. An alternative is to work with relative paths. These work by assuming you’re staring in your current working directory, and then, by default, looking forward in the directory structure (or down if you like to think from top to bottom). We’ll come back to relative paths shortly.\n\n\nOK, we made it to a directory created specifically for this workshop. Let’s see what’s in there.\n\n\n3.3 ls\nThe ls command lists everything inside the current directory.\n\n\n\n\n\n\nHere we see 3 directories – data, jelmer, and participants.\nLet’s see what’s inside the data directory (and another good chance to try Tab Completion)…\n\n\n\n\n\n\n\n\n\nTwo more directories this time. Try viewing the contents of the fastq directory yourself…\n\nOn Your Own: list the contents of a directory\nTry to list the contents of the fastq directory we just saw.\n\n\nSolutions (click here)\n\n\nls data/fastq/\n\nOR\n\ncd data/fastq/\nls\n\n\n\nLet’s check to make sure we’re still in the /fs/ess/scratch/PAS2250 directory, and also remind ourselves exactly what’s in there…\n\n\n\nIf you’re not in the PAS2250 directory for some reason, you can use cd to get back there. You should see data, jelmer, and participants when listing the contents of your current directory.\nNow let’s move our working directory again – this time we’ll go in to participants. We could use cd with the absolute path – similar to what we did before. However, we’ll save ourselves some typing and use a relative path. Keep using Tab Completion.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.4 mkdir\nThe mkdir command allows you to create a new directory. Create one for yourself in the current directory (participants) according to this example (replace mikes_dir with a name for your folder – avoid spaces and special characters in the name)…\n\n\n\n\n\n\n\n\n3.5 Summary Thus Far\nSo far, we’ve used…\n\npwd (print working directory)\ncd (change directory)\nls (list)\nmkdir (make new directory)\nTab Completion (not really a command, but a useful feature)\n\nWe also have a directory structure that looks like…\n\n\n\nThe data files in the data/fastq directory are FASTQ formatted files from an RNA experiment, and are what we’ll be analyzing as we go through the workshop. We’ll talk more about them soon, but for now, let’s make sure everyone has a copy of the data. We’ll copy the data directory and its contents into the new directory you just made.\n\n\n\n\n\n3.6 cp\nThe cp command allows you to copy files or directories from one location to another. It has 2 required arguments – what you want to copy, and where you want to copy it to.\nLet’s start with what we want to copy. It’s the data directory and all of its contents. Notice in the diagram above that data is at the same level in the directory structure as our current working directory, participants. This means using data as a relative path won’t work, because the computer looks down the directory structure (it will see the contents of ‘participants’). But there’s a way to deal with that. We can use .. to move us up a level in the directory structure.\n\n\n\n\n\n\nNotice we get a message that it omitted copying the directory data (which is what we wanted to copy). Indeed, the copy didn’t work (you can ls the contents of the target directory to check – it will still be empty). cp works in this simplest form with individual files, but not with directories that have contents inside them. If you want to copy a directory and all of its contents, we need one of those options that modify the behavior of the cp command. In this case, -r, which tells it to copy in a recursive manner.\nAnd this is a good spot to introduce the Command History. At the prompt, try hitting the up arrow. A record of all your previous commands is kept, so you can scroll back through them. Use this to get the previous cp command, and then add the -r argument.\n\n\n\n\n\n\nAnd we can check to make sure the copy worked…\n\n\n\n\n\n3.7 man\nWe haven’t talked about man yet. This stands for manual, and is a great way to get details on any command. For example, we can check out the man page for cp…\n\n\n\n\n\n\nIf you scroll down, you’ll see information on the -r option we just used (among others). As it says at the bottom of the page, type q to quit and get back to your command line prompt."
  },
  {
    "objectID": "modules/04-shell.html#working-with-text-files",
    "href": "modules/04-shell.html#working-with-text-files",
    "title": "The Unix Shell",
    "section": "4 Working With Text Files",
    "text": "4 Working With Text Files\nNow let’s start to explore our FASTQ files a bit. In preparation, it’s a good chance to practice a few of the commands we’ve seen so far.\n\nOn Your Own: Explore the Files\nSet your working directory to the data/fastq directory inside the folder you created for yourself. Then list the contents of that fastq directory. How many files are in there? See if you can get the sizes of each file.\n\n\nHint (click here)\n\nUse cd and a relative path (<your_dir>/data/fastq/) to change you working directory.\nOnce you’re there, use ls to list the contents of the current directory. Recall the option that we used above to give more detailed information about each file, or check out the man page for ls.\n\n\n\nSolutions (click here)\n\n\ncd <your_dir>/data\n\nls\n\nls -l\n\n\n\n\n4.1 Compressed Files\nYou might have noticed these files all have a .gz extension, indicating they are ‘gzip-compressed’. This is a common type of compression for large genomic-scale files. The fact that they’re compressed means we can’t just open them up and look inside – we need to uncompress them first. The gunzip command would allow us to do this – it uncompresses the file it’s given and writes the uncompressed version to a new file.\nWe could do this, but there’s another approach. FASTQ files can get big, and sometimes it helps to be able to keep them compressed as much as possible. It’s a good time for us to explore the pipe.\n\n\n4.2 | (pipe)\nWe talked earlier about command line expressions having 3 parts – the command itself, options and arguments, and output. By default, any output is printed to the screen. That’s what we’ve seen so far. But you can also redirect the output, and there are three primary ways to redirect it…\n\nWith >, which is followed by the name of a text file the output will be written to\nWith >>, which is simlar to > but will append the output (that is, it won’t overwrite any existing content like >)\nWith | (pipe), which takes the output of one command and “pipes” it as input for a subsequent command.\n\nLet’s try to preview the contents of one of the compressed files.\n\n\n4.3 head\nThe head command is a great way to preview the contents of a text file. By default, head prints the first 10 lines of a file. Since these are FASTQ files, let’s print 8 lines (a multiple of 4 – it will become clear why shortly). We can use the -n argument to specify the number of lines that will be returned.\n\n\n\n\n\n\nThis isn’t what we want – we’re seeing the first 8 lines of the compressed files - not helpful.\n\n\n4.4 zcat\nThe zcat function prints human-readable contents of a gzip-compressed file to the screen. We can try running it on the file, but remember the file is pretty big – there are lots of lines of text in there that will all get printed to the screen. Instead, we can pipe the output of zcat to the head command.\n\n\n\n\n\n\nMuch better – this is what the raw RNAseq data look like!\n\n\n\n\n\n\nWarning\n\n\n\nTo get the number of lines (= number sequences x 4 – see below) for a gzipped FASTQ file, it’s important to use zcat x.fastq.gz | wc -l instead of wc -l x.fastq.gz, because the compressed file does not have the same number of lines!\n\n\n\n\n4.5 FASTQ Format\nIf you’re not familiar with it, FASTQ is a very common format for genomic data files. The raw data produced by a high-throughput sequencer will almost certainly be returned to you in this format. These are plain text files, and each sequence that is read by the sequencer is represented by 4 lines:\n\na name (header) line\nthe sequence itself\na plus sign\nquality scores corresponding to each base position in the sequence\n\n\n\n4.6 wc\nSince each read in a FASTQ file is represented by 4 lines, we should expect the number of lines in each of the FASTQ files to be a multiple of 4. Let’s check one. The wc command stands for word count, but by default, it returns the number of words, lines, and characters in a file. The -l option tells it to return just the number of lines, so we’ll use it since that’s all we’re interested in right now. And remember, we’ll want to do this on the uncompressed data.\n\n\n\n\n\n\n\n\n4.7 grep\ngrep allows you to search through a file for specific patterns of text and returns the matching lines. For example, let’s say we wanted to see what sequences in sample SRR7609467 contain the sequence “ACCGATACG”:\n\n\n\n\n\n\n\nOn Your Own: Finding a Sequence\nHow many sequences in sample SRR7609467 contain the sequence “CCAGTA”?\n\n\nHint (click here)\n\nPipe the results of the grep to wc -l. Alternatively, check out the -c option to grep in the man page.\n\n\n\nSolutions (click here)\n\n\nzcat SRR7609467.fastq.gz | grep 'CCAGTA' | wc -l\n\nOR\n\nzcat SRR7609467.fastq.gz | grep -c 'CCAGTA'"
  },
  {
    "objectID": "modules/04-shell.html#downloading-files-from-the-web",
    "href": "modules/04-shell.html#downloading-files-from-the-web",
    "title": "The Unix Shell",
    "section": "5 Downloading Files from the Web",
    "text": "5 Downloading Files from the Web\nAt command line, you can’t just open a web browser and download a file you might want. But of course, there are commands to do that. As we move toward starting to analyze our example RNAseq dataset, one thing we’ll need is the reference genome for the species these sequences came from – in this case, Phaseolus vulgaris. Before we get that, it’s another good time to practice some of those common commands…\n\nOn Your Own: Create a Directory\nCreate a new (empty) directory named reference that will later store the reference genome for our analyses. Put it in your own directory inside participants. Then make this reference directory your working directory.\n\n\nHint (click here)\n\nUse the mkdir command (and cd as necessary). Remember that .. moves you up/back one directory, and these can be combined. For example, ../../../ would move you up/back 3 directories.\n\n\n\nSolution (click here)\n\n\nmkdir ../../reference\n  \ncd ../../reference\n\nOR\n\ncd ../../\n  \nmkdir reference\n   \ncd reference\n\n\n\n\n5.1 curl\ncurl is one command that allows you to download files from online (wget is another). Technically, all you need for curl is the name of the command and the web address for what you want to download.\nHowever, the default for curl is to print the downloaded contents to the screen. This usually isn’t what we want. Instead, we want to save them to a file. One option would be to redirect the output to a text file with >. But curl also has a built-in option to write the contents to a file: -o, so we’ll use that.\nSince the file that gets downloaded is a gzip-compressed, FASTA-formatted text file, we’ll give the name of the file a .fa.gz extension. The reference genome for Phaseolus vulgaris is available at:\n\nhttps://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/499/845/GCF_000499845.1_PhaVulg1_0/GCF_000499845.1_PhaVulg1_0_genomic.fna.gz\n\n\n\n\n\n\n\n\nOn Your Own: Preview a FASTA file\nTry previewing the contents of the reference genome file you just downloaded.\n\n\nHint (click here)\n\nRemember the file is gzip-compressed. Use zcat and pipe the results to head.\n\n\n\nSolution (click here)\n\n\nzcat Pvulg.fa.gz | head\n\n\n\nOK, now we’ve got our raw data (FASTQ) and our reference genome (FASTA). This is a good start in terms of getting ready to start analyzing the data. One more thing we can do now is try to understand a little bit about the samples themselves. There is a tab-separated text file named meta.tsv in the data/meta directory. Let’s take a look at its contents…\n\n\n5.2 less\nless is a command that opens up a text file within your shell. Once you’re finished viewing the file, type q to quit and return to your prompt."
  },
  {
    "objectID": "modules/07-software.html",
    "href": "modules/07-software.html",
    "title": "Using Software at OSC",
    "section": "",
    "text": "So far, we have only used commands that are available in any Unix shell. But to actually analyze genomics data sets, we also need to use specialized bioinformatics software.\nMost software that is already installed at OSC must nevertheless be “loaded” (“activated”) before we can use it — and if our software of choice is not installed, we have to do so ourselves. We will cover those topics in this module."
  },
  {
    "objectID": "modules/07-software.html#setup",
    "href": "modules/07-software.html#setup",
    "title": "Using Software at OSC",
    "section": "1 Setup",
    "text": "1 Setup\n\n\n\n\n\n\nStarting a VS Code session with an active terminal (click here)\n\n\n\n\n\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 4 or more in the box Number of hours\nTo avoid having to switch folders within VS Code, enter /fs/ess/scratch/PAS2250/participants/<your-folder> in the box Working Directory (replace <your-folder> by the actual name of your folder).\nClick Launch.\n\nOn the next page, once the top bar of the box is green and says Runnning, click Connect to VS Code.\nOpen a terminal:    => Terminal => New Terminal.\nIn the terminal, type bash and press Enter.\nType pwd in the termain to check you are in /fs/ess/scratch/PAS2250.\nIf not, click    =>   File   =>   Open Folder and enter /fs/ess/scratch/PAS2250/<your-folder>."
  },
  {
    "objectID": "modules/07-software.html#running-command-line-programs",
    "href": "modules/07-software.html#running-command-line-programs",
    "title": "Using Software at OSC",
    "section": "2 Running command-line programs",
    "text": "2 Running command-line programs\nAs pointed out in the introduction to the workshop, bioinformatics software (programs) that we use to analyze genomic data are typically run from the command line. That is, they have “command-line interfaces” (CLIs) rather than “graphical user interfaces” (GUIs), and are run using commands that are structurally very similar to how we’ve been using basic Unix commands.\nFor instance, we can run the program FastQC as follows, instructing it to process the FASTQ file sampleA.fastq.gz with default options:\n\nfastqc sampleA.fastq.gz       # Don't run\n\nSo, with all the scaffolding we have learned in the previous modules, we only need to make small modifications to have our scripts run command-line programs. But, we first need to load and/or install these programs.\n\n\n\n\n\n\nRunning inside a script or interactively\n\n\n\nLike any other command, we could in principle run the line of code above either in our interactive shell or from inside a script. In practice, it is better to do this in a script, especially at OSC, because:\n\nSuch programs typically take a while to run\nWe are not supposed to run processes that use significant resources on login nodes\nWe can run the same script simultaneously for different input files."
  },
  {
    "objectID": "modules/07-software.html#software-at-osc-with-lmod",
    "href": "modules/07-software.html#software-at-osc-with-lmod",
    "title": "Using Software at OSC",
    "section": "3 Software at OSC with Lmod",
    "text": "3 Software at OSC with Lmod\nOSC administrators manage software with the Lmod system of software modules. For us users, this means that even though a lot of software is installed, most of it can only be used after we explicitly load it.\n(That may seem like a drag, but on the upside, this practice enables the use of different versions of the same software, and of mutually incompatible software on a single system.)\n\n3.1 Checking for available software\nThe OSC website has a list of software that has been installed at OSC. You can also search for available software in the shell using two subtly different commands:\n\nmodule spider lists modules that are installed.\nmodule avail lists modules that can be directly loaded, given the current environment (i.e., depending on which other software has been loaded).\n\nSimply running module spider or module avail would spit out complete lists — more usefully, we can add search terms as arguments to these commands:\n\nmodule spider python\n\n\n\n\n\npython:\n\n\n\n Versions:\n    python/2.7-conda5.2\n    python/3.6-conda5.2\n    python/3.7-2019.10\n\n\nmodule avail python\n\n\npython/2.7-conda5.2         python/3.6-conda5.2 (D)         python/3.7-2019.10\n\n\n\n\n\n\n\n(D) = default version\n\n\n\nThe (D) in the output above marks the default version of the program; that is, the version of the program that would be loaded if we don’t specify a version (see examples below).\n\n\n\n\n\n3.2 Loading software\nAll other Lmod software functionality is also accessed using module “subcommands” (we call module the command and e.g. spider the subcommand). For instance, to load a module:\n\n# Load a module:\nmodule load python              # Load the default version\nmodule load python/3.7-2019.10  # Load a specific version (copy from module spider output)\n\nTo check which modules have been loaded (the list includes automatically loaded modules):\n\nmodule list\n\n\nCurrently Loaded Modules:\n    1) xalt/latest       2) gcc-compatibility/8.4.0       3) intel/19.0.5       4) mvapich2/2.3.3       5) modules/sp2020\n\n\n\n\n\n\n\nUnloading modules\n\n\n\nOccasionally useful when running into conflicting (mutually incompatible) modules:\n\nmodule unload python        # Unload a module\nmodule purge                # Unload all modules\n\n\n\n\n\n\n3.3 A practical example\nLet’s load a very commonly used bioinformatics program that we will also use in examples later on: FastQC. FastQC performs quality control (hence: “QC”) on FASTQ files.\nFirst, let’s test that we indeed cannot currently use fastqc by running fastqc with the --help flag:\n\nfastqc --help\n\n\nbash: fastqc: command not found\n\n\n\n\n\n\n\nHelp!\n\n\n\nA solid majority of command-line programs can be run with with a --help (and/or -h) flag, and this is often a good thing to try first, since it will tell use whether we can use the program, and if we can, we immediately get some usage information.\n\n\nNext, let’s check whether FastQC is available at OSC, and if so, in which versions:\n\nmodule avail fastqc\n\n\nfastqc/0.11.8\n\nThere is only one version available (0.11.8), which means that module load fastqc and module load fastqc/0.11.8 would each load that same version.\n\n\n\n\n\n\nWhat might still be a reason to specify the version when we load FastQC?\n\n\n\n\n\nWhen we use it inside a script:\n\nThis would ensure that when we run the same script a year later, the same version would be used (assuming it hasn’t been removed) — otherwise, it’s possible a newer version would has been installed in the meantime, which might produce different results.\nIt will make it easy to see which version we used, which is something we typically report in papers.\n\n\n\n\nLet’s load the FastQC module:\n\nmodule load fastqc/0.11.8\n\nAfter we have loaded the module, we can retry our --help attempt:\n\nfastqc --help | head     # I'm piping into head to avoid pages worth of output \n\n\n        FastQC - A high throughput sequence QC analysis tool\nSYNOPSIS\n    fastqc seqfile1 seqfile2 .. seqfileN\n\nfastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] \n       [-c contaminant file] seqfile1 .. seqfileN"
  },
  {
    "objectID": "modules/07-software.html#when-software-isnt-installed-at-osc",
    "href": "modules/07-software.html#when-software-isnt-installed-at-osc",
    "title": "Using Software at OSC",
    "section": "4 When software isn’t installed at OSC",
    "text": "4 When software isn’t installed at OSC\nIt’s not too uncommon that software you need for your project is not installed at OSC, or that you need a more recent version of the software than is available. The main options available to you in such a case are to:\n\n“Manually” install the software, which in the best case involves downloading a directly functioning binary (executable), but more commonly requires you to “compile” (build) the program. This is sometimes straightforward but can also become extremely tricky, especially at OSC where you don’t have “administrator privileges”1 at OSC and will often have difficulties with “dependencies”2.\nSend an email to OSC Help. They might be able to help you with your installation, or in case of commonly used software, might be willing to perform a system-wide installation (that is, making it available through module).\nUse Apptainer / Singularity “containers”. Containers are self-contained software environments that include operating systems, akin to mini virtual machines.\nUse conda, which creates software environments that are activated like in the module system.\n\nConda and containers are useful not only at OSC, where they bypass issues with dependencies and administrator privileges, but more generally, for reproducible and portable software environments. They also allow you to easily maintain distinct “environments”, each with a different version of the same software, or with mutually incompatible software.\nWe will teach conda here because it is easier to learn and use than containers, and because nearly all open-source bioinformatics software is available as a conda package.\n\n\n\n\n\n\nWhen to use containers instead of Conda\n\n\n\n\nIf you need to use software that requires a different Operating System (OS) or OS version than the one at OSC.\nIf you want or require even greater reproducibility and portability to create an isolated environment that can be exported and used anywhere."
  },
  {
    "objectID": "modules/07-software.html#using-conda",
    "href": "modules/07-software.html#using-conda",
    "title": "Using Software at OSC",
    "section": "5 Using conda",
    "text": "5 Using conda\nConda creates so-called environments in which you can install one or more software packages. As mentioned above, these environments are activated and deactivated in a similar manner as with the Lmod system – but the key difference is that we can create and manage these environments ourselves.\n\n\n\n\n\n\nUse one environment per program (as here) or one per research project\n\n\n\n\n\nBelow are two reasonable ways to organize your conda environments, and their advantages:\n\nHave one environment per program (my preference)\n\nEasier to keep an overview of what you have installed\nNo need to reinstall the same program across different projects\nLess risk of running into problems with your environment due to mutually incompatible software and complicated dependency situations\n\nHave one environment per research project\n\nYou just need to activate that one environment when you’re working on your project.\nEasier when you need to share your entire project with someone else (or yourself) on a different (super)computer.\n\n\nIts not recommended to simply install all programs across all projects in one environment. This doesn’t benefit reproducibility and your environment is likely to sooner or later stop functioning properly.\nA side note is that even when you want to install a single program, multiple programs are in fact nearly always installed: the programs that your target program depends on (“dependencies”).\n\n\n\n\n\n5.1 Loading the (mini)conda module\nWhile it is also fairly straightforward to install conda for yourself 3, we will use OSC’s system-wide installation of conda in this workshop. Therefore, we first need to use a module load command to make it available:\n\n# (The most common installation of conda is actually called \"miniconda\")\nmodule load miniconda3\n\n\n\n\n5.2 One-time conda configuration\nWe will also do some one-time configuration, which will set the conda “channels” (basically, software repositories) that we want to use when we install software. This config also includes setting relative priorities among channels, since one software package may be available from multiple channels.\nLike with module commands, conda commands consist of two parts, the conda command itself and a subcommand, such as config:\n\nconda config --add channels defaults     # Added first => lowest priority\nconda config --add channels bioconda\nconda config --add channels conda-forge  # Added last => highest priority\n\nLet’s check whether this configuration step worked:\n\nconda config --get channels\n\n\n–add channels ‘defaults’       # lowest priority\n–add channels ‘bioconda’\n–add channels ‘conda-forge’       # highest priority\n\n\n\n\n5.3 Example: Creating an environment for cutadapt\nTo practice using conda, we will now create a conda environment with the program cutadapt installed.\ncutadapt is a commonly used program to remove adapters or primers from sequence reads in FASTQ files — in particular, it is ubiquitous for primer removal in (e.g. 16S rRNA) microbiome metabarcoding studies. But there is no Lmod module on OSC for it, so if we want to use it, our best option is to resort to conda.\nHere is the command to create a new environment and install cutadapt into that environment:\n\nconda create -y -n cutadapt -c bioconda cutadapt   # Don't run this\n\nLet’s break the above command down:\n\ncreate is the conda subcommand to create a new environment.\n-y is a flag that prevents us from being asked to confirm installation.\nFollowing the -n option, we can specify the name of the environment, so -n cutadapt means that we want our environment to be called cutadapt. We can use whatever name we like for the environment, but of course a descriptive yet concise name is a good idea. Since we are making a single-program environment, it makes sense to simply name it after the program.\nFollowing the -c option, we can specify a channel from which we want to install, so -c bioconda indicates we want to use the bioconda channel. (Given that we’ve done some config above, this is not always necessary, but it can be good to be explicit.)\nThe cutadapt at the end of the line simply tells conda to install the package of that name. This is a “positional” argument to the command (note that there’s no option like -s before it): we put any software package(s) we want to install at the end of the command.\n\n\nSpecifying a version\nIf we want to be explicit about the version we want to install, we can add the version after = following the package name. We do that below, and we also include the version in the environment name.\n Let’s run the command above and see if we can install cutadapt\n\nconda create -y -n cutadapt-4.1 -c bioconda cutadapt=4.1\n\n\nCollecting package metadata (current_repodata.json): done\nSolving environment:\n\n\n\n\n\n\n\nSee the full output when I ran this command\n\n\n\n\n\n\n\n\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n\n==> WARNING: A newer version of conda exists. <==\n  current version: 4.10.3\n  latest version: 4.13.0\n\nPlease update conda by running\n\n    $ conda update -n base -c defaults conda\n\n\n\n## Package Plan ##\n\n  environment location: /fs/project/PAS0471/jelmer/conda/cutadapt-TMP\n\n  added / updated specs:\n    - cutadapt=4.1\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    cutadapt-4.1               |  py310h1425a21_1         211 KB  bioconda\n    dnaio-0.9.1                |  py310h1425a21_1          80 KB  bioconda\n    libsqlite-3.39.2           |       h753d276_1         789 KB  conda-forge\n    openssl-3.0.5              |       h166bdaf_1         2.8 MB  conda-forge\n    pip-22.2.2                 |     pyhd8ed1ab_0         1.5 MB  conda-forge\n    python-isal-1.0.1          |  py310h5764c6d_0          47 KB  conda-forge\n    setuptools-65.0.2          |  py310hff52083_0         1.4 MB  conda-forge\n    sqlite-3.39.2              |       h4ff8645_1         788 KB  conda-forge\n    tzdata-2022c               |       h191b570_0         119 KB  conda-forge\n    xopen-1.6.0                |  py310hff52083_0          27 KB  conda-forge\n    xz-5.2.6                   |       h166bdaf_0         409 KB  conda-forge\n    ------------------------------------------------------------\n                                           Total:         8.1 MB\n\nThe following NEW packages will be INSTALLED:\n\n  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge\n  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu\n  bzip2              conda-forge/linux-64::bzip2-1.0.8-h7f98852_4\n  ca-certificates    conda-forge/linux-64::ca-certificates-2022.6.15-ha878542_0\n  cutadapt           bioconda/linux-64::cutadapt-4.1-py310h1425a21_1\n  dnaio              bioconda/linux-64::dnaio-0.9.1-py310h1425a21_1\n  isa-l              conda-forge/linux-64::isa-l-2.30.0-ha770c72_4\n  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.36.1-hea4e1c9_2\n  libffi             conda-forge/linux-64::libffi-3.4.2-h7f98852_5\n  libgcc-ng          conda-forge/linux-64::libgcc-ng-12.1.0-h8d9b700_16\n  libgomp            conda-forge/linux-64::libgomp-12.1.0-h8d9b700_16\n  libnsl             conda-forge/linux-64::libnsl-2.0.0-h7f98852_0\n  libsqlite          conda-forge/linux-64::libsqlite-3.39.2-h753d276_1\n  libuuid            conda-forge/linux-64::libuuid-2.32.1-h7f98852_1000\n  libzlib            conda-forge/linux-64::libzlib-1.2.12-h166bdaf_2\n  ncurses            conda-forge/linux-64::ncurses-6.3-h27087fc_1\n  openssl            conda-forge/linux-64::openssl-3.0.5-h166bdaf_1\n  pbzip2             conda-forge/linux-64::pbzip2-1.1.13-0\n  pigz               conda-forge/linux-64::pigz-2.6-h27826a3_0\n  pip                conda-forge/noarch::pip-22.2.2-pyhd8ed1ab_0\n  python             conda-forge/linux-64::python-3.10.5-ha86cf86_0_cpython\n  python-isal        conda-forge/linux-64::python-isal-1.0.1-py310h5764c6d_0\n  python_abi         conda-forge/linux-64::python_abi-3.10-2_cp310\n  readline           conda-forge/linux-64::readline-8.1.2-h0f457ee_0\n  setuptools         conda-forge/linux-64::setuptools-65.0.2-py310hff52083_0\n  sqlite             conda-forge/linux-64::sqlite-3.39.2-h4ff8645_1\n  tk                 conda-forge/linux-64::tk-8.6.12-h27826a3_0\n  tzdata             conda-forge/noarch::tzdata-2022c-h191b570_0\n  wheel              conda-forge/noarch::wheel-0.37.1-pyhd8ed1ab_0\n  xopen              conda-forge/linux-64::xopen-1.6.0-py310hff52083_0\n  xz                 conda-forge/linux-64::xz-5.2.6-h166bdaf_0\n  zlib               conda-forge/linux-64::zlib-1.2.12-h166bdaf_2\n\n\n\nDownloading and Extracting Packages\nsqlite-3.39.2        | 788 KB    | ################################################################################################################################################################### | 100% \nxz-5.2.6             | 409 KB    | ################################################################################################################################################################### | 100% \ncutadapt-4.1         | 211 KB    | ################################################################################################################################################################### | 100% \nxopen-1.6.0          | 27 KB     | ################################################################################################################################################################### | 100% \nlibsqlite-3.39.2     | 789 KB    | ################################################################################################################################################################### | 100% \ndnaio-0.9.1          | 80 KB     | ################################################################################################################################################################### | 100% \npython-isal-1.0.1    | 47 KB     | ################################################################################################################################################################### | 100% \nsetuptools-65.0.2    | 1.4 MB    | ################################################################################################################################################################### | 100% \npip-22.2.2           | 1.5 MB    | ################################################################################################################################################################### | 100% \ntzdata-2022c         | 119 KB    | ################################################################################################################################################################### | 100% \nopenssl-3.0.5        | 2.8 MB    | ################################################################################################################################################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n#\n# To activate this environment, use\n#\n#     $ conda activate cutadapt-4.1\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n\n\n\n\n\n\n\n\n\n5.4 Activating conda environments\nWhereas we use the term “load” for Lmod modules, we use “activate” for conda environments — it means the same thing. Oddly enough, the most foolproof way to activate a conda environment is to use source activate rather than the expected conda activate — for instance:\n\nsource activate cutadapt-4.1\n\n\n(cutadapt-4.1) [jelmer@pitzer-login03 PAS2250]$\n\n\n\n\n\n\n\nEnvironment indicator\n\n\n\nWhen we have an active conda environment, its name is conveniently displayed in our prompt, as depicted above.\n\n\nAfter we have activated the cutadapt environment, we should be able to actually use the program. To test this, we’ll again simply run it with a --help option:\n\ncutadapt --help | head     # I'm piping into head to avoid pages worth of output \n\n\ncutadapt version 4.1\nCopyright (C) 2010-2022 Marcel Martin marcel.martin@scilifelab.se\ncutadapt removes adapter sequences from high-throughput sequencing reads.\nUsage:\n        cutadapt -a ADAPTER [options] [-o output.fastq] input.fastq\nFor paired-end reads:\n\n\n\n\n5.5 Creating an environment for any program\nMinor variations on the conda create command above can be used to install almost any program for which is conda package is available. However, you may be wondering how we would know:\n\nWhether the program is available and what its conda package’s name is\nWhich conda channel we should use\nWhich versions are available\n\nMy strategy to finding these things out is to simply Google the program name together with “conda”, e.g. cutadapt conda.\nLet’s see that in action:\n\n\n\n\nWe click on that first link (it should always be the first Google hit):\n\n\n\nI always take the top of the two example installation commands as a template, here: conda install -c bioconda cutadapt\nYou may notice the install subcommand, which we haven’t yet seen. This would install Cutadapt into the currently activated conda environment. Since our strategy here –and my general strategy– is to create a new environment each time you’re installing a program, the all-in-one command is to replace install by create -y -n <env-name>.\nThen, our full command (without version specification) again becomes:\n\nconda create -y -n cutadapt -c bioconda cutadapt\n\nTo see which version will be installed by default, and to see which older versions are available:\n\n\n\nFor almost any other program, this works exactly the same!\n\n\n5.6 Lines to add to your Bash script\nWhile you’ll typically want to do installation interactively and only need to do to it once (see note below), you should always include the necessary code to load/activate your programs in your shell scripts.\nWhen your program is in an Lmod module, this simply entails a module load call — e.g., for fastqc:\n\n#!/bin/bash\nset -ueo pipefail\n\n# Load software\nmodule load fastqc\n\nWhen your program is available in a conda environment, this entails a module load command to load conda itself, followed by a source activate command to load the relevant conda environment:\n\n#!/bin/bash\n\n# Load software\nmodule load miniconda3\nsource activate cutadapt-4.1\n\n# Strict/safe Bash settings \nset -ueo pipefail\n\n\n\n\n\n\n\nWarning\n\n\n\nWe’ve moved the set -ueo pipefail line below the source activate command, because the conda activation procedure can sometims throw “unbound variable” errors otherwise.\n\n\n\n\n\n\n\n\nInstall once, load/activate always\n\n\n\n\nProvided you don’t need to switch versions, you only need to install a program once. This is true also at OSC and also when using conda: your environments won’t disappear unless you delete them.\nIn every single “session” that you want to use a program via an Lmod module or a conda environment, you need to load/activate the program. So the line(s) to do so should always be in your script for that program."
  },
  {
    "objectID": "modules/07-software.html#addendum-a-few-other-useful-conda-commands",
    "href": "modules/07-software.html#addendum-a-few-other-useful-conda-commands",
    "title": "Using Software at OSC",
    "section": "6 Addendum: a few other useful conda commands",
    "text": "6 Addendum: a few other useful conda commands\n\nDeactivate the currently active conda environment:\n\nconda deactivate   \n\nActivate one environment and then “stack” an additional environment (a regular conda activate command would switch environments):\n\nsource activate cutadapt         # Now, the env \"cutadapt\" is active\nconda activate --stack multiqc   # Now, both \"cutadapt\" and \"multiqc\" are active\n\nRemove an environment entirely:\n\nconda env remove -n cutadapt\n\nList all your conda environments:\n\nconda env list\n\nList all packages (programs) installed in an environment:\n\nconda list -n cutadapt"
  },
  {
    "objectID": "modules/01-intro.html#what-you-will-learn",
    "href": "modules/01-intro.html#what-you-will-learn",
    "title": "Introduction to the Workshop",
    "section": "What you will learn",
    "text": "What you will learn\nThe focus of this workshop is building some general (foundational) skills for analyzing genomics data — specifically, for doing so with command-line programs at the Ohio Supercomputer Center (OSC).\nCommand-line programs are preferred for many of the steps to analyze genomic sequencing data. Because such datasets tend to contain a lot of data, it is also preferable to run your analyses not on a laptop or desktop, but at a supercomputer like OSC.\nThese realities mean that in the field of genomics, you need the following set of skills that you may not have been taught previously:\n\nHaving a basic understanding of a supercomputer\n\nAnd being able to:\n\nUse the Unix shell (work in a terminal)\nWrite small shell scripts that run command-line programs\nSubmit scripts to a supercomputer’s “queue” — and monitor & manage the resulting batch jobs\nActivate and probably install software in a Linux environment where you don’t have admin rights\n\nWe will teach the basics of these skills during this workshop!\n\n\n\n\n\n\nWhat you won’t learn\n\n\n\nIt may be useful to point out that we will not teach you much, if anything, about:\n\nDetails of genomic data file types — except, briefly, FASTQ\nDetails of specific (genomic) analyses\nMaking biological inferences from your data"
  },
  {
    "objectID": "modules/01-intro.html#practicalities",
    "href": "modules/01-intro.html#practicalities",
    "title": "Introduction to the Workshop",
    "section": "Practicalities",
    "text": "Practicalities\nWe have a slightly complicated set up with participants in-person in Wooster with an instructor (Jelmer now via Zoom), in-person in Columbus with an instructor, and directly via Zoom. Some notes:\n\nThe workshop consists of a series of 9 modules: see the schedule. We intend to respect the start and end times for every day, but individual modules may take shorter or longer than indicated below.\nWe have one 10-15 minute break between two modules every day, and another impromptu bathroom break in a longer module.\nThe instructors will be available for additional questions from about 15 minutes before we start, and for about 30 minutes after we end each day.\nThis website has all the material that we will go through, with one page for each of module. See the links in the schedule as well as in the top bar menus to access it.\nIn-person participants don’t need to connect to the Zoom call, since Zoom will be broadcast on the large screen (but you can of course connect if you can better see the instructor’s screen that way).\nBecause we’re not all on Zoom, we’ll try to avoid the Zoom chat and instead use this Google Doc to share links, inpromptu code that is not on the website, and non-urgent questions.\nWhenever you have a question, please feel free to interrupt and speak up, both in-person and on Zoom. Because we will mute the in-person rooms on the Zoom call by default, signal to Mike (Columbus) or Menuka (Wooster) when you have a question, who will then unmute the room.\nIf your question is not urgent and you don’t want to interrupt the flow, put it in the Google Doc or ask about it during a break."
  },
  {
    "objectID": "modules/01-intro.html#sign-up-form-responses",
    "href": "modules/01-intro.html#sign-up-form-responses",
    "title": "Introduction to the Workshop",
    "section": "Sign-up form responses",
    "text": "Sign-up form responses"
  },
  {
    "objectID": "modules/01-intro.html#personal-introductions",
    "href": "modules/01-intro.html#personal-introductions",
    "title": "Introduction to the Workshop",
    "section": "Personal introductions",
    "text": "Personal introductions\n\nInstructors\n\nJelmer Poelstra, Molecular and Cellular Imaging Center (MCIC), Wooster\nMike Sovic, Center for Applied Plant Sciences (CAPS), Wooster\n\nA special mention goes out to Menuka Bhandari who is managing the Selby in-person room. Menuka is also knowledgeable about the workshop material and may be able to help you if you need/prefer someone in the room itself for a certain question.\n\n\nParticipants\nPlease very briefly introduce yourself — include your position, department, and why you wanted to go to this workshop."
  },
  {
    "objectID": "modules/05-vars-loops.html",
    "href": "modules/05-vars-loops.html",
    "title": "Variables, Globbing, and Loops",
    "section": "",
    "text": "In this module, we will cover a few topics that are good to know about before you start writing and running shell scripts:\nThese are valuable skills in general — globbing is an essential technique in the Unix shell, and variables and for loops ubiquitous programming concepts."
  },
  {
    "objectID": "modules/05-vars-loops.html#setup",
    "href": "modules/05-vars-loops.html#setup",
    "title": "Variables, Globbing, and Loops",
    "section": "1 Setup",
    "text": "1 Setup\nStarting a VS Code session with an active terminal:\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 4 or more in the box Number of hours\nTo avoid having to switch folders within VS Code, enter /fs/ess/scratch/PAS2250/participants/<your-folder> in the box Working Directory (replace <your-folder> by the actual name of your folder).\nClick Launch.\n\nOn the next page, once the top bar of the box is green and says Runnning, click Connect to VS Code.\nOpen a terminal:    => Terminal => New Terminal.\nIn the terminal, type bash and press Enter.\nType pwd in the termain to check you are in /fs/ess/scratch/PAS2250.\nIf not, click    =>   File   =>   Open Folder and enter /fs/ess/scratch/PAS2250/<your-folder>."
  },
  {
    "objectID": "modules/05-vars-loops.html#variables",
    "href": "modules/05-vars-loops.html#variables",
    "title": "Variables, Globbing, and Loops",
    "section": "2 Variables",
    "text": "2 Variables\nIn programming, we use variables for things that:\n\nWe refer to repeatedly and/or\nAre subject to change.\n\nThese tend to be settings like the paths to input and output files, and parameter values for programs.\nUsing variables makes it easier to change such settings. We also need to understand variables to work with loops and with scripts.\n\n2.1 Assigning and referencing variables\nTo assign a value to a variable in Bash (in short: to assign a variable), use the syntax variable=value:\n\n# Assign the value \"beach\" to the variable \"location\":\nlocation=beach\n\n# Assign the value \"200\" to the variable \"nlines\":\nnlines=200\n\n\n\n\n\n\n\nBe aware: don’t put spaces around the equals sign (=)!\n\n\n\n\n\n\nTo reference a variable (i.e., to access its value), you need to put a dollar sign $ in front of its name. We’ll use the echo command to review the values that our variables contain:\n\n\n\n\n\n\necho simply prints back (“echoes”) whatever you tell it to\n\n\n\n\necho Hello!\n\nHello!\n\n\n\n\n\necho $location\n\n\n\nbeach\n\n\n\necho $nlines\n\n\n\n200\n\n\nConveniently, we can directly use variables in lots of contexts, as if we had instead typed their values:\n\ninput_file=data/fastq/SRR7609467.fastq.gz\n\nls -lh $input_file \n\n-rw-r--r-- 1 jelmer jelmer 8.3M Aug 16 13:45 data/fastq/SRR7609467.fastq.gz\n\n\n\nls_options=\"-lh\"            # (We'll talk about the quotes that are used here later)\n\nls $ls_options data/meta\n\ntotal 4.0K\n-rw-r--r-- 1 jelmer jelmer 583 Aug 16 10:36 meta.tsv\n\n\n\n\n\n2.2 Rules and tips for naming variables\nVariable names:\n\nCan contain letters, numbers, and underscores\nCannot contain spaces, periods, or other special symbols\nCannot start with a number\n\nTry to make your variable names descriptive, like $input_file and $ls_options above, as opposed to say $x and $bla.\nThere are multiple ways of distinguishing words in the absence of spaces, such as $inputFile and $input_file: I prefer the latter, which is called “snake case”, and I always use lowercase.\n\n\n\n2.3 Quoting variables\nAbove, we learned that a variable name cannot contain spaces. But what happens if our variable’s value contains spaces? First off, when we try to assign the variable without using quotes, we get an error:\n\ntoday=Thu, Aug 18\n\n\nAug: command not found\n\n\n\n\n\n\n\nWhy do you think we got this error?\n\n\n\n\n\nBash tried assign everything up to the first space (i.e., Thu,) to today. After that, since we used a space, it assumed the next word (Aug) was something else: specifically, another command.\n\n\n\nBut it works when we quote (with double quotes, \"...\") the entire string that makes up the value:\n\ntoday=\"Thu, Aug 18\"\necho $today\n\nThu, Aug 18\n\n\n\nNow, let’s try to reference this variable in another context. Note that the touch command can create new files, e.g. touch a.txt creates the file a.txt. So let’s try make a new file with today’s date:\n\n\n\n\ntouch README_$today.txt\nls\n\n\n18.txt\nAug\nREADME_Thu,\n\n\n\n\n\n\n\nWhat went wrong here?\n\n\n\n\n\nThe shell performed so-called field splitting using spaces as a separator, splitting the value into three separate units – as a result, three files were created.\n\n\n\nLike with assignment, our problems can be avoided by quoting a variable when we reference it:\n\ntouch README_\"$today\".txt\n\n# This will list the most recently modified file (ls -t sorts by last modified date):\nls -t | head -n 1\n\n\n\nREADME_Thu, Aug 18.txt\n\n\nIt is good practice to quote variables when you reference them: it never hurts, and avoids unexpected surprises.\n\n\n\n\n\n\nAt-home reading: Where does a variable name end?\n\n\n\n\n\nAnother issue we can run into when we don’t quote variables is that we can’t explicitly define where a variable name ends within a longer string of text:\n\necho README_$today_final.txt\n\n\n\nREADME_.txt\n\n\n\n\n\n\n\n\nWhat went wrong here? (Hint: check the coloring highlighting above)\n\n\n\n\n\n\nFollowing a $, the shell will stop interpreting characters as being part of the variable name only when it encounters a character that cannot be part of a variable name, such as a space or a period.\nSince variable names can contain underscores, it will look for the variable $today_final, which does not exist.\nImportantly, the shell does not error out when you reference a non-existing variable – it basically ignores it, such that README_$today_final.txt becomes README_.txt, as if we hadn’t referenced any variable.\n\n\n\n\nQuoting solves this issue, too:\n\necho README_\"$today\"_final.txt\n\n\n\nREADME_Thu, Aug 18_final.txt\n\n\n\n\n\n\n\n\n\n\n\nAt-home reading: Quoting as “escaping” special meaning – and double vs. single quotes\n\n\n\n\n\nBy double-quoting a variable, we are essentially escaping (or “turning off”) the default special meaning of the space as a separator, and are asking the shell to interpret it as a literal space.\nSimilarly, we are escaping other “special characters”, such as globbing wildcards, with double quotes. Compare:\n\necho *     # This will echo/list all files in the current working dir (!)\n\n18.txt Aug data README_Thu, README_Thu, Aug 18.txt sandbox scripts\n\n\n\necho \"*\"   # This will simply print the \"*\" character \n\n*\n\n\nHowever, as we saw above, double quotes do not turn off the special meaning of $ (denoting a string as a variable):\n\necho \"$today\"\n\n\n\nThu, Aug 18\n\n\n…but single quotes will:\n\necho '$today'\n\n$today\n\n\n\n\n\n\n\n\n2.4 Command substitution\nIf you want to store the result of a command in a variable, you can use a construct called “command substitution” by wrapping the command inside $().\nLet’s see an example. The date command will print the current date and time:\n\ndate\n\nFri Aug 19 11:01:06 PM CEST 2022\n\n\nIf we try to store the date in a variable directly, it doesn’t work: the literal string “date” is stored, not the output of the command:\n\ntoday=date\necho \"$today\"\n\ndate\n\n\nThat’s why we need command substitution with $():\n\ntoday=$(date)\necho \"$today\"\n\nFri Aug 19 11:01:06 PM CEST 2022\n\n\n\nIn practice, you might use command substitution with date to include the current date in files. To do so, first, note that we can use date +%F to print the date in YYYY-MM-DD format, and omit the time:\n\ndate +%F\n\n2022-08-19\n\n\nLet’s use that in a command substitution — but a bit differently than before: we use the command substitution $(date +%F) directly in our touch command, rather than first assigning it to a variable:\n\n# Create a file with our $today variable:\ntouch README_\"$(date +%F)\".txt\n\n# Check the name of our newly created file:\nls -t | head -n 1\n\n\n\nREADME_2022-08-19.txt\n\n\nAmong many other uses, command substitution is handy when you want your script to report some results, or when a next step in the script depends on a previous result.\n\n\nOn Your Own: Command substitution\nSay we wanted to store and report the number of lines in a file, which can be a good QC measure for FASTQ and other genomic data files.\nwc -l gets you the number of lines, and you can use a trick to omit the filename:\n\nwc -l data/fastq/SRR7609472.fastq.gz\n\n30387 data/fastq/SRR7609472.fastq.gz\n\n\n\n# Use `<` (input redirection) to omit the filename:\nwc -l < data/fastq/SRR7609472.fastq.gz\n\n30387\n\n\nUse command substitution to store the output of the last command in a variable, and then use an echo command to print:\nThe file has 30387 lines\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nnlines=$(wc -l < data/fastq/SRR7609472.fastq.gz)\n\necho \"The file $nlines lines\"\n\nThe file 30387 lines\n\n\nNote: You don’t have to quote variables inside a quoted echo call, since it’s, well, already quoted. If you also quote the variables, you will in fact unquote it, although that shouldn’t pose a problem inside echo statements.\n\n\n\n\n\n\n2.5 At-home reading: Environment variables\n\n\n\n\n\n\nEnvironment variable basics\n\n\n\n\n\nThere are also predefined variables in the Unix shell: that is, variables that exist in your environment by default. These so-called “environment variables” are always spelled in all-caps:\n\n# Environment variable $USER contains your user name \necho $USER\n\njelmer\n\n\n\n# Environment variable $HOME contains the path to your home directory\necho $HOME\n\n\n/users/PAS0471/jelmer\n\nEnvironment variables can provide useful information. They can especially come in handy in in scripts submitted to the Slurm compute job scheduler."
  },
  {
    "objectID": "modules/05-vars-loops.html#globbing-with-shell-wildcard-expansion",
    "href": "modules/05-vars-loops.html#globbing-with-shell-wildcard-expansion",
    "title": "Variables, Globbing, and Loops",
    "section": "3 Globbing with Shell wildcard expansion",
    "text": "3 Globbing with Shell wildcard expansion\nShell wildcard expansion is a very useful technique to select files. Selecting files with wildcard expansion is called globbing.\n\n3.1 Shell wildcards\nIn the term “wildcard expansion”, wildcard refers to a few symbols that have a special meaning: specifically, they match certain characters in file names. We’ll see below what expansion refers to.\nHere, we’ll only talk about the most-used wildcard, *, in detail. But for the sake of completeness, I list them all below:\n\n\n\n\n\n\n\nWildcard\nMatches\n\n\n\n\n*\nAny number of any character, including nothing\n\n\n?\nAny single character\n\n\n[] and [^]\nOne [] or everything except one ([^]) of the “character set” within brackets\n\n\n\n\n\n\n3.2 The * wildcard and wildcard expansion\nA a first example of using *, to match all files in a directory:\n\nls data/fastq/*\n\ndata/fastq/SRR7609467.fastq.gz\ndata/fastq/SRR7609468.fastq.gz\ndata/fastq/SRR7609469.fastq.gz\ndata/fastq/SRR7609470.fastq.gz\ndata/fastq/SRR7609471.fastq.gz\ndata/fastq/SRR7609472.fastq.gz\ndata/fastq/SRR7609473.fastq.gz\ndata/fastq/SRR7609474.fastq.gz\ndata/fastq/SRR7609475.fastq.gz\ndata/fastq/SRR7609476.fastq.gz\ndata/fastq/SRR7609477.fastq.gz\ndata/fastq/SRR7609478.fastq.gz\n\n\nOf course ls data/fastq would have shown the same files, but what happens under the hood is different:\n\nls data/fastq — The ls command detects and lists all files in the directory\nls data/fastq/* — The wildcard * is expanded to all matching files, (in this case, all the files in this directory), and then that list of files is passed to ls. This command is therefore equivalent to running:\n\nls data/fastq/SRR7609467.fastq.gz data/fastq/SRR7609468.fastq.gz data/fastq/SRR7609469.fastq.gz data/fastq/SRR7609470.fastq.gz data/fastq/SRR7609471.fastq.gz data/fastq/SRR7609472.fastq.gz data/fastq/SRR7609473.fastq.gz data/fastq/SRR7609474.fastq.gz data/fastq/SRR7609475.fastq.gz data/fastq/SRR7609476.fastq.gz data/fastq/SRR7609477.fastq.gz data/fastq/SRR7609478.fastq.gz\n\n\nTo see this, note that we don’t need to use ls at all to get a listing of these files!\n\necho data/fastq/*\n\ndata/fastq/SRR7609467.fastq.gz data/fastq/SRR7609468.fastq.gz data/fastq/SRR7609469.fastq.gz data/fastq/SRR7609470.fastq.gz data/fastq/SRR7609471.fastq.gz data/fastq/SRR7609472.fastq.gz data/fastq/SRR7609473.fastq.gz data/fastq/SRR7609474.fastq.gz data/fastq/SRR7609475.fastq.gz data/fastq/SRR7609476.fastq.gz data/fastq/SRR7609477.fastq.gz data/fastq/SRR7609478.fastq.gz\n\n\n\nA few more examples:\n\n# This will still list all 12 FASTQ files --\n# can be a good pattern to use to make sure you're not selecting other types of files \nls data/fastq/*fastq.gz\n\ndata/fastq/SRR7609467.fastq.gz\ndata/fastq/SRR7609468.fastq.gz\ndata/fastq/SRR7609469.fastq.gz\ndata/fastq/SRR7609470.fastq.gz\ndata/fastq/SRR7609471.fastq.gz\ndata/fastq/SRR7609472.fastq.gz\ndata/fastq/SRR7609473.fastq.gz\ndata/fastq/SRR7609474.fastq.gz\ndata/fastq/SRR7609475.fastq.gz\ndata/fastq/SRR7609476.fastq.gz\ndata/fastq/SRR7609477.fastq.gz\ndata/fastq/SRR7609478.fastq.gz\n\n\n\n# Only select the ...67.fastq.gz, ...68.fastq.gz, and ...69.fastq.gz files \nls data/fastq/SRR760946*fastq.gz\n\ndata/fastq/SRR7609467.fastq.gz\ndata/fastq/SRR7609468.fastq.gz\ndata/fastq/SRR7609469.fastq.gz\n\n\n\n\n\n\n\n\nWhat pattern would you use if you wanted to select all gzipped (.fastq.gz) and plain FASTQ files (.fastq) at the same time?\n\n\n\n\n\n\nls data/fastq/SRR760946*.fastq*\n\nThe second * will match filenames with nothing after .fastq as well as file names with characters after .fastq, such as .gz.\n\n\n\n\n\n\n3.3 Common uses of globbing\nWhat can we use this for, other than listing matching files? Below, we’ll use globbing to select files to loop over. Even more commonly, we can use this to move (mv), copy (cp), or remove (rm) multiple files at once. For example:\n\ncp data/fastq/SRR760946* .     # Copy 3 FASTQ files to your working dir \nls *fastq.gz                   # Check if they're here\n\nSRR7609467.fastq.gz\nSRR7609468.fastq.gz\nSRR7609469.fastq.gz\n\n\n\nrm *fastq.gz                  # Remove all FASTQ files in your working dir\nls *fastq.gz                  # Check if they're here\n\n\n\n\n\nls: cannot access ’*fastq.gz’: No such file or directory\n\nFinally, let’s use globbing to remove the mess of files we made when learning about variables:\n\nrm README_*\nrm Aug 18.txt\n\n\n\n\n\n\n\nDon’t confuse shell wildcards with regular expressions!\n\n\n\n\n\nFor those of you who know some regular expressions: these are conceptually similar to wildcards, but the * and ? symbols don’t have the same meaning, and there are way fewer shell wildcards than regular expression symbols.\nIn particular, note that . is not a shell wildcard and thus represents a literal period."
  },
  {
    "objectID": "modules/05-vars-loops.html#for-loops",
    "href": "modules/05-vars-loops.html#for-loops",
    "title": "Variables, Globbing, and Loops",
    "section": "4 For loops",
    "text": "4 For loops\nLoops are a universal element of programming languages, and are used to repeat operations, such as when you want to run the same script or command for multiple files.\nHere, we’ll only cover what is by far the most common type of loop: the for loop.\nfor loops iterate over a collection, such as a list of files: that is, they allow you to perform one or more actions for each element in the collection, one element at a time.\n\n4.1 for loop syntax and mechanics\nLet’s see a first example, where our “collection” is just a very short list of numbers (1, 2, and 3):\n\nfor a_number in 1 2 3; do\n    echo \"In this iteration of the loop, the number is $a_number\"\n    echo \"--------\"\ndone\n\nIn this iteration of the loop, the number is 1\n--------\nIn this iteration of the loop, the number is 2\n--------\nIn this iteration of the loop, the number is 3\n--------\n\n\nfor loops contain the following mandatory keywords:\n\n\n\n\n\n\n\nKeyword\nPurpose\n\n\n\n\nfor\nAfter for, we set the variable name\n\n\nin\nAfter in, we specify the collection we are looping over\n\n\ndo\nAfter do, we have one ore more lines specifying what to do with each item\n\n\ndone\nTells the shell we are done with the loop\n\n\n\n\n\n\n\n\n\nA semicolon ; (as used before do) separates two commands on a single line\n\n\n\n\n\nA semicolon separates two commands written on a single line – for instance, instead of:\n\nmkdir results\ncd results\n\n…you could equivalently type:\n\nmkdir results; cd results\n\nThe ; in the for loop syntax has the same function, and as such, an alternative way to format a for loop is:\n\nfor a_number in 1 2 3\ndo\n    echo \"In this iteration of the loop, the number is $a_number\"\ndone\n\nBut that’s one line longer and a bit awkwardly asymmetric.\n\n\n\nThe aspect that is perhaps most difficult to understand is that in each iteration of the loop, one element in the collection (in the example above, either 1, 2, or 3) is being assigned to the variable specified after for (in the example above, a_number).\n\nIt is also important to realize that the loop runs sequentially for each item in the collection, and will run exactly as many times as there are items in the collection.\nThe following example, where we let the computer sleep for 1 second before printing the date and time with the date command, demonstrates that the loop is being executed sequentially:\n\nfor a_number in 1 2 3; do\n    echo \"In this iteration of the loop, the number is $a_number\"\n    sleep 1s          # Let the computer sleep for 1 second\n    date              # Print the date and time\n    echo \"--------\"\ndone\n\nIn this iteration of the loop, the number is 1\nFri Aug 19 11:01:08 PM CEST 2022\n--------\nIn this iteration of the loop, the number is 2\nFri Aug 19 11:01:09 PM CEST 2022\n--------\nIn this iteration of the loop, the number is 3\nFri Aug 19 11:01:10 PM CEST 2022\n--------\n\n\n\n\nOn Your Own: A simple loop\nCreate a loop that will print:\nmorel is an Ohio mushroom  \ndestroying_angel is an Ohio mushroom  \neyelash_cup is an Ohio mushroom\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nJust like we looped over 3 numbers above (1, 2, and 3), you want to loop over the three mushroom names, morel, destroying_angel, and eyelash_cup.\nNotice that when we specify the collection “manually”, like we did above with numbers, the elements are simply separated by a space.\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nfor mushroom in morel destroying_angel eyelash_cup; do\n    echo \"$mushroom is an Ohio mushroom\"\ndone\n\nmorel is an Ohio mushroom\ndestroying_angel is an Ohio mushroom\neyelash_cup is an Ohio mushroom\n\n\n\n\n\n\n\n\n4.2 Looping over files with globbing\nIn practice, we rarely manually list the collection of items we want to loop over. Instead, we commonly loop over files directly using globbing:\n\n# We make sure we only select gzipped FASTQ files using the `*fastq.gz` glob\nfor fastq_file in data/raw/*fastq.gz; do\n    echo \"File $fastq_file has $(wc -l < $fastq_file) lines.\"\n    # More processing...\ndone\n\nThis technique is extremely useful, and I use it all the time. Take a moment to realize that we’re not doing a separate ls and storing the results: as mentioned, we can directly use a globbing pattern to select our files.\nIf needed, you can use your globbing / wild card skills to narrow down the file selection:\n\n# Perhaps we only want to select R1 files (forward reads): \nfor fastq_file in data/raw/*R1*fastq.gz; do\n    # Some file processing...\ndone\n\n# Or only filenames starting with A or B:\nfor fastq_file in data/raw/[AB]*fastq.gz; do\n    # Some file processing...\ndone\n\n\n\n\n\n\n\nAt-home reading: Alternatives to looping with a glob\n\n\n\n\n\nWith genomics data, the routine of looping over an entire directory of files, or selections made with simple globbing patterns, should serve you very well.\nBut in some cases, you may want to iterate only over a specific list of filenames (or partial filenames such as sample IDs) that represent a complex selection.\n\nIf this is a short list, you could directly specify it in the loop:\n\nfor sample in A1 B6 D3; do\n    R1=data/fastq/\"$sample\"_R1.fastq.gz\n    R2=data/fastq/\"$sample\"_R2.fastq.gz\n    # Some file processing...\ndone\n\nIf it is a longer list, you could create a simple text file with one line per sample ID / filename, and use command substitution as follows:\n\nfor fastq_file in $(cat file_of_filenames.txt); do\n    # Some file processing...\ndone\n\n\nIn cases like this, Bash arrays (basically, variables that consist of multiple values, like a vector in R) or while loops may provide more elegant solutions, but those are outside the scope of this introduction."
  },
  {
    "objectID": "modules/03-vscode.html",
    "href": "modules/03-vscode.html",
    "title": "The VS Code Text Editor",
    "section": "",
    "text": "In this module, we will learn the basics of a fancy text editor called VS Code (in full, Visual Studio Code). Conveniently, we can use a version of this editor (sometimes referred to as Code Server) in our browser via the OSC OnDemand website.\nWe will use VS Code throughout the workshop as practically a one-stop solution for our computing activities at OSC. This is also how I use this editor in my daily work.\nTo emphasize the additional functionality relative to basic text editors like Notepad and TextEdit, editors like VS Code are also referred to as “IDEs”: Integrated Development Environments. If you’ve ever worked with R, the RStudio program is another good example of an IDE."
  },
  {
    "objectID": "modules/03-vscode.html#starting-vs-code-at-osc",
    "href": "modules/03-vscode.html#starting-vs-code-at-osc",
    "title": "The VS Code Text Editor",
    "section": "1 Starting VS Code at OSC",
    "text": "1 Starting VS Code at OSC\nIn the previous module, Mike already guided us through starting a VS Code session via OnDemand, but for the sake of completeness, instructions to do so are also shown below.\n\n\n\n\n\n\nStarting VS Code at OSC\n\n\n\n\n\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nMake sure that Number of hours is at least 4\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code."
  },
  {
    "objectID": "modules/03-vscode.html#getting-started-with-vs-code",
    "href": "modules/03-vscode.html#getting-started-with-vs-code",
    "title": "The VS Code Text Editor",
    "section": "2 Getting started with VS Code",
    "text": "2 Getting started with VS Code\n\n\n\n\n2.1 Side bars\nThe Activity Bar (narrow side bar) on the far left has:\n\nA      (“hamburger menu” icon) in the top, which has most of the standard menu items that you often find in a top bar, like File.\nA      (cog wheel icon) in the bottom, through which you can mainly access settings.\nA bunch of icons in the middle that serve to switch between different options for the (wide) Side Bar, which can show one of the following:\n\nExplorer: File browser (and, e.g., an outline for the active file)\nSearch: To search recursively across all files in the active folder\nSource Control: To work with version control systems like Git (not used in this workshop)\nRun and Debug: For debugging your code (not used in this workshop)\nExtensions: To install extensions (we’ll install one later)\n\n\n\n\n2.2 Editor pane and Welcome document\nThe main part of the VS Code is the editor pane. Whenever you open VS Code, a tab with a Welcome document is automatically opened. This provides some help for beginners, but also, for example, a handy overview of recently opened folders.\nWe can also use the Welcome document to open a new text file by clicking New file below Start (alternatively, click      =>   File   =>   New File), which open as a second “tab” in the editor pane. We’ll work with our own text files (scripts) starting tomorrow.\n\n\n\n\n\n\nRe-open the Welcome document\n\n\n\nIf you’ve closed the Welcome document but want it back, click      =>   Help   =>   Welcome.\n\n\n\n\n2.3 Terminal (with a Unix shell)\n By default, no terminal is open in VS Code – open one by clicking      => Terminal => New Terminal.\nIn the terminal, the prompt says Singularity>. This is because in OSC OnDemand, VS Code runs inside a Singularity container (for our purposes, it is not important what that means, exactly).\n Break out of the Singularity shell to get a regular Bash Unix shell: type bash and press Enter.\nIn the next module, Mike will teach us how to use the Unix shell."
  },
  {
    "objectID": "modules/03-vscode.html#a-folder-as-a-starting-point",
    "href": "modules/03-vscode.html#a-folder-as-a-starting-point",
    "title": "The VS Code Text Editor",
    "section": "3 A folder as a starting point",
    "text": "3 A folder as a starting point\nConveniently, VS Code takes a specific folder (directory) as a starting point in all parts of the program:\n\nIn the file explorer in the side bar\nIn the terminal\nWhen saving files in the editor pane.\n\nBy default, VS Code via OnDemand will open your Home directory. But in this workshop, we’ll work within the scratch directory for the PAS2250 project, to which you have all been added, which is /fs/ess/scratch/PAS2250.\n Let’s open that folder. Click Open folder... in the Welcome tab (or      =>   File   =>   Open Folder).\nYou’ll notice that the program completely reloads.\n\n\n\n\n\n\nTaking off where you were\n\n\n\nAdditionally, when you reopen a folder later, VS Code will to some extent resume where you were before!\nIt will reopen any files you had open, and if you had an active terminal, it will re-open a terminal. This is very convenient, especially when you start working on multiple projects (different folders) in VS Code and frequently switch between those."
  },
  {
    "objectID": "modules/03-vscode.html#some-vs-code-tips-and-tricks",
    "href": "modules/03-vscode.html#some-vs-code-tips-and-tricks",
    "title": "The VS Code Text Editor",
    "section": "4 Some VS Code tips and tricks",
    "text": "4 Some VS Code tips and tricks\n\n4.1 Making use of your screen’s real estate\nSince we are using VS Code inside a browser window, we are unfortunately losing some screen space. Make sure to maximize the browser window and if you have a bookmarks bar, try to hide it (for Chrome: Ctrl/⌘+Shift+B).\nYou may also opt to hide the side bars using the    =>   View   =>   Appearance menu (or Ctrl/⌘+B for the (wide) Side Bar).\n\n\n4.2 Resizing panes\nYou can resize panes (the terminal, editor, and side bar) by hovering your cursor over the borders and then dragging it.\n\n\n4.3 The Command Palette / Color themes\nTo access all the menu options that are available in VS Code, the so-called “Command Palette” can be handy, especially if you know what you are looking for.\nTo access the Command Palette, click      and then Command Palette (or press F1 or Ctrl/⌘+Shift+P).\n\nOn Your Own: Try a few color themes\nOpen the Command Palette and start typing “color theme”, and you’ll see the relevant option pop up.\nThen, try out a few themes and see what you like!\n(You can also access the Color Themes option via      => Color Theme.)\n\n\n\n\n\n\n“Solution” (click here)\n\n\n\n\n\n“Quiet Light” is the best one"
  },
  {
    "objectID": "modules/03-vscode.html#addendum-keyboard-shortcuts",
    "href": "modules/03-vscode.html#addendum-keyboard-shortcuts",
    "title": "The VS Code Text Editor",
    "section": "Addendum: keyboard shortcuts",
    "text": "Addendum: keyboard shortcuts\nWorking with keyboard shortcuts (also called “keybindings”) for common operations can be a lot faster than using your mouse. Below are some useful ones for VS Code (for Mac, replace Ctrl with ⌘).\n\n\n\n\n\n\nKeyboard shortcut cheatsheet\n\n\n\nFor a single-page PDF overview of keyboard shortcuts for your operating system:      =>   Help   =>   Keyboard Shortcut Reference. (Or for direct links to these PDFs: Windows / Mac / Linux.)\n\n\n\nOpen a terminal: Ctrl+` or Ctrl+Shift+C.\nToggle between the terminal and the editor pane: Ctrl+` and Ctrl+1.\nToggle the (wide) Side Bar: Ctrl+B\nLine actions:\n\nCtrl+X / C will cut/copy the entire line where the cursor is, when nothing is selected (!)\nCtrl+Shift+K will delete a line\nAlt+⬆/⬇ will move lines up or down.\n\nMultiple cursors: Press & hold Ctrl+Shift, then ⬆/⬇ arrows to add cursors upwards or downwards.\nToggle line comment (“comment out” code, and removing those comment signs): Ctrl+/\nSplit the editor window vertically: Ctrl+\\ (See also the options in      View => Editor Layout)\n\n\n\n\n\n\n\nBrowser interference\n\n\n\nUnfortunately, some VS Code and terminal keyboard shortcuts don’t work in this setting where we are using VS Code inside a browser, because existing browser keyboard shortcuts take precedence.\nIf you end up using VS Code a lot in your work, it is therefore worth switching to your own installation of the program (see At-home bonus: local installation)"
  },
  {
    "objectID": "modules/03-vscode.html#at-home-bonus-local-installation",
    "href": "modules/03-vscode.html#at-home-bonus-local-installation",
    "title": "The VS Code Text Editor",
    "section": "At-home bonus: local installation",
    "text": "At-home bonus: local installation\nAnother nice feature of VS Code is that it is freely available for all operating systems (and even though it is made by Microsoft, it is also open source).\nTherefore, if you like the program, you can also install it on your own computer and do your local text editing / script writing in the same environment at OSC (it is also easy to install on OSU-managed computers, because it is available in the Self Service software installer).\nEven better, the program can be “tunneled into” OSC, so that your working directory for the entire program can be at OSC rather than on your local computer. This gives the same experience as using VS Code through OSC OnDemand, except that you’re not working witin a browser window, which has some advantages (also: no need to fill out a form, or to break out of the Singularity shell).\nTo install VS Code on your own machine, follow these instructions: Windows / Mac / Linux.\nTo SSH-tunnel VS Code into OSC, see these instructions (they are a bit rudimentary, ask Jelmer if you get stuck)."
  },
  {
    "objectID": "info/notes.html",
    "href": "info/notes.html",
    "title": "Notes From and After the Workshop",
    "section": "",
    "text": "Copy workshop files to your home directory\nRemember that the PAS2250 directory, where we put our files during the workshop, is a scratch directory: those files will eventually be automatically deleted.\nThe command below would copy the files you created and copied during the workshop to a directory (folder) in your home directory called 2022-08_command-line-workshop. In the command, replace <your-folder> by the actual name of your folder.\n\ncp -r /fs/ess/scratch/PAS2250/participants/<your-folder> ~/2022-08_command-line-workshop\n\nFor instance:\n\ncp -r /fs/ess/scratch/PAS2250/participants/mike ~/2022-08_command-line-workshop\n\nNote that ~ is a shortcut to your home directory (you can do echo ~ to check this).\n\n\n\nThings that came up during the workshop\n\nOSC website with help: https://osc.edu, especially:\n\nGetting Started\nHOWTOs for specific tasks, such as transferring files with Globus\n\nCancel, exit, and history with keyboard shortcuts:\n\nCtrl + C to cancel a running process. It can also get you out of situations where you unexpectedly aren’t getting your prompt back, such as when you’ve forgotten to close a quote.\nCtrl + D to exit (same as the command exit). For example, this will close your terminal if you’re on a login node, and send your shell back to a login node if you’re on an interactive compute node.\nCtrl + R: Remember that the shell keeps a record of all of the commands you’ve run and that you can use the up arrow to scroll back through these. Alternatively, this keyboard shortcut allows you to search for text within those commands.\n\nA nice trick to get columns lined up when viewing tab-separated files in the terminal is to use the command column:\n\ncolumn -t data/meta/meta.tsv\n\nAlso, don’t forget that you can simply open such files in the VS Code editor!\nMoreover, the “Rainbow CSV” VS Code extension will help to easily distinguish columns in comma-and tab-separated files (.csv & .tsv file, though note that these are often just saved as .txt, too).\nThe Shellcheck extension is a very useful VS Code extension that checks the code in your shell script for things like: unused variables, referencing non-existent variables, syntax errors, wrong or suboptimal coding practices, and so on. See these two screenshots for an illustration of the functionality:\n\n\n\n\n\n\n\n\n\n\n\n\n\nVS Code extensions\n\n\n\nTo find and install VS Code extensions, click the Extension icon in the Activity Bar (narrow side bar): see this page.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIn VS Code on OnDemand, Shellcheck sometimes fails to install. It should certainly work if you install VS Code on your computer, though!\n\n\n\nFile permissions:\n\n# Give execute permissions to all files in the folder \"myfolder\"\nchmod +x myfolder/*\n\n# Give execute permissions to all shell scripts in the folder \"myfolder\"\nchmod +x myfolder/*sh\n\n# For folders, people need _execute_ permissions to enter them.\n# If you want others to be able to see and copy all your files in a certain folder:\nchmod -R +X myfolder\n\nFore more, see this file permission tutorial."
  },
  {
    "objectID": "info/notes.html#copy-workshop-files-to-your-home-directory",
    "href": "info/notes.html#copy-workshop-files-to-your-home-directory",
    "title": "Workshop Notes",
    "section": "1 Copy workshop files to your home directory",
    "text": "1 Copy workshop files to your home directory\nRemember that the PAS2250 directory, where we put our files during the workshop, is a scratch directory: those files will eventually be automatically deleted.\nThe command below would copy the files you created and copied during the workshop to a directory (folder) in your home directory called 2022-08_command-line-workshop. In the command, replace <your-folder> by the actual name of your folder.\n\ncp -r /fs/ess/scratch/PAS2250/participants/<your-folder> ~/2022-08_command-line-workshop\n\nFor instance:\n\ncp -r /fs/ess/scratch/PAS2250/participants/mike ~/2022-08_command-line-workshop\n\nNote that ~ is a shortcut to your home directory (you can do echo ~ to check this)."
  },
  {
    "objectID": "info/notes.html#things-that-came-up-during-the-workshop",
    "href": "info/notes.html#things-that-came-up-during-the-workshop",
    "title": "Workshop Notes",
    "section": "2 Things that came up during the workshop",
    "text": "2 Things that came up during the workshop\n\n2.1 OSC Help\nOSC website with help: https://osc.edu, especially:\n\nGetting Started\nHOWTOs for specific tasks, such as transferring files with Globus\n\n\n\n\n2.2 A few shell keyboard shortcuts\n\nCtrl + C to cancel a running process. It can also get you out of situations where you unexpectedly aren’t getting your prompt back, such as when you’ve forgotten to close a quote.\nCtrl + D to exit your current shell (same as the command exit). For example, this will close your terminal if you’re on a login node, and send your shell back to a login node if you’re on an interactive compute node.\nCtrl + R: Remember that the shell keeps a record of all of the commands you’ve run and that you can use the up arrow to scroll back through these. Alternatively, this keyboard shortcut allows you to search for text within those commands.\n\n\n\n\n2.3 Viewing (small) text files\nA nice trick to get columns lined up when viewing tab-separated files in the terminal is to use the command column:\n\ncolumn -t data/meta/meta.tsv\n\n\n\n\nAlso, don’t forget that you can simply open such files in the VS Code editor!\nMoreover, the “Rainbow CSV” VS Code extension will help to easily distinguish columns in comma-and tab-separated files (.csv & .tsv file, though note that these are often just saved as .txt, too).\n\n\n\n\n\n\n2.4 Shellcheck\nThe Shellcheck extension is a very useful VS Code extension that checks the code in your shell script for things like: unused variables, referencing non-existent variables, syntax errors, wrong or suboptimal coding practices, and so on. See these two screenshots for an illustration of the functionality:\n\n\n\n\n\n\n\n\n\n\n\n\nVS Code extensions\n\n\n\nTo find and install VS Code extensions, click the Extension icon in the Activity Bar (narrow side bar): see this page.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIn VS Code on OnDemand, Shellcheck sometimes fails to install. It should certainly work if you install VS Code on your computer, though!\n\n\n\n\n\n2.5 File permissions\n\n# Give execute permissions to all files in the folder \"myfolder\"\nchmod +x myfolder/*\n  \n# Give execute permissions to all shell scripts in the folder \"myfolder\"\nchmod +x myfolder/*sh\n  \n# For folders, people need _execute_ permissions to enter them.\n# If you want others to be able to see and copy all your files in a certain folder:\nchmod -R +X myfolder\n\nFore more, see this file permission tutorial."
  },
  {
    "objectID": "info/notes.html#want-to-learn-r",
    "href": "info/notes.html#want-to-learn-r",
    "title": "Workshop Notes",
    "section": "3 Want to learn R?",
    "text": "3 Want to learn R?\nSign up for the OSU Code Club!"
  },
  {
    "objectID": "info/notes.html#want-to-learn-r-sign-up-for-osu-code-cub",
    "href": "info/notes.html#want-to-learn-r-sign-up-for-osu-code-cub",
    "title": "Workshop Notes",
    "section": "3 Want to learn R? Sign up for OSU Code Cub!",
    "text": "3 Want to learn R? Sign up for OSU Code Cub!\nSign up for the OSU Code Club!"
  },
  {
    "objectID": "info/notes.html#want-to-learn-r-sign-up-for-osu-code-club",
    "href": "info/notes.html#want-to-learn-r-sign-up-for-osu-code-club",
    "title": "Workshop Notes",
    "section": "3 Want to learn R? Sign up for OSU Code Club!",
    "text": "3 Want to learn R? Sign up for OSU Code Club!\nSign up for OSU Code Club here!\nWe will restart Code Club in September for the fall semester, working our way through the book R for Data Science."
  }
]