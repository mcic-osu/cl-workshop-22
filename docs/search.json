[
  {
    "objectID": "info/glossary.html",
    "href": "info/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Directory\nSyntax\nString\npseudocode\nbinary (executable)\ndependency"
  },
  {
    "objectID": "info/info.html#locations-and-links",
    "href": "info/info.html#locations-and-links",
    "title": "Practical Workshop Information",
    "section": "Locations and links",
    "text": "Locations and links\n\nColumbus: Aronoff Laboratory, room 104 (instructor: Mike Sovic)\nWooster: Selby Hall, room 203 (instructor: Jelmer Poelstra)\nZoom: email us for the link! (instructor: Jelmer Poelstra)\nGoogle Doc for sharing links and code, and for non-urgent questions"
  },
  {
    "objectID": "info/info.html#computer-setup",
    "href": "info/info.html#computer-setup",
    "title": "Practical Workshop Information",
    "section": "Computer Setup",
    "text": "Computer Setup\n\nYour computer\nSince we will be working entirely at the Ohio Supercomputer Center (OSC), and will be doing so through our internet browsers:\n\nYou won’t need to install anything\nAny operating system will work\nYou won’t need an especially powerful machine (though browsers, especially in combination with Zoom, can use their fair share of memory).\n\nIf you’re attending in person, you will need to bring a laptop. You can watch the presentation on a big screen in the room, which will make it easier to code along (see below). You won’t need to connect to the Zoom call.\nIf you’re attending via Zoom, we would recommend a two-monitor setup. This is because much of the time, you need to be able to simultaneously see the instructor’s screen via Zoom as well as your own browser window.\n\n\nOSC account and project\nTo work with OSC resources, we need access to an “OSC project”. We will be using the project PAS2250 during the workshop, and all participants will be added to that project. If you don’t yet have a personal OSC account, you will receive an invitation to create one when you’ve been added to the project.\n\n\nGoogle Doc\nWe’ll use this Google Doc for sharing links and code, and for non-urgent questions."
  },
  {
    "objectID": "info/info.html#miscellaneous-info",
    "href": "info/info.html#miscellaneous-info",
    "title": "Practical Workshop Information",
    "section": "Miscellaneous info",
    "text": "Miscellaneous info\n\nExpect to participate!\nThe modules will be a mixture of lectures that include “participatory live-coding” (also called “code-along”; with the instructor slowly demonstrating and participants expected to follow along for themselves) and small single-person exercises (we won’t be doing breakout rooms / groups). Therefore, be prepared to actively participate during much of the workshop!\n\n\nExample data\nWe will mainly use a set of FASTQ files from a published RNAseq experiment as example data. (It may be worth emphasizing that the exact data type matters relatively little for the purposes of our workshop, since we focus on foundational skills and not specific genomic analyses.)\nIf you have any genomic data of your own, you can bring it along and you should be able to experiment a bit with it during our second session on Friday afternoon. If this is a large dataset (say, >10GB), uploading it to OSC will take some time. You could try to start this after Wednesday’s sessions, when you’ve had some background on this. Alternatively, you can contact the instructors about this prior to the workshop.\n\n\nParticipants\nWe’re expecting up to 14 people in Wooster, 14 in Columbus, and 6 via Zoom."
  },
  {
    "objectID": "info/about.html",
    "href": "info/about.html",
    "title": "General Workshop Info & Signing Up",
    "section": "",
    "text": "This workshop is geared towards people who would like to get started with analyzing genomic datasets.\nIt will be taught in-person with video-linking at the Wooster and Columbus Ohio State campuses, and it is also possible to join online through Zoom. We will have an instructor at each campus: Jelmer Poelstra from the Molecular and Cellular Imaging Center (MCIC) at the Wooster campus, and Mike Sovic from the Center for Applied Plant Sciences (CAPS) at the Columbus campus.\nThe workshop will be highly hands-on and take place across three afternoons:\nWed, Aug 17 - Fri, Aug 19, 2022.\n\nAnyone affiliated with The Ohio State University or Wooster USDA can attend\nAttendance is free\nNo prior experience with coding or genomic data is required\nYou will need to bring a laptop and don’t need to install anything prior to or during the workshop\nWe will work with example genomics data but if you have any, you are also welcome to bring your own data.\n\nSee below for information about the contents of the workshop and to sign up.\nFor questions, please email Jelmer."
  },
  {
    "objectID": "info/about.html#contents-of-the-workshop",
    "href": "info/about.html#contents-of-the-workshop",
    "title": "General Workshop Info & Signing Up",
    "section": "Contents of the workshop",
    "text": "Contents of the workshop\nThe focus of the workshop is on building general skills for analyzing genomics data, such as RNAseq, metabarcoding, metagenomic shotgun sequencing, or whole-genome sequencing. These skills boil down to the ability to write small shell scripts that run command-line programs and submit these scripts to a compute cluster – in our case, at the Ohio Supercomputer Center (OSC).\n\nTopics\n\nIntroduction to the Ohio Supercomputer Center (OSC)\nUsing the VS Code text editor at OSC\nIntroduction to the Unix shell (= the terminal / command line)\nBasics of shell scripts\nSoftware at OSC with modules & Conda\nSubmitting your scripts using the SLURM scheduler\nPutting it all together: practical examples of running analysis jobs at OSC\n\nThe modules will be a mixture of lectures that include “participatory live-coding” (with the instructor slowly demonstrating and participants expected to follow along for themselves) and exercises.\n\n\nSome more background\nCommand-line programs are preferred for many of the steps to analyze genomic sequencing data, such as those involving quality control, trimming or adapter removal, and assembly or mapping. Other features of such datasets are that they tend to contain a lot of data, and that many analysis steps can be done independently for each sample. It therefore pays off -or may be necessary- to run your analyses not on a laptop or desktop, but at a supercomputer like OSC.\nBeing able to run your analysis with command-line programs at OSC involves a number of skills that may seem overwhelming at first. Fortunately, learning the basics of these skills does not take a lot of time, and will enable you to be up-and-running with working on your own genomic data! Keep in mind that these days, excellent programs are available for almost any genomics analysis, so you do not need to be able to code it all up from scratch. You will just need to know how to efficiently run such programs, which is what this workshop aims to teach you."
  },
  {
    "objectID": "info/about.html#sign-up",
    "href": "info/about.html#sign-up",
    "title": "General Workshop Info & Signing Up",
    "section": "Sign up!",
    "text": "Sign up!\nTo apply to attend the workshop, please fill out the form below. There is no real selection procedure: we accept anyone who is at OSU/USDA and signs up before we have reached the maximum number of participants.\n\nLoading…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Command line basics  for genomic analysis at OSC",
    "section": "",
    "text": "Approximate Schedule\nStart and end times for every day will be respected, but individual modules may take shorter or longer than indicated below. The instructors will be available for additional questions from about 15 minutes before we start and for about 30 minutes after we end.\n\n\n\nDay\nModule\nInstructor\nTime\n\n\n\n\nDay 1\n1. Introduction to the Workshop\nMike / Jelmer\nWed 1:00 - 1:15 pm\n\n\n\n2. The Ohio Supercomputer Center (OSC)\nMike\nWed 1:15 - 1:45 pm\n\n\n\n3. The VS Code Text Editor\nJelmer\nWed 1:45 - 2:00 pm\n\n\n\n4. The Unix Shell (& FASTQ files)\nMike\nWed 2:15 - 4:30 pm\n\n\nDay 2\n5. Variables and Loops\nJelmer\nThu 12:00 - 12:45 pm\n\n\n\n6. Shell Scripting\nJelmer\nThu 12:45 - 1:30 pm\n\n\n\n7. Using Software at OSC\nJelmer\nThu 1:45 - 2:30 pm\n\n\nDay 3\n8. Compute Jobs with Slurm\nJelmer\nFri 1:00 - 2:30 pm\n\n\n\n9. Batch Jobs in Practice\nMike / Jelmer\nFri 2:45 - 4:30 pm"
  },
  {
    "objectID": "sessions/08-slurm.html",
    "href": "sessions/08-slurm.html",
    "title": "Compute Jobs with Slurm",
    "section": "",
    "text": "We have so far been working on login nodes at OSC, but in order to run some actual analyses, you will need access to compute nodes.\nAutomated scheduling software allows hundreds of people with different requirements to access compute nodes effectively and fairly. For this purpose, OSC uses the Slurm scheduler (Simple linux utility for resource management).\nA temporary reservation of (parts of) a compute node is called a compute job. What are the options to start a compute job at OSC?\nWhen running command-line programs for genomics analyses, batch jobs are the most useful and will be the focus of this module. We’ll also touch on interactive shell jobs, which can occasionally be handy and are requested and managed in a very similar way to batch jobs."
  },
  {
    "objectID": "sessions/08-slurm.html#setup",
    "href": "sessions/08-slurm.html#setup",
    "title": "Compute Jobs with Slurm",
    "section": "1 Setup",
    "text": "1 Setup\n\n\n\n\n\n\nStarting a VS Code session with an active terminal (click here)\n\n\n\n\n\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 3 in the box Number of hours\nEnter /fs/ess/scratch/PAS2250 in the box Working Directory\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\nOpen a terminal by clicking the “hamburger menu” way in the top left, and then Terminal > New Terminal.\nIn the terminal, type Bash and press Enter."
  },
  {
    "objectID": "sessions/08-slurm.html#interactive-shell-jobs",
    "href": "sessions/08-slurm.html#interactive-shell-jobs",
    "title": "Compute Jobs with Slurm",
    "section": "2 Interactive shell jobs",
    "text": "2 Interactive shell jobs\nInteractive shell jobs will grant you interactive shell access on a compute node. Working in an interactive shell job is operationally identical to working on a login node as we’ve been doing so far, but the difference is that it’s now okay to use significant computing resources. (How much and for how long depends on what you reserve.)\n\n2.1 Using srun\nA couple of different commands can be used to start an interactive shell job. I prefer the general srun command1, which we can use with the --pty /bin/bash option to get an interactive Bash shell.\nHowever, if we run that command without additional options, we get an error:\n\nsrun --pty /bin/bash\n\n\nsrun: error: ERROR: Job invalid: Must specify account for job\nsrun: error: Unable to allocate resources: Unspecified error\n\nAs the error message Must specify account for job tries to tell us, we need to indicate which OSC project (or as SLURM puts it, “account”) we want to use for this compute job. This is because an OSC project always has to be charged for the computing resources used during a compute job.\nTo specify the project/account, we can use the --account= option followed by the project number:\n\nsrun --account=PAS2250 --pty /bin/bash\n\n\nsrun: job 12431932 queued and waiting for resources\nsrun: job 12431932 has been allocated resources\n[…regular login info, such as quota, not shown…]\n[jelmer@p0133 PAS2250]$\n\nThere we go! First we got some Slurm scheduling info:\n\nInitially, the job is “queued”: that is, waiting to start.\nVery soon (usually!), the job has been “allocated resources”: that is, computing resources such as a compute node were found and reserved for the job.\n\nThen:\n\nThe job starts and because we’ve reserved an interactive shell job, this means that a new Bash shell is initiated: for that reason, we get to see our regular login info once again.\nMost importantly, we are no longer on a login node but on a compute node, as our prompt hints at: we switched from something like [jelmer@pitzer-login04 PAS2250]$ to the [jelmer@p0133 PAS2250]$ shown above.\nNote also that the job has a number (above: job 12431932): every compute job has such a unique identifier among all jobs by all users at OSC, and we can use this number to monitor and manage it. All of us will therefore see a different job number pop up.\n\n\n\n\n\n\n\nOSC projects\n\n\n\nDuring this workshop, we can all use the project PAS2250, which is actually a project that OSC has freely given me to introduce people to working at OSC. The project will still be charged but the credits on it were freely awarded.\nTo work on your own research project at OSC, you will either have to get your own project (typically, PIs get one for their lab or for a specific research project) or you can become an MCIC member and use the MCIC project.\n\n\n\n\n2.2 Compute job options\nThe --account= option is just one of out of many options we can use when reserving a compute job, but is the only one that always has to be specified (including for batch jobs and for Interactive Apps).\nDefaults exist for all other options, such as the amount of time (1 hour) and the number of cores (1). These options are all specified in the same way for interactive and batch jobs, and we’ll dive into them below.\n\n\n\n\n\n\nTip\n\n\n\nMany SLURM options have a long format (--account=PAS2250) and a short format (-A PAS2250), which can generally be used interchangeably. For clarity, we’ll try to stick to long format options during this workshop."
  },
  {
    "objectID": "sessions/08-slurm.html#intro-to-batch-jobs",
    "href": "sessions/08-slurm.html#intro-to-batch-jobs",
    "title": "Compute Jobs with Slurm",
    "section": "3 Intro to batch jobs",
    "text": "3 Intro to batch jobs\nWhen requesting batch jobs, we are asking the Slurm scheduler to run a script on a compute node. For this reason, we can also refer to it as “submitting a script (to the queue)”.\nIn contrast to interactive shell jobs, we stay in our current shell on a login node when submitting a script, and cannot really interact with the process on the compute node, other than:\n\nOutput from the script that would normally be printed to screen ends up in a file.\nWe can do things like monitoring whether the job is still running and cancelling the job, which will revoke the compute node reservation and stop the ongoing process.\n\n\n\n\n\n\n\nTip\n\n\n\nThe script that we submit can be in different languages but typically, including in all examples in this workshop, they are shell (Bash) scripts.\n\n\n\n3.1 The sbatch command\nWhereas we used Slurm’s srun command to start an interactive shell job, we use its sbatch command to submit a script.\nRecall from the Bash scripting module that we can run a Bash script as follows:\n\nbash printname.sh Jane Doe\n\nFirst name: Jane\nLast name: Doe\n\n\n\n\n\n\n\n\nCan’t find the printname.sh script?\n\n\n\n\n\n\nOpen a new file in the VS Code editor (     =>   File   =>   New File) and save it as printname.sh\nCopy the code below into the script:\n\n\n#!/bin/bash\nset -ueo pipefail\n\nfirst_name=$1\nlast_name=$2\n  \necho \"First name: $first_name\"\necho \"Last name: $last_name\"\n\n\n\n\nThe above command ran the script on our current node, a login node. To instead submit the script to the Slurm queue, we would start by simply replacing bash by sbatch:\n\nsbatch printname.sh Jane Doe\n\n\nsrun: error: ERROR: Job invalid: Must specify account for job\nsrun: error: Unable to allocate resources: Unspecified error\n\nAs we’ve learned, we always have to specify the OSC account when submitting a compute job. Conveniently, we can also specify Slurm/sbatch options inside our script, but first, let’s add the --account option on the command line:\n\nsbatch --account=PAS2250 printname.sh Jane Doe\n\n\nSubmitted batch job 12431935\n\n\n\n\n\n\n\nsbatch options vs. script arguments\n\n\n\n\nsbatch [sbatch-options] myscript.sh [script-arguments]\n\n\nsbatch printname.sh                             # No options/arguments for either\nsbatch printname.sh Jane Doe                    # Script arguments but no sbatch option\nsbatch --account=PAS2250 printname.sh           # sbatch option but no script arguments\nsbatch --account=PAS2250 printname.sh Jane Doe  # Both sbatch option and script arguments\n\n\n\n\n\n3.2 Adding sbatch options in scripts\nInstead of specifying Slurm/sbatch options on the command-line when we submit the script, we can also add these options inside the script.\nThis is handy because even though we have so far only seen the account= option, you often want to specify several options. That would lead to very long sbatch commands. Additionally, it can be practical to store a script’s typical Slurm options along with the script itself.\nWe add the options in the script using another type of special comment line akin to the shebang line, marked by #SBATCH. The equivalent of adding --account=PAS2250 after sbatch on the command line, is a line in a script that reads #SBATCH --account=PAS2250.\nJust like the shebang line, the #SBATCH line(s) should be at the top of the script. Let’s add one to the printname.sh script, such that the first few lines read:\n\n#!/bin/bash\n#SBATCH --account=PAS2250\n\nset -ueo pipefail\n\nAfter having added this to the script, we can successfully run our earlier sbatch command without options:\n\nsbatch printname.sh Jane Doe\n\n\nSubmitted batch job 12431942\n\n\n\n\n\n\n\nRunning a script with #SBATCH in other contexts\n\n\n\nBecause #SBATCH lines are special comment lines, they will simply be ignored and not throw any errors when you run a script that contains them in other contexts: when not running them as a batch job at OSC, or even when running them on a computer without Slurm installed.\n\n\n\n\n\n\n\n\nsbatch option precedence\n\n\n\nAny sbatch option provided on the command line will override the equivalent option provided inside the script. This is sensible: we can provide “defaults” inside the script, and change one or more of those when needed on the command line.\n\n\n\n\n3.3 Where does the output go?\nAbove, we saw that when we ran the printname.sh script directly, its output was printed to the screen, whereas when we submitted it as a batch job, we merely got Submitted batch job 12431942 printed to screen. So where did our output go?\nIt ended up in a file slurm-12431942.out (i.e., slurm-<job-number>.out), which we might call a Slurm log file.\n\n\n\n\n\n\nAny idea why we might not want batch job output printed to screen, even if we could?\n\n\n\n\n\nThe power of submitting batch jobs is that you can submit many at once, e.g. one per sample, running the same script. If the output from all those scripts ends up on your screen, things become a big mess, and you have no lasting record of what happened.\n\n\n\nIt’s important to conceptually distinguish two broad types of output that a script may have:\n\nOutput that is printed to screen when we directly run a script, such as what was produced by our echo statements, by any errors that may occur, and possibly by a program that we run in the script.2 As we saw, this output ends up in the Slurm log file when we submit the script as a batch job.\nOutput that we redirect to a file (> myfile.txt) or that a program that we run in the script writes to file(s). This type of output will always end up in those very same files regardless of whether we run the script directly or as a batch job.\n\n\n\n\n\n\n\nTip\n\n\n\nBoth interactive and batch jobs start in the directory that they were submitted from: that is, your working directory will remain the same."
  },
  {
    "objectID": "sessions/08-slurm.html#other-common-sbatch-options",
    "href": "sessions/08-slurm.html#other-common-sbatch-options",
    "title": "Compute Jobs with Slurm",
    "section": "4 Other common sbatch options",
    "text": "4 Other common sbatch options\n\n4.1 --time: Time limit (“wall time”)\nSpecify the maximum amount of time your job will run for. Wall time is a term meant to distinguish it from, say “core hours”: if a job runs for 2 hour and used 8 cores, the wall time was 2 hours and the number of core hours was 2 x 8 = 16.\n\nYour job gets killed as soon as it hits the specified time limit!\nYou will only be charged for the time your job actually used.\nIn general, shorter jobs are likely to start running sooner\nThe default is 1 hour. Acceptable time formats include:\n\nminutes\nhours:minutes:seconds\ndays-hours\n\nFor single-node jobs, up to 168 hours (7 days) can be requested. If that’s not enough, you can request access to the longserial queue for jobs of up to 336 hours (14 days).\n\n\n#!/bin/bash\n#SBATCH --time=1:00:00\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are uncertain about the time your job will take, ask for (much) more time than you think you will need.\n\n\n\n\n4.2 --mem: RAM memory\nSpecify a maximum amount of RAM (Random Access Memory) that your job can use.\n\nThe default unit is MB (MegaBytes) – use “G” for GB.\nLike with the time limit, your job gets killed when it hits the memory limit. But this is not that common – as the OSC documentation mentions:\n\n\nThere is no need to specify a memory limit unless your memory requirements are disproportionate to the number of cores you are requesting or you need a large-memory node.\n\n#!/bin/bash\n#SBATCH --mem=20G\n\n\n4.3 Nodes, cores, and tasks\nSpecify the number of nodes (≈ computers), cores, or “tasks” (processes). These are separate but related options, and this is where things can get confusing!\n\nSlurm for the most part uses “core” and “CPU” interchangeably (even though technically, one CPU often contains multiple cores). In this context, “thread” is also commonly used interchangeably with core/CPU (even though technically, one core often contains multiple threads).\nRunning a program that uses multiple threads/cores/CPUs (“multi-threading”) is common.\nIn such cases, specify the number of threads/cores/CPUs n with --cpus-per-task=n (and keep --nodes and ntasks at their defaults of 1).\nThe program you’re running may have an argument like --cores or --threads, which you should then set to n as well.\n\n\n\n\n\n\n\nUncommon cases\n\n\n\n\nOnly ask for >1 node when you have explicit parallelization with something like “MPI”, which is uncommon in bioinformatics.\nFor jobs with multiple processes (tasks), use --ntasks=n or --ntasks-per-node=n.\n\n\n\n\n\n\n\n\n\n\n\n\nResource/use\nshort\nlong\ndefault\n\n\n\n\nNr. of nodes\n-N 1\n--nodes=1\n1\n\n\nNr. of cores/CPUs/threads (per task)\n-c 1\n--cpus-per-task=1\n1\n\n\nNr. of “tasks” (processes)\n-n 1\n--ntasks=1\n1\n\n\nNr. of tasks per node\n-\n--ntasks-per-node=1\n1"
  },
  {
    "objectID": "sessions/06-scripts.html",
    "href": "sessions/06-scripts.html",
    "title": "Shell Scripting",
    "section": "",
    "text": "Shell scripts (or to be slightly more species, Bash scripts) enable us to run sets of commands non-interactively. This is especially beneficial or necessary when a set of commands:\nScripts form the basis for analysis pipelines and if we code things cleverly, it should be straightforward to rerun much of our project workflow:"
  },
  {
    "objectID": "sessions/06-scripts.html#setup",
    "href": "sessions/06-scripts.html#setup",
    "title": "Shell Scripting",
    "section": "1 Setup",
    "text": "1 Setup\n\n\n\n\n\n\nStarting a VS Code session with an active terminal (click here)\n\n\n\n\n\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 3 in the box Number of hours\nEnter /fs/ess/scratch/PAS2250 in the box Working Directory\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\nOpen a terminal by clicking the “hamburger menu” way in the top left, and then Terminal > New Terminal.\nIn the terminal, type Bash and press Enter."
  },
  {
    "objectID": "sessions/06-scripts.html#script-header-lines-and-zombie-scripts",
    "href": "sessions/06-scripts.html#script-header-lines-and-zombie-scripts",
    "title": "Shell Scripting",
    "section": "2 Script header lines and zombie scripts",
    "text": "2 Script header lines and zombie scripts\n\n2.1 Shebang line\nUse a so-called “shebang” line as the first line of a script to indicate which language your script use. More specifically, this line tell the computer where to find the binary (executable) that will run your script.\nSuch a line starts with #!, basically marking it as a special type of comment. After that, we provide the location to the relevant program: in our case Bash, which is always located at /bin/bash on Linux and Mac computers.\n#!/bin/bash\nAdding a shebang line is good practice and is necessary when we want to submit our script to OSC’s SLURM queue, which we’ll do tomorrow.\nAnother line that is good practice to add to your Bash scripts changes some default settings to safer alternatives.\n\n\n2.2 Bash script settings\nTwo Bash default settings are bad ideas inside scripts.\nFirst, and as we’ve seen in the previous module, Bash does not complain when you reference a variable that does not exist (in other words, it does not consider that an error).\nIn scripts, this can lead to all sorts of downstream problems, because you probably tried to do something with an existing variable but made a typo. Even more problematically, it can lead to potentially very destructive file removal:\n\n# Using a variable, we try to remove some temporary files whose names start with tmp_\ntemp_prefix=\"temp_\"\nrm \"$tmp_prefix\"*     # DON'T TRY THIS!\n\n\n# Using a variable, we try to remove a temporary directory\ntempdir=output/tmp\nrm -rf $tmpdir/*      # DON'T TRY THIS!\n\n\n\n\n\n\n\nThe comments above specified the intent we had. What would have actually happened?\n\n\n\n\n\nIn the first example, rm \"$tmp_prefix\"* would have been interpreted as rm *, because the non-existent variable is simply ignored. Therefore, we would have removed all files in the current working directory.\nAlong similar lines, in the second example, rm -rf $tmpdir/* would have been interpreted as rm -rf /*. Horrifyingly, this would attempt to remove the entire filesystem (recall that a leading / in a path is a computer’s root directory).1\n\n\n\n\nSecond, a Bash script keeps running after encountering errors. That is, if an error is encountered when running line 2 of a script, any remaining lines in the script will nevertheless be executed.\nIn the best case, this is a waste of computer resources, and in worse cases, it can lead to all kinds of unintended consequences. Additionally, if your script prints a lot of output, and much of it appears to be okay, you might not notice an error somewhere in the middle; but this error which might still have led to completely wrong results downstream.\n\nThe following three settings will make your Bash scripts more robust and safer. With these settings, the script terminates, with an appropriate error message, if:\n\nset -u — An unset variable is referenced.\nset -e — Almost any error occurs.\nset -o pipefail — An error occurs in a shell “pipeline” (e.g., sort | uniq).\n\nWe can change all of these settings in one line in a script:\n\nset -u -e -o pipefail     # (Don't run in the terminal)\n\nOr even more concisely:\n\nset -ueo pipefail         # (Don't run in the terminal)\n\n\n\n2.3 Our header lines as a rudimentary script\nLet’s go ahead and start a script with the header lines that we have so far discussed.\n\nOpen a new file in the VS Code editor (     =>   File   =>   New File) and save it as printname.sh (shell scripts, including Bash scripts, most commonly have the extension .sh).\nType the following lines in that script (please actually type instead of copy-pasting):\n\n\n#!/bin/bash\nset -ueo pipefail\n\n# (Note: this is a partial script. Don't enter this directly in your terminal.)\n\nAlready now, we could run (execute) the script. One way of doing this is calling the bash command followed by the name of the script2:\n\nbash printname.sh\n\nDoing this won’t print anything to screen (or file). This makes sense because our script doesn’t have any output, and as we’ve seen before with Bash, no output can be a good sign because it means that no errors were encountered."
  },
  {
    "objectID": "sessions/06-scripts.html#command-line-arguments-for-scripts",
    "href": "sessions/06-scripts.html#command-line-arguments-for-scripts",
    "title": "Shell Scripting",
    "section": "3 Command-line arguments for scripts",
    "text": "3 Command-line arguments for scripts\n\n3.1 Calling a script with arguments\nWhen you call a script, you can pass it command-line arguments, such as a file to operate on.\nThis is much like when you provide commands like ls with arguments:\n\n# Run ls without arguments:\nls\n\n# Pass 1 filename as an argument to ls:\nls data/sampleA.fastq.gz\n\n# Pass 2 filenames as arguments to ls, separated by spaces:\nls data/sampleA.fastq.gz data/sampleB.fastq.gz\n\nLet’s see the same thing with our printname.sh script and a fictional script fastqc.sh (which would probably run the FastQC program – we’ll make such a script later):\n\n# Run scripts without any arguments:\nbash fastqc.sh                          # (Fictional script)\nbash printname.sh\n\n# Run scripts with 1 or 2 arguments:\nbash fastqc.sh data/sampleA.fastq.gz    # 1 argument, a filename\nbash printname.sh John Doe              # 2 arguments, strings representing names\n\nIn the next section, we’ll see what happens when we pass arguments on the command line (in short: command-line arguments) to a script.\n\n\n3.2 Placeholder variables\nInside the script, any command-line arguments are automatically assigned to placeholder variables.\nA first argument will be assigned to the variable $1, any second argument will be assigned to $2, any third argument will be assigned to $3, and so on.\n\n\n\n\n\n\nIn the calls to fastqc.sh and printname.sh above, which placeholder variables were created and what were there values?\n\n\n\n\n\nIn bash fastqc.sh data/sampleA.fastq.gz, a single argument, data/sampleA.fastq.gz, is passed to the script, and will be assigned to $1.\nIn bash printname.sh John Doe, two arguments are passed to the script: the first one (John) will be stored in $1, and the second one (Doe) in $2.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nArguments passed to a script are only assigned to placeholder varaibles; unless we explicitly include code in the script to do something with those variables, nothing else happens.\n\n\nLet’s add code to our script to “process” any first and last name that are passed to the script as command-line arguments. First, our small script will simply echo the placeholder variables, so that we can see what happens. We’ll add two echo commands to our printname.sh script, such that the script reads:\n\n#!/bin/bash\nset -ueo pipefail\n\necho \"First name: $1\"\necho \"Last name: $2\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\n\n\n\nNext, we’ll run the script, passing the arguments John and Doe:\n\nbash printname.sh John Doe\n\nFirst name: John\nLast name: Doe\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nIn each case below, think about what might happen before you run the script. If you didn’t make a successful predictions, try to figure out what happened instead.\n\nRun the script without passing arguments to it.\nDeactivate (“comment out”) the line with set settings by inserting a # as the first character. Then, run the script again without passing arguments to it.\nDouble-quote John Doe when you run the script, i.e. run bash printname.sh \"John Doe\"\nRemove the # you inserted in the script in step 2 above.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nThe script will error out because we are referencing variables that don’t exist: since we didn’t pass command-line arguments to the script, the $1 and $2 have not been set.\n\n\nbash printname.sh\n\n\nprintname.sh: line 4: $1: unbound variable\n\n\nThe script will run in its entirety and not throw any errors, because we are now using default Bash settings such that referencing non-existent variables does not throw an error. Of course, no names are printed either, since we didn’t specify any:\n\n\n\n\n\nbash printname.sh\n\nFirst name: \nLast name: \n\n\nThe set line should read:\n\n#set -ueo pipefail\n\n\nBecause we are quoting \"John Doe\", both names are passed as a single argument and both names end up in $1, the “first name”:\n\n\nbash printname.sh \"John Doe\"\n\nFirst name: John Doe\nLast name: \n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3 Descriptive variable names\nWhile you can use the $1-style placeholder variables throughout your script, I find it very useful to copy them to more descriptively named variables as follows:\n\n#!/bin/bash\nset -ueo pipefail\n\nfirst_name=$1\nlast_name=$2\n  \necho \"First name: $first_name\"\necho \"Last name: $last_name\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\nUsing descriptively named variables in your scripts has several advantages. It will make your script easier to understand for others and for yourself. It will also make it less likely that you make errors in your script in which you use the wrong variable in the wrong place.\n\n\n\n\n\n\nOther variables that are automatically available inside scripts\n\n\n\n\n$0 contains the script’s file name\n$# contains the number of command-line arguments passed\n\n\n\n\n\n\n\n\n\nExercise: a script to print a specific line\n\n\n\n\n\nWrite a script that prints a specific line (identified by line number) from a file.\n\nSave the script as line.sh\nStart with the shebang and set lines\nYour script takes two arguments: a file name ($1) and a line number ($2)\nCopy the $1 and $2 variables to descriptively named variables\nTo print a specific line, think how you might combine head and tail to do this. If you’re at a loss, feel free to check out the solution below.\nTest the script by printing line 4 from samples.txt.\n\n\n\n\n\n\n\nSolution: how to print a specific line number\n\n\n\n\n\nFor example, to print line 37 of samples.txt directly:\n\nhead -n 37 samples.txt | tail -n 1\n\nIn the script, you’ll have to use variables instead of 37 and samples.txt.\nHow this command works:\n\nhead -n 37 samples.txt will print the first 37 lines of samples.txt\nWe pipe those 37 lines into the tail command\nWe ask tail to just print the last line of its input, which will in this case be line 37 of the original input file.\n\n\n\n\n\n\n\n\n\n\nFull solution\n\n\n\n\n\n\n#!/bin/bash\nset -ueo pipefail\n  \ninput_file=$1\nline_nr=$2\n\nhead -n \"$line_nr\" \"$input_file\" | tail -n 1\n\nTo run the script and make it print the 4th line of samples.txt:\n\nbash line.sh samples.txt 4"
  },
  {
    "objectID": "sessions/06-scripts.html#script-variations-and-improvements",
    "href": "sessions/06-scripts.html#script-variations-and-improvements",
    "title": "Shell Scripting",
    "section": "4 Script variations and improvements",
    "text": "4 Script variations and improvements\n\n4.1 A script to serve as a starting point\nWe’ve learned that the head command prints the first lines of a file, whereas the tail command prints the last lines. Sometimes it’s nice to be able to quickly see both ends of a file, so let’s write a little script that can do that for us, as a starting point for the next few modifications.\nOpen a new file, save it as headtail.sh, and add the following code to it:\n\n#!/bin/bash\nset -ueo pipefail\n\ninput_file=$1\n\nhead -n 2 \"$input_file\"\necho \"---\"\ntail -n 2 \"$input_file\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\nNext, let’s run our headtail.sh script:\n\nbash headtail.sh samples.txt\n\n\n\n4.2 Redirecting output to a file\nSo far, the output of our scripts was printed to screen, e.g.:\n\nIn printnames.sh, we simply echo’d, inside sentences, the arguments passed to the script.\nIn headtail.sh, we printed the first and last few lines of a file.\n\nAll this output was printed to screen because that is the default output mode of Unix commands, and this works the same way regardless of whether those commands are run directly on the command line, or are run inside a script.\nAlong those same lines, we have already learned that we can “redirect” output to a file using > (write/overwrite) and >> (append) when we run shell commands — and this, too, works exactly the same way inside a script.\n\nWhen working with genomics data, we commonly have files as input, and new/modified files as output. So let’s practice with this and modify our headtail.sh script so that it writes output to a file.\nWe’ll make the following changes:\n\nWe will have the script accept a second argument: the output file name. Of course, we could also simply write the output to a predefined (“hardcoded”) file name such as out.txt, but in general, it’s better practice to keep this flexible via an argument.\nWe will redirect the output of our head, echo, and tail commands to the output file. We’ll have to append (>>) in the last two cases.\n\n\n#!/bin/bash\nset -ueo pipefail\n\ninput_file=$1\noutput_file=$2\n\nhead -n 2 \"$input_file\" > \"$output_file\"\necho \"---\" >> \"$output_file\"\ntail -n 2 \"$input_file\" >> \"$output_file\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\nNow we run the script again, this time also passing the name of an output file:\n\nbash headtail.sh samples.txt samples_headtail.txt\n\nThe script will no longer print any output to screen, and our output should instead be in samples_headtail.txt:\n\n# Check that the file exists and was just modified:\nls -lh samples_headtail.txt\n\n# Print the contents of the file to screen\ncat samples_headtail.txt\n\n\n\n4.3 Report what’s happening\nIt is often useful to have your scripts “report” or “log” what is going on. Let’s keep thinking about a script that has file(s) as the main output, but instead of having no output printed to screen at all, we’ll print some logging output to screen. For instance: what is the date and time, which arguments were passed to the script, what are the output files, and perhaps even summaries of the output. All of this can help with troubleshooting.3\nLet’s try this with our headtail.sh script.\n\n#!/bin/bash\nset -ueo pipefail\n\n## Process command-line arguments\ninput_file=$1\noutput_file=$2\n\n## Initial logging \necho \"Starting script $0\"           # Print name of script\ndate                                # Print date & time\necho \"Input file:   $input_file\"\necho \"Output file:  $output_file\" \necho                                # Print empty line to separate initial & final logging\n\n## Print the first and last two lines to a separate file\nhead -n 2 \"$input_file\" > \"$output_file\"\necho \"---\" >> \"$output_file\"\ntail -n 2 \"$input_file\" >> \"$output_file\"\n\n## Final logging\necho \"Listing the output file:\"\nls -lh \"$output_file\"\necho \"Done with script $0\"\ndate\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\nA couple of notes about the lines that were added to the script above:\n\nPrinting the date at the end of the script as well will allow you to check for how long the script ran, which can be informative for longer-running scripts.\nPrinting the input and output files (and the command-line arguments more generally) can be particularly useful for troubleshooting\nPrinting a “marker line” like Done with script, indicating that the end of the script was reached, is handy because due to our set settings, seeing this line printed means that no errors were encountered.\nBecause our script grew so much, I also added some comment headers like “Initial logging” to make the script easier to read, and such comments can be made more extensive to really explain what is being done.\n\n\n\n\nLet’s run the script again:\n\nbash headtail.sh printname.sh tmp.txt\n\nStarting script headtail.sh\nMon Aug 15 04:32:43 PM CEST 2022\nInput file:   printname.sh\nOutput file:  tmp.txt\n\nListing the output file:\n-rw-rw-r-- 1 jelmer jelmer 77 Aug 15 16:32 tmp.txt\nDone with script headtail.sh\nMon Aug 15 04:32:44 PM CEST 2022\n\n\nThe script printed some details for the output file, but not its contents (that would have worked here, but is usually not sensible when working with genomics data). Let’s take a look, though, to make sure the script worked:\n\ncat tmp.txt\n\n#!/bin/bash\nset -ueo pipefail\n---\necho \"First name: $1\"\necho \"Last name: $2\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe reporting (echo-ing) may have started to seem silly for our litle script, but fairly extensive reporting (as well as testing, which is outside the scope of this workshop) can be very useful — and will be eventually a time-saver.\nThis is especially true for long-running scripts, or scripts that you often reuse and perhaps share with others.\n\n\n \n\n\n\n\n\n\n\nKeyboard shortcut to run shell commands from the editor\n\n\n\nTo add a keyboard shortcut that will send code selected in the editor pane to the terminal (such that you don’t have to copy and paste):\n\nClick the      (bottom-left) => Keyboard Shortcuts.\nFind Terminal: Run Selected Text in Active Terminal, click on it, then add a shortcut, e.g. Ctrl+Enter."
  },
  {
    "objectID": "sessions/09-examples.html#tba",
    "href": "sessions/09-examples.html#tba",
    "title": "Example Compute Jobs",
    "section": "1 TBA",
    "text": "1 TBA"
  },
  {
    "objectID": "sessions/02-osc.html#tba",
    "href": "sessions/02-osc.html#tba",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "1 TBA",
    "text": "1 TBA"
  },
  {
    "objectID": "sessions/04-shell.html#tba",
    "href": "sessions/04-shell.html#tba",
    "title": "The Unix Shell",
    "section": "1 TBA",
    "text": "1 TBA"
  },
  {
    "objectID": "sessions/07-software.html",
    "href": "sessions/07-software.html",
    "title": "Using Software at OSC",
    "section": "",
    "text": "So far, we have only used commands that are available in any Unix shell. But to actually analyze genomics data sets, we also need to use specialized bioinformatics software.\nMost software that is already installed at OSC must nevertheless be “loaded” (“activated”) before we can use it; and if our software of choice is not installed, we have to do so ourselves. We will cover those topics in this module."
  },
  {
    "objectID": "sessions/07-software.html#setup",
    "href": "sessions/07-software.html#setup",
    "title": "Using Software at OSC",
    "section": "1 Setup",
    "text": "1 Setup\n\n\n\n\n\n\nStarting a VS Code session with an active terminal (click here)\n\n\n\n\n\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 3 in the box Number of hours\nEnter /fs/ess/scratch/PAS2250 in the box Working Directory\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\nOpen a terminal by clicking the “hamburger menu” way in the top left, and then Terminal > New Terminal.\nIn the terminal, type Bash and press Enter."
  },
  {
    "objectID": "sessions/07-software.html#running-command-line-programs",
    "href": "sessions/07-software.html#running-command-line-programs",
    "title": "Using Software at OSC",
    "section": "2 Running command-line programs",
    "text": "2 Running command-line programs\nAs pointed out in the introduction to the workshop, bioinformatics software (programs) that we use to analyze genomic data are typically run from the command line. That is, they have “command-line interfaces” (CLIs) rather than “graphical user interfaces” (GUIs), and are run using commands that are structurally very similar to how we’ve been using basic Unix commands.\nFor instance, we can run the program FastQC as follows, instructing it to process the FASTQ file sampleA.fastq.gz with default options:\n\nfastqc sampleA.fastq.gz\n\nSo, what we have learned in the previous modules can easily be applied to run command-line programs. But, we first need to load and/or install these programs.\n\n\n\n\n\n\nRunning inside a script or interactively\n\n\n\nLike any other command, we could in principle run the line of code above either in our interactive shell or from inside a script. In practice, it is better to do this in a script, especially at OSC, because:\n\nSuch programs typically take a while to run\nWe are not supposed to run processes that use significant resources on login nodes\nWe can run the same script simultaneously for different input files."
  },
  {
    "objectID": "sessions/07-software.html#software-at-osc-with-lmod",
    "href": "sessions/07-software.html#software-at-osc-with-lmod",
    "title": "Using Software at OSC",
    "section": "3 Software at OSC with Lmod",
    "text": "3 Software at OSC with Lmod\nOSC administrators manage software with the Lmod system of software modules. For us users, this means that even though a lot of software is installed, most of it can only be used after we explicitly load it.\n(That may seem like a drag, but on the upside, this practice enables the use of different versions of the same software, and of mutually incompatible software on a single system.)\n\n3.1 Checking for available software\nThe OSC website has a list of software that has been installed at OSC. You can also search for available software in the shell:\n\nmodule spider lists modules that are installed.\nmodule avail lists modules that can be directly loaded, given the current environment (i.e., depending on which other software has been loaded).\n\nSimply running module spider or module avail would spit out complete lists — more usefully, we can provide search terms as arguments to these commands:\n\nmodule spider python\n\n\n\n\n\npython:\n\n\n\n Versions:\n    python/2.7-conda5.2\n    python/3.6-conda5.2\n    python/3.7-2019.10\n\n\nmodule avail python\n\n\npython/2.7-conda5.2         python/3.6-conda5.2 (D)         python/3.7-2019.10\n\n\n\n\n\n\n\nTip\n\n\n\nThe (D) in the output above marks the default version of the program; that is, the version of the program that would be loaded if we don’t specify a version (see examples below).\n\n\n\n\n3.2 Loading software\nAll other Lmod software functionality is also accessed using module “subcommands” (we call module the command and e.g. spider the subcommand). For instance, to load and unload software:\n\n# Load a module:\nmodule load python              # Load the default version\nmodule load python/3.7-2019.10  # Load a specific version\n\n# Unload a module:\nmodule unload python\n\nTo check which modules have been loaded (the list will include modules that have been loaded automatically):\n\nmodule list\n\n\nCurrently Loaded Modules:\n    1) xalt/latest       2) gcc-compatibility/8.4.0       3) intel/19.0.5       4) mvapich2/2.3.3       5) modules/sp2020\n\n\n\n3.3 A practical example\nLet’s load a very commonly used bioinformatics program that we will also use in examples later on: FastQC. FastQC performs quality control (hence: “QC”) on FASTQ files.\nFirst, let’s test that we indeed cannot currently use fastqc by running fastqc with the --help flag:\n\nfastqc --help\n\n\nbash: fastqc: command not found\n\n\n\n\n\n\n\nHelp!\n\n\n\nA solid majority of command-line programs can be run with with a --help (and/or -h) flag, and this is perfect to try first, since it will tell use whether we can use the program, and if we can, we immediately get some usage information.\n\n\nNext, let’s check whether it is available at OSC, and if so, in which versions:\n\nmodule avail fastqc\n\n\nfastqc/0.11.8\n\nThere is only one version available (0.11.8), which means that module load fastqc and module load fastqc/0.11.8 would each load that same version.\n\n\n\n\n\n\nWhat might still be an argument to specify the version when we load the software?\n\n\n\n\n\nWhen we use it inside a script:\n\nThis would ensure that when we run the same script a year later, the same version would be used (assuming it hasn’t been removed) — otherwise, it’s possible a newer version would has been installed in the meantime, which might produce different results.\nIt will make it easy to see which version we used, which is something we typically report in papers.\n\n\n\n\nLet’s load the FastQC module:\n\nmodule load fastqc/0.11.8\n\nAfter we have loaded the module, we can retry our --help attempt:\n\nfastqc --help\n\n\n        FastQC - A high throughput sequence QC analysis tool\nSYNOPSIS\n    fastqc seqfile1 seqfile2 .. seqfileN\n\nfastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] \n       [-c contaminant file] seqfile1 .. seqfileN\n       \n[…and much more]"
  },
  {
    "objectID": "sessions/07-software.html#when-software-isnt-installed-at-osc",
    "href": "sessions/07-software.html#when-software-isnt-installed-at-osc",
    "title": "Using Software at OSC",
    "section": "4 When software isn’t installed at OSC",
    "text": "4 When software isn’t installed at OSC\nIt’s not too uncommon that software you need for your project is not installed at OSC, or that you need a more recent version of the software than those available. The main options available to you in such a case are to:\n\n“Manually” install the software, which in the best case involves downloading a directly functioning binary (executable), but more commonly requires you to “compile” (build) the program. This is sometimes straightforward but can also become extremely tricky, especially at OSC where you don’t have “administrator privileges”1 at OSC and will often have difficulties with “dependencies”2.\nSend an email to OSC Help. They might be able to help you with your installation, or in case of commonly used software, might be willing to perform a system-wide installation (that is, making it available through module).\nUse conda, which creates software environments that are activated like in the module system.\nUse Apptainer / Singularity “containers”. Containers are software environments that are more self-contained, akin to mini virtual machines.\n\nConda and containers are useful not only at OSC, where they bypass issues with dependencies and administrator privileges, but more generally, for reproducible and portable software environments. They also allow you to easily maintain distinct “environments”, each with a different version of the same software, or with mutually incompatible software.\nWe will teach conda here because it is easier to learn and use than containers, and because nearly all open-source bioinformatics software is available as a conda package.\n\n\n\n\n\n\nWhen to use containers instead of Conda\n\n\n\n\nIf you need to use software that requires a different Operating System (OS) or OS version than the one at OSC.\nIf you want or require even greater reproducibility and portability to create an isolated environment that can be exported and used anywhere."
  },
  {
    "objectID": "sessions/07-software.html#using-conda",
    "href": "sessions/07-software.html#using-conda",
    "title": "Using Software at OSC",
    "section": "5 Using conda",
    "text": "5 Using conda\nConda creates so-called environments in which you can install one or more software packages. As mentioned above, these environments are activated and deactivated in a similar manner as with the Lmod system – but the key difference is that we can create and manage these environments ourselves.\n\n\n\n\n\n\nWhat’s in an environment?\n\n\n\nOne environment per software, or one per project\nNote that even when you install a single program, many things are usually installed: dependencies\n\n\n\n5.1 Loading the (mini)conda module\nWhile it is also fairly straightforward to install conda for yourself 3, we will use OSC’s system-wide installation of conda in this workshop. Therefore, we first need to use a module load command to make it available:\n\n# (The most common installation of conda is actually called \"miniconda\")\nmodule load miniconda3\n\n\n\n5.2 One-time conda configuration\nWe will also do some one-time configuration, which will set the conda “channels” (basically, software repositories) that we want to use when we install software. This config also includes setting relative priorities among the channels, since one software package may be available from multiple channels.\nLike with module commands, conda commands consist of two parts, the conda command itself and a subcommand, such as config:\n\nconda config --add channels defaults     # Added first => lowest priority\nconda config --add channels bioconda\nconda config --add channels conda-forge  # Added last => highest priority\n\nLet’s check whether this configuration step worked:\n\nconda config --get channels\n\n\n\n5.3 Creating an environment for cutadapt\nTo practice using conda, we will now create a conda environment with the program cutadapt installed.\ncutadapt is a commonly used program to remove adapters or primers from sequence reads in FASTQ files; in particular, it is ubiquitous for primer removal in (e.g. 16S rRNA) microbiome metabarcoding studies. But there is no Lmod module on OSC for it, so if we want to use, our best option is to resort to conda.\nHere is the command to create a new environment and install cutadapt into that environment:\n\nconda create -y -n cutadapt -c bioconda cutadapt\n\nLet’s break the above command down:\n\ncreate is the conda subcommand to create a new environment.\n-y is a flag that prevents us from being asked to confirm installation.\nFollowing the -n option, we can specify the name of the environment, so -n cutadapt means that we want our environment to be called cutadapt. We can use whatever name we like for the environment, but of course a descriptive yet concise name is a good idea. Since we are making a single-program environment, it makes sense to simply name it after the program.\nFollowing the -c option, we can specify a channel from which we want to install, so -c bioconda indicates we want to use the bioconda channel. (Given that we’ve done some config above, this is not always necessary, but it can be good to be explicit.)\nThe cutadapt at the end of the line simply tells conda to install the package of that name. This is a “positional” argument to the command (note that there’s no option like -s before it): we put any software package(s) we want to install at the end of the command.\n\n\n\n\n\n\n\nSpecifying a version\n\n\n\nIf we want to be explicit about the version we want to install, we can add the version after = following the package name. We do that below, and we also include the version in the environment name:\n\nconda create -y -n cutadapt-4.1 -c bioconda cutadapt=4.1\n\n Let’s run the command above and see if we can install cutadapt\n\n\n\n\n5.4 Creating an environment for any program\nMinor variations on the conda create command above can be used to install almost any program for which is conda package is available.\nHowever, you may be wondering how we would know:\n\nWhether the program is available and what its conda package’s name is\nWhich versions are available\nWhich conda channel we should use\n\nADD…\n\n\n5.5 Activating conda environments\nWhereas we use the term “load” for Lmod modules, we use “activate” to the same effect for conda environments.\nOddly enough, the most foolproof way to activate a conda environment is to use source activate rather than the expected conda activate — for instance:\n\nsource activate cutadapt-4.1\n\n\n(cutadapt-4.1) [jelmer@pitzer-login03 PAS2250]$\n\n\n\n\n\n\n\nEnvironment indicator\n\n\n\nWhen we have an active conda environment, its name is conveniently displayed in our prompt, as depicted above.\n\n\nAfter we have activated the cutadapt environment, we should be able to actually use the program. To test this, we’ll again simply run it with a --help option:\n\ncutadapt --help\n\n\n\n\n5.6 Lines to add to your scripts\nWhile you’ll typically want to do installation interactively and only need to do to it once (see note below), you should always include the necessary code to load/activate your programs in your shell scripts.\nWhen your program is available as an Lmod module, this simply entails a module load call — e.g., for fastqc:\n\n#!/bin/bash\nset -ueo pipefail\n\nmodule load fastqc\n\nWhen your program is available in a conda environment, this entails a module load command to load conda itself, followed by a source activate command to load the relevant conda environment:\n\n#!/bin/bash\n\n## Load software\nmodule load miniconda3\nsource activate cutadapt-4.1\n\n## Strict/safe Bash settings \nset -ueo pipefail\n\n\n\n\n\n\n\nWarning\n\n\n\nWe’ve moved the set -ueo pipefail line below the source activate command, because the conda activation procedure may otherwise throw “unbound variable” errors.\n\n\n\n\n\n\n\n\nInstall once, load always\n\n\n\n\nProvided you don’t need to switch versions, you only need to install a program once. This is true also at OSC and also when using conda (your environments won’t disappear unless you delete them).\nIn every single “session” that you want to use a program via an Lmod module or conda environment, you …"
  },
  {
    "objectID": "sessions/07-software.html#addendum-a-few-other-useful-conda-commands",
    "href": "sessions/07-software.html#addendum-a-few-other-useful-conda-commands",
    "title": "Using Software at OSC",
    "section": "6 Addendum: a few other useful conda commands",
    "text": "6 Addendum: a few other useful conda commands\n\nDeactivate the currently active conda environment:\n\nconda deactivate   \n\nActivate one environment and then “stack” an additional environment (a regular conda activate command would switch environments):\n\nsource activate cutadapt         # Now, the env \"cutadapt\" is active\nconda activate --stack multiqc   # Now, both \"cutadapt\" and \"multiqc\" are active\n\nRemove an environment entirely:\n\nconda env remove -n cutadapt\n\nList all your conda environments:\n\nconda env list\n\nList all packages (programs) installed in an environment:\n\nconda list -n cutadapt"
  },
  {
    "objectID": "sessions/01-intro.html#what-you-will-and-wont-learn",
    "href": "sessions/01-intro.html#what-you-will-and-wont-learn",
    "title": "Introduction to the Workshop",
    "section": "What you will and won’t learn",
    "text": "What you will and won’t learn\nThe focus of the workshop is on building some general skills for analyzing genomics data.\nCommand-line programs are preferred for many of the steps to analyze genomic sequencing data. Because such datasets tend to contain a lot of data, it is also preferable to run your analyses not on a laptop or desktop, but at a compute cluster like the Ohio Supercomputer Center (OSC).\nThese realities mean that in genomics, you need the following set of skills that you may not have been thought during your biology education:\n\nHaving a basic understanding of a compute cluster (supercomputer)\n\nAnd being able to:\n\nUse the Unix shell (work in a terminal)\nWrite small shell scripts\nSubmit scripts to a “queue” and monitor and manage the resulting compute jobs\nActivate and probably install software in a Linux environment where you don’t have “admin rights”\n\nWe will teach the basics of these skills during this workshop!\nIt may be useful to point out that given this focus, we will not teach you much if anything about:\n\nDetails of genomic data file types\nDetails of specific (genomic) analyses\nMaking biological inferences from your data"
  },
  {
    "objectID": "sessions/01-intro.html#mechanics-of-a-hybrid-workshop",
    "href": "sessions/01-intro.html#mechanics-of-a-hybrid-workshop",
    "title": "Introduction to the Workshop",
    "section": "Mechanics of a hybrid workshop",
    "text": "Mechanics of a hybrid workshop\nWe have a slightly complicated set up with participants in-person in Wooster with an instructor (Jelmer now via Zoom), in-person in Columbus with an instructor, and directly via Zoom. Some notes:\n\nThis website has all the material that we will go through during each of the modules! See the links in the schedule as well as in the top bar menus to access it.\nIn-person participants don’t need to connect to the Zoom call, since Zoom will be broadcast on the large screen (but you can of course connect if you can better see the instructor’s screen that way).\nBecause we’re not all on Zoom, we’ll use this Google Doc to share links, inpromptu code that is not on the website, and non-urgent questions.\nWhenever you have a question, please feel free to interrupt and speak up, both in-person and on Zoom. Only if your question is not urgent and you don’t want to interrupt the flow, put it in the Google Doc or ask about it during a break."
  },
  {
    "objectID": "sessions/01-intro.html#personal-introductions",
    "href": "sessions/01-intro.html#personal-introductions",
    "title": "Introduction to the Workshop",
    "section": "Personal introductions",
    "text": "Personal introductions\n\nInstructors\n\nJelmer Poelstra, Molecular and Cellular Imaging Center (MCIC), Wooster\nMike Sovic, Center for Applied Plant Sciences (CAPS), Wooster\n\n\n\nYou!\nPlease very briefly introduce yourself – include your position, department, and why you wanted to go to this workshop.\n\nAdd figure(s) showing data types & previous experience"
  },
  {
    "objectID": "sessions/05-vars-loops.html",
    "href": "sessions/05-vars-loops.html",
    "title": "Variables and Loops",
    "section": "",
    "text": "In this module, we will cover a couple of topics that are good to know about before you start writing and running shell scripts:"
  },
  {
    "objectID": "sessions/05-vars-loops.html#setup",
    "href": "sessions/05-vars-loops.html#setup",
    "title": "Variables and Loops",
    "section": "1 Setup",
    "text": "1 Setup\n\n\n\n\n\n\nStarting a VS Code session with an active terminal (click here)\n\n\n\n\n\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 3 in the box Number of hours\nEnter /fs/ess/scratch/PAS2250 in the box Working Directory\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\nOpen a terminal by clicking the “hamburger menu” way in the top left, and then Terminal > New Terminal.\nIn the terminal, type Bash and press Enter."
  },
  {
    "objectID": "sessions/05-vars-loops.html#variables",
    "href": "sessions/05-vars-loops.html#variables",
    "title": "Variables and Loops",
    "section": "2 Variables",
    "text": "2 Variables\nIn programming, we use variables for things that:\n\nWe refer to repeatedly and/or\nAre subject to change.\n\nThese tend to be settings like the paths to input and output files, and parameter values for programs.\n\n2.1 Assigning and referencing variables\nTo assign a value to a variable in Bash (in short: to assign a variable), use the syntax variable=value. For example:\n\n# Assign the value \"low\" to the variable \"treatment\":\ntreatment=low\n\n# Assign the value \"200\" to the variable \"nlines\":\nnlines=200\n\n\n\n\n\n\n\nSpace-sensitive\n\n\n\nBe aware that there can be no spaces around the equals sign (=)!\n\n\nTo reference a variable (i.e., to access its value), you need to put a dollar sign $ in front of its name. We’ll use the echo command to review the values that our variables contain:\n\necho $treatment\n\n\n\nlow\n\n\n\necho $nlines\n\n\n\n200\n\n\nConveniently, we can directly use variables in lots of contexts, as if we had instead typed their values:\n\nls_options=\"-lh\"\n\nls $ls_options\n\ntotal 376K\n-rw-rw-r-- 1 jelmer jelmer  20K Aug 15 16:33 01-intro.html\n-rw-rw-r-- 1 jelmer jelmer 2.9K Aug 11 10:01 01-intro.qmd\n-rw-rw-r-- 1 jelmer jelmer  16K Aug 15 16:32 02-osc.html\n-rw-rw-r-- 1 jelmer jelmer  248 Aug  9 21:42 02-osc.qmd\n-rw-rw-r-- 1 jelmer jelmer  11K Aug 11 10:43 03-vscode.qmd\n-rw-rw-r-- 1 jelmer jelmer  15K Aug 15 16:32 04-shell.html\n-rw-rw-r-- 1 jelmer jelmer  218 Aug  9 21:43 04-shell.qmd\n-rw-rw-r-- 1 jelmer jelmer  13K Aug 15 13:21 05-vars-loops.qmd\n-rw-rw-r-- 1 jelmer jelmer  13K Aug 15 16:33 05-vars-loops.rmarkdown\n-rw-rw-r-- 1 jelmer jelmer  74K Aug 15 16:32 06-scripts.html\n-rw-rw-r-- 1 jelmer jelmer  21K Aug 14 16:02 06-scripts.qmd\n-rw-rw-r-- 1 jelmer jelmer  58K Aug 15 16:33 07-software.html\n-rw-rw-r-- 1 jelmer jelmer  17K Aug 14 22:50 07-software.qmd\n-rw-rw-r-- 1 jelmer jelmer  46K Aug 15 16:32 08-slurm.html\n-rw-rw-r-- 1 jelmer jelmer  12K Aug 15 16:32 08-slurm.qmd\n-rw-rw-r-- 1 jelmer jelmer  16K Aug 15 16:32 09-examples.html\n-rw-rw-r-- 1 jelmer jelmer  312 Aug 15 13:23 09-examples.qmd\n-rw-rw-r-- 1 jelmer jelmer    0 Aug 15 13:21 18.txt\n-rw-rw-r-- 1 jelmer jelmer    0 Aug 15 13:21 Aug\ndrwxrwxr-x 2 jelmer jelmer 4.0K Aug 10 15:09 img\n-rw-rw-r-- 1 jelmer jelmer    0 Aug 15 13:21 README_2022-08-15.txt\n-rw-rw-r-- 1 jelmer jelmer    0 Aug 15 13:21 README2_Thu,\n-rw-rw-r-- 1 jelmer jelmer    0 Aug 15 13:21 README3_Thu, Aug 18.txt\ndrwxrwxr-x 2 jelmer jelmer 4.0K Aug 14 15:16 sandbox\n\n\n\ninput_file=04-shell.qmd\n\nls -lh $input_file \n\n-rw-rw-r-- 1 jelmer jelmer 218 Aug  9 21:43 04-shell.qmd\n\n\n\n\n2.2 Rules for naming variables\nVariable names:\n\nCan contain letters, numbers, and underscores\nCannot contain spaces, periods, or other special symbols\nCannot start with a number\n\n\n\n2.3 Command substitution\nIf you want to store the result of a command in a variable, you can use a construct called “command substitution” by wrapping the command inside $():\n\n# (date +%F will return the date in YYYY-MM-DD format)\ntoday=$(date +%F)\n\n# Create a file with our $today variable:\ntouch README_$today.txt\n\n# Check the name of our newly created file:\nls README_*\n\nREADME_2022-08-15.txt\n\n\n\n# Define a filename that we'll use in the next few commands:\ninput_file=shell-scripting.qmd\n\n# `wc -l` will count the number of lines\n# Using `<` (input redirection) is a trick to avoid the filename from being printed \nnlines=$(wc -l < $input_file)\n\n# We can directly use the variables in our quoted echo statement:\necho \"The file $input_file has $nlines lines\"\n\nbash: line 6: shell-scripting.qmd: No such file or directory\nThe file shell-scripting.qmd has  lines\n\n\nCommand substitution can for instance be useful when you want your script to report some results, or when a next step in the script depends on a previous result.\n\n\n2.4 Environment variables\nThere are also predefined variables in the Unix shell: that is, variables that exist in your environment by default. These so-called “environment variables” are always spelled in all-caps:\n\n# Environment variable $USER contains your user name \necho $USER\n\njelmer\n\n\n\n# Environment variable $HOME contains the path to your home directory\necho $HOME\n\n/home/jelmer\n\n\nEnvironment variables can provide useful information. We’ll see them again when we talk about the SLURM compute job scheduler.\n\n\n2.5 Quoting variables\nAbove, we learned that a variable name cannot contain spaces. But what happens if our variable’s value contains spaces?\nFirst off, when we try to assign the variable without quotes, we get an error:\n\ntoday=Thu, Aug 18\n\n\nAug: command not found\n\nBut it works when we quote (with double quotes, \"...\") the entire string that makes up the value:\n\ntoday=\"Thu, Aug 18\"\necho $today\n\nThu, Aug 18\n\n\nNow, let’s try to reference this variable:\n\ntouch README2_$today.txt\nls README2_*\n\n\n\nREADME2_Thu,\n\n\n\n\n\n\n\n\nWhat went wrong here? How many files were created?\n\n\n\n\n\nThe shell performed so-called field splitting using a space as a separator, splitting the value into three separate units – as a result, three files were created: README2_Thu, (listed above), as well as Aug and 18.txt.\nThe following code will list all these three files:\n\n# `ls -t` will sort by last-modified date, and `head -n 3` prints the top 3\n# Therefore, this will print the last 3 files that were created/modified\nls -t | head -n 3\n\n18.txt\nAug\nREADME2_Thu,\n\n\n\n\n\nSimilar to what we had to do when assigning the variable, our problems can be avoided by quoting the variable when we reference it:\n\ntouch README3_\"$today\".txt\nls README3_*\n\n\n\nREADME3_Thu, Aug 18.txt\n\n\nAnother issue we can run into when we don’t quote variables is that we can’t explicitly define where a variable name ends within a longer string of text:\n\necho README_$today_final.txt\n\n\n\nREADME_.txt\n\n\n\n\n\n\n\n\nWhat went wrong here?\n\n\n\n\n\n\nFollowing a $, the shell will stop interpreting characters as being part of the variable name only when it encounters a character that cannot be part of a variable name, such as a space or a period.\nSince variable names can contain underscores, it will look for the variable $today_final, which does not exist.\nImportantly, the shell does not error out when you reference a non-existing variable – it basically ignores it, such that README_$today_final.txt becomes README_.txt, as if we hadn’t referenced any variable.\n\n\n\n\nQuoting solves this issue, too:\n\necho README_\"$today\"_final.txt\n\n\n\nREADME_Thu, Aug 18_final.txt\n\n\n\n\n\n\n\n\nMore on quoting – and double vs. single quotes\n\n\n\n\n\nBy double-quoting a variable, we are essentially escaping (or “turning off”) the default special meaning of the space as a separator, and are asking the shell to interpret it as a literal space.\nSimilarly, we are escaping other “special characters”, such as globbing wildcards, with double quotes. Compare:\n\necho *\n\n01-intro.html 01-intro.qmd 02-osc.html 02-osc.qmd 03-vscode.qmd 04-shell.html 04-shell.qmd 05-vars-loops.qmd 05-vars-loops.rmarkdown 06-scripts.html 06-scripts.qmd 07-software.html 07-software.qmd 08-slurm.html 08-slurm.qmd 09-examples.html 09-examples.qmd 18.txt Aug img README_2022-08-15.txt README2_Thu, README3_Thu, Aug 18.txt sandbox\n\n\n\necho \"*\"\n\n*\n\n\nHowever, as we also saw above, double quotes do not turn off the special meaning of $ (i.e., denoting a string as a variable):\n\necho \"$today\"\n\n\n\nThu, Aug 18\n\n\n…but single quotes will:\n\necho '$today'\n\n$today\n\n\n\n\n\nAll in all, it is good practice to quote variables when you reference them: it never hurts, and avoids unexpected surprises."
  },
  {
    "objectID": "sessions/05-vars-loops.html#for-loops",
    "href": "sessions/05-vars-loops.html#for-loops",
    "title": "Variables and Loops",
    "section": "3 For loops",
    "text": "3 For loops\nLoops are a universal element of programming languages, and are extremely useful to repeat operations, such as when you want to run the same script or command for multiple files.\nHere, we’ll only cover what is by far the most common type of loop: the for loop.\nfor loops iterate over a collection, such as a list of files: that is, they allow you to perform one or more actions for each element in the collection, one element at a time.\n\n3.1 for loop syntax and mechanics\nLet’s see a first example, where our collection is just a very short list of numbers (1, 2, and 3):\n\nfor a_number in 1 2 3; do\n    echo \"In this iteration of the loop, the number is $a_number\"\n    echo \"--------\"\ndone\n\nIn this iteration of the loop, the number is 1\n--------\nIn this iteration of the loop, the number is 2\n--------\nIn this iteration of the loop, the number is 3\n--------\n\n\nfor loops contain the following mandatory keywords:\n\n\n\n\n\n\n\nKeyword\nPurpose\n\n\n\n\nfor\nAfter for, we set the variable name\n\n\nin\nAfter in, we specify the collection we are looping over\n\n\ndo\nAfter do, we have one ore more lines specifying what to do with each item\n\n\ndone\nTells the shell we are done with the loop\n\n\n\n\n\n\n\n\n\nWhy the semicolon ; before do?\n\n\n\n\n\nA semicolon separates two commands written on a single line – for instance, instead of:\n\nmkdir results\ncd results\n\n…you could equivalently type:\n\nmkdir results; cd results\n\nThe ; in the for loop syntax has the same function, and as such, an alternative way to format a for loop is:\n\nfor a_number in 1 2 3\ndo\n    echo \"In this iteration of the loop, the number is $a_number\"\ndone\n\nBut that’s one line longer and a bit awkwardly asymmetric.\n\n\n\nIt is important to realize that the loop runs sequentially for each item in the collection, and will therefore run as many times as there are items in the collection.\nThe following example, where we let the computer sleep for 1 second before printing the date and time with the date command, demonstrates that the loop is being executed sequentially:\n\nfor a_number in 1 2 3; do\n    echo \"In this iteration of the loop, the number is $a_number\"\n    #sleep 1s          # Let the computer sleep for 1 second\n    date              # Print the date and time\n    echo \"--------\"\ndone\n\nIn this iteration of the loop, the number is 1\nMon Aug 15 04:33:08 PM CEST 2022\n--------\nIn this iteration of the loop, the number is 2\nMon Aug 15 04:33:08 PM CEST 2022\n--------\nIn this iteration of the loop, the number is 3\nMon Aug 15 04:33:08 PM CEST 2022\n--------\n\n\nThe aspect that is perhaps most difficult to understand is that in each iteration of the loop, one element in the collection (in the example above, either 1, 2, or 3) is being assigned to the variable specified after for (in the example above, a_number).\nWhen we specify the collection “manually”, like we did above with numbers, we separate the elements by a space, as this example also shows:\n\nfor mushroom in morel destroying_angel eyelash_cup; do\n    echo \"$mushroom is an Ohio mushroom\"\ndone\n\nmorel is an Ohio mushroom\ndestroying_angel is an Ohio mushroom\neyelash_cup is an Ohio mushroom\n\n\n\n\n3.2 Looping over files with globbing\nIn practice, we rarely manually list the collection of items we want to loop over. Instead, we commonly loop over files directly using globbing:\n\n# We make sure we only select gzipped FASTQ files using the `*fastq.gz` glob\nfor fastq_file in data/raw/*fastq.gz; do\n    echo \"File $fastq_file has $(wc -l < $fastq_file) lines.\"\n    # More processing...\ndone\n\nIf needed, you can use your globbing / wild card skills to narrow down the file selection:\n\n# Perhaps we only want to select R1 files (forward reads): \nfor fastq_file in data/raw/*R1*fastq.gz; do\n    # Some file processing...\ndone\n\n# Or only filenames starting with A or B:\nfor fastq_file in data/raw/[AB]*fastq.gz; do\n    # Some file processing...\ndone\n\n\n\n\n\n\n\nAlternatives to looping with a glob\n\n\n\n\n\nWith genomics data, the routine of looping over an entire directory of files, or selections made with simple globbing patterns, should serve you very well.\nBut in some cases, you may want to iterate only over a specific list of filenames (or partial filenames such as sample IDs) that represent a complex selection.\n\nIf this is a short list, you could directly specify it in the loop:\n\nfor sample in A1 B6 D3; do\n    R1=data/fastq/\"$sample\"_R1.fastq.gz\n    R2=data/fastq/\"$sample\"_R2.fastq.gz\n    # Some file processing...\ndone\n\nIf it is a longer list, you could create a simple text file with one line per sample ID / filename, and use command substitution as follows:\n\nfor fastq_file in $(cat file_of_filenames.txt); do\n    # Some file processing...\ndone\n\n\n(In cases like this, Bash arrays (basically, variables that consist of multiple values, like a vector in R) or while loops may provide more elegant solutions, but those are outside the scope of this introduction.)"
  },
  {
    "objectID": "sessions/03-vscode.html",
    "href": "sessions/03-vscode.html",
    "title": "The VS Code Text Editor",
    "section": "",
    "text": "In this module, we will learn the basics of a fancy text editor called Visual Studio Code (VS Code for short). Conveniently, we can use a version of this editor (sometimes referred to as Code Server) in our browser via the OSC OnDemand website.\nWe will use VS Code throughout the workshop as practically a one-stop solution for our computing activities at OSC: accessing the Unix shell and writing scripts. This is also how I use this editor in my daily work.\nTo emphasize the additional functionality relative to basic text editors like Notepad and TextEdit, editors like VS Code are also referred to as “IDEs”: Integrated Development Environments. If you’ve ever worked with R, the RStudio program is another good example of an IDE."
  },
  {
    "objectID": "sessions/03-vscode.html#starting-vs-code-at-osc",
    "href": "sessions/03-vscode.html#starting-vs-code-at-osc",
    "title": "The VS Code Text Editor",
    "section": "1 Starting VS Code at OSC",
    "text": "1 Starting VS Code at OSC\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nEnter 3 in the box Number of hours\nEnter /fs/ess/scratch/PAS2250 in the box Working Directory\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\n\n\n\n\n\n\n\n\n\n\nVS Code runs on a login node\n\n\n\nIn the previous module, we’ve learned that all serious computation at OSC should be done not on login nodes but on compute nodes.\nStarting an RStudio session, for instance, requires filling out a similar form, and RStudio will subsequently run on a compute node and your selected OSC project will be charged.\nRunning VS Code is a slightly peculiar case: we do have to fill out a form and reserve a pre-specified number of hours (the session will actually stop working after the allotted time has passed), but we’re on a login node and are not being charged."
  },
  {
    "objectID": "sessions/03-vscode.html#getting-started-with-vs-code",
    "href": "sessions/03-vscode.html#getting-started-with-vs-code",
    "title": "The VS Code Text Editor",
    "section": "2 Getting started with VS Code",
    "text": "2 Getting started with VS Code\n\n\n\n\n2.1 Side bars\nThe narrow side bar on the far left has:\n\nA      (“hamburger menu” icon) in the top, which has most of the standard menu items that you often find in a top bar, like File.\nA      (cog wheel icon) in the bottom, through which you can mainly access settings.\nA bunch of icons in the middle that serve to switch between different options for the wide side bar (to the right of the narrow side bar), which can show:\n\nExplorer: File browser (and, e.g., an outline for the active file)\nSearch: To search recursively across all files in the active folder\nSource Control: To work with version control systems like Git (not used in this workshop)\nRun and Debug: For debugging your code (not used in this workshop)\nExtensions: To install extensions (we’ll install one later)\n\n\n\n\n2.2 Editor pane and Welcome document\nThe main part of the VS Code is the editor pane. Whenever you open VS Code, a tab with a Welcome document is automatically opened. This provides some help for beginners, but also, for example, an overview of recently opened folders.\nWe can also use the Welcome document to open a new text file by clicking New file below Start (alternatively, click      =>   File   =>   New File). We’ll work with files starting tomorrow, but if you want, you could already start a file with notes on the workshop now.\n\n\n\n\n\n\nRe-open the Welcome document\n\n\n\nIf you’ve closed the Welcome document but want it back, click      =>   Help   =>   Welcome.\n\n\n\n\n2.3 Terminal\nBy default, no terminal is open in VS Code – to do so, click the      => Terminal => New Terminal.\nIn the terminal, the prompt says Singularity>. This is because in OSC OnDemand, VS Code runs inside a Singularity container (for our purposes, it is not important what that means, exactly). To break out of the Singularity shell and get a regular Bash Unix shell, type bash and press Enter.\nIn the next module, Mike will teach us how to use the terminal."
  },
  {
    "objectID": "sessions/03-vscode.html#some-tips-and-tricks",
    "href": "sessions/03-vscode.html#some-tips-and-tricks",
    "title": "The VS Code Text Editor",
    "section": "3 Some tips and tricks",
    "text": "3 Some tips and tricks\n\n3.1 Making use of your screen’s real estate\nSince we are using VS Code inside a browser window, we are unfortunately losing some screen space. Make sure to maximize the browser window and if you have a bookmarks bar, try to hide it (for Chrome: Ctrl/⌘+Shift+B).\n\n\n3.2 Resizing panes\nYou can resize panes (the terminal, editor, and wide sidebar) by hovering your cursor over the borders and then dragging it.\n\n\n3.3 The Command Palette\nTo access all the menu options that are available in VS Code, the so-called “Command Palette” can be handy, especially if you know what you are looking for.\nTo access the Command Palette, click      and then Command Palette or press F1 (or Ctrl/⌘+Shift+P).\nFor a quick test, open the Command Palette and start typing “color theme”, and you’ll see the relevant options pop up.\n\n\n3.4 Color themes\nTo try out different color themes for the entire program, click      and then Color Theme. (I like “Quiet Light”.)"
  },
  {
    "objectID": "sessions/03-vscode.html#working-directory",
    "href": "sessions/03-vscode.html#working-directory",
    "title": "The VS Code Text Editor",
    "section": "4 Working directory",
    "text": "4 Working directory\nSetting a “working directory” means that you designate a folder on a computer as the starting point for your operations.\n\n\n\n\n\n\nFolder vs. directory\n\n\n\n“Folder” and “directory” mean the same thing – the latter is most commonly used in the context of the Unix Shell.\n\n\nVS Code has a concept of a working directory that is effective in all parts of the program: in the file explorer in the side bar, in the terminal, and when saving or opening files in the editor.\nIn this workshop, we’ll exclusively work within the folder /fs/ess/scratch/PAS2250 (you’ll make personal folders within there shortly). By opening this folder beforehand (we did this in the form on the OnDemand site), we make sure that VS Code always takes this folder as a starting point, which will make navigation and saving files much easier.\n\n\n\n\n\n\nTaking off where you were\n\n\n\nAdditionally, when you reopen a folder later, VS Code will to some extent resume where you were before! It will reopen the text files that you had open and if you had an active terminal, it will also open a terminal. This is very convenient, especially when you start working on multiple projects (different folders) in VS Code and switch between those.\n\n\n\n\n\n\n\n\nSwitching folders\n\n\n\nTo switch to a different folder from within VS Code, click      =>   File   =>   Open Folder."
  },
  {
    "objectID": "sessions/03-vscode.html#addendum-keyboard-shortcuts",
    "href": "sessions/03-vscode.html#addendum-keyboard-shortcuts",
    "title": "The VS Code Text Editor",
    "section": "5 Addendum: keyboard shortcuts",
    "text": "5 Addendum: keyboard shortcuts\nWorking with keyboard shortcuts (also called “keybindings”) for common operations can be a lot faster than using your mouse. Below are some useful ones for VS Code (for Mac, replace Ctrl by ⌘).\n\n\n\n\n\n\nKeyboard shortcut cheatsheet\n\n\n\nFor a single-page PDF overview of keyboard shortcuts for your operating system:      =>   Help   =>   Keyboard Shortcut Reference. (Or for direct links to these PDFs: Windows / Mac / Linux.)\n\n\n\nToggle the wide side bar: Ctrl+B\nOpen a terminal: Ctrl+` or Ctrl+Shift+C.\nToggle between the terminal and the editor pane: Ctrl+` and Ctrl+1.\nLine actions:\n\nCtrl+X / C will cut/copy the entire line where the cursor is, when nothing is selected (!)\nCtrl+Shift+K will delete a line\nAlt+⬆/⬇ will move lines up or down.\n\nMultiple cursors: Press & hold Ctrl+Shift, then ⬆/⬇ arrows to add cursors upwards or downwards.\nToggle line comment (“comment out” code, and removing those comment signs): Ctrl+/\nSplit the editor window vertically: Ctrl+\\ (See also the options in      View => Editor Layout)\n\n\n\n\n\n\n\nBrowser interference\n\n\n\nUnfortunately, some VS Code and terminal keyboard shortcuts don’t work in this setting where we are using VS Code inside a browser, because existing browser keyboard shortcuts take precedence.\nIf you end up using VS Code a lot in your work, it is therefore worth switching to your own installation of the program (see At-home bonus: local installation)"
  },
  {
    "objectID": "sessions/03-vscode.html#at-home-bonus-local-installation",
    "href": "sessions/03-vscode.html#at-home-bonus-local-installation",
    "title": "The VS Code Text Editor",
    "section": "6 At-home bonus: local installation",
    "text": "6 At-home bonus: local installation\nAnother nice feature of VS Code is that it is freely available for all operating systems (and even though it is made by Microsoft, it is also open source).\nTherefore, if you like the program, you can also install it on your own computer and do your local text editing / script writing in the same environment at OSC (it is also easy to install on OSU-managed computers, because it is available in the Self Service software installer).\nEven better, the program can be “tunneled into” OSC, so that your working directory for the entire program can be at OSC rather than on your local computer. This gives the same experience as using VS Code through OSC OnDemand, except that you’re not working witin a browser window, which has some advantages (also: no need to fill out a form, or to break out of the Singularity shell).\nFor installation and SSH-tunneling setup, see this page - TBA."
  },
  {
    "objectID": "sessions/08-slurm.html#common-sbatch-options",
    "href": "sessions/08-slurm.html#common-sbatch-options",
    "title": "Compute Jobs with Slurm",
    "section": "4 Common sbatch options",
    "text": "4 Common sbatch options\n\n4.1 --account: The OSC project\nAs seen above. Always specify the project when submitting a batch job.\n\n\n4.2 --time: Time limit (“wall time”)\nSpecify the maximum amount of time your job will run for. Wall time is a term meant to distinguish it from, say “core hours”: if a job runs for 2 hour and used 8 cores, the wall time was 2 hours and the number of core hours was 2 x 8 = 16.\n\nYour job gets killed as soon as it hits the specified time limit!\nYou will only be charged for the time your job actually used.\nIn general, shorter jobs are likely to start running sooner\nThe default is 1 hour. Acceptable time formats include:\n\nminutes\nhours:minutes:seconds\ndays-hours\n\nFor single-node jobs, up to 168 hours (7 days) can be requested. If that’s not enough, you can request access to the longserial queue for jobs of up to 336 hours (14 days).\n\n\n#!/bin/bash\n#SBATCH --time=1:00:00\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are uncertain about the time your job will take, ask for (much) more time than you think you will need.\n\n\n\n\n4.3 --mem: RAM memory\nSpecify a maximum amount of RAM (Random Access Memory) that your job can use.\n\nThe default unit is MB (MegaBytes) — use “G” for GB.\nThe default amount is 4 GB per core that you reserve (see below).\nLike with the time limit, your job gets killed when it hits the memory limit. But this is not that common so I would usually not specify unless the program I’m running reports that it needs a lot of memory, or I got “out-of-memory” errors when trying to run the script before.\n\n#!/bin/bash\n#SBATCH --mem=20G\n\n\n4.4 Cores (& nodes and tasks)\nSpecify the number of nodes (≈ computers), cores, or “tasks” (processes). These are separate but related options, and this is where things can get confusing!\n\nSlurm for the most part uses “core” and “CPU” interchangeably3. More generally, “thread” is also commonly used interchangeably with core/CPU4.\n\n\nRunning a program that uses multiple threads/cores/CPUs (“multi-threading”) is common. In such cases, specify the number of threads/cores/CPUs n with --cpus-per-task=n (and keep --nodes and --ntasks at their defaults of 1).\nThe program you’re running may have an argument like --cores or --threads, which you should then set to n as well.\n\n\n\n\n\n\n\nUncommon cases\n\n\n\n\nOnly ask for >1 node when you have explicit parallelization with something like “MPI”, which is uncommon in bioinformatics.\nFor jobs with multiple processes (tasks), use --ntasks=n or --ntasks-per-node=n.\n\n\n\n\n\n\n\n\n\n\n\n\nResource/use\nshort\nlong\ndefault\n\n\n\n\nNr. of cores/CPUs/threads (per task)\n-c 1\n--cpus-per-task=1\n1\n\n\nNr. of “tasks” (processes)\n-n 1\n--ntasks=1\n1\n\n\nNr. of tasks per node\n-\n--ntasks-per-node=1\n1\n\n\nNr. of nodes\n-N 1\n--nodes=1\n1\n\n\n\n\n#!/bin/bash\n#SBATCH --cpus-per-task=2\n\n\n\n4.5 --output: Slurm log files\nAs we saw above, by default, all output from a script that would normally5 be printed to screen will end up in Slurm log file when we submit the script as a batch job. This file will be created in the directory from which you submitted the script, and will be called slurm-<job-number>.out, e.g. slurm-12431942.out.\nBut it is possible to change the name of this file. For instance, it can be useful to include the name of the program that the script runs, so that it’s easier to recognize this file later.\nWe can do this with the --output option, e.g. --output=slurm-fastqc.out if we were running FastQC.\nHowever, you’ll generally want to keep the batch job number in the file name too6. Since we won’t know the batch job number in advance, we need a trick here and that is to use %j, which represents the batch job number:\n\n#!/bin/bash\n#SBATCH --output=slurm-fastqc-%j.out\n\n\n\n\n\n\n\nstdout and stderr\n\n\n\nBy default, two output streams “standard output” (stdout) and “standard error” (stderr) all end up in the same Slurm log file, but it is also possible to separate them into two separate files.\nBecause stderr, as you might have guessed, often contains error messages, it could be useful to have those in a separate file. You can make that happen with the --error option, e.g. --error=slurm-fastqc-%j.err.\nHowever, reality is more messy: some programs print their main output not to a file but to standard out, and their logging output, errors and regular messages alike, to standard error. Yet other programs use stdout or stderr for all messages. I therefore usually only use --output.\n\n\n\n\n4.6 Other sbatch options\nHere are some other sbatch options that can be useful in certain cases:\n\n\n\n\n\n\n\nResource/use\nlong option\n\n\n\n\nJob name (displayed in the queue)\n--job-name=fastqc\n\n\nPartition (=queue type)\n--partition=longserial (long jobs)  --partition=hugemem (jobs needing lots of memory)\n\n\nGet email when job starts, ends, fails,  or all of the above\n--mail-type=START  --mail-type=END  --mail-type=FAIL  --mail-type=ALL\n\n\nLet job begin at/after specific time\n--begin=2021-02-01T12:00:00\n\n\nLet job begin after other job is done\n--dependency=afterany:123456"
  },
  {
    "objectID": "sessions/08-slurm.html#addendum-table-with-all-discussed-sbatch-options",
    "href": "sessions/08-slurm.html#addendum-table-with-all-discussed-sbatch-options",
    "title": "Compute Jobs with Slurm",
    "section": "5 Addendum: Table with all discussed sbatch options",
    "text": "5 Addendum: Table with all discussed sbatch options\n\n\n\n\n\n\n\n\n\nResource/use\nshort\nlong\ndefault\n\n\n\n\nProject to be billed\n-A PAS0471\n--account=PAS0471\nN/A\n\n\nTime limit\n-t 4:00:00\n--time=4:00:00\n1:00:00\n\n\nNr of nodes\n-N 1\n--nodes=1\n1\n\n\nNr of cores\n-c 1\n--cpus-per-task=1\n1\n\n\nNr of “tasks” (processes)\n-n 1\n--ntasks=1\n1\n\n\nNr of tasks per node\n-\n--ntasks-per-node\n1\n\n\nMemory limit per node\n-\n--mem=4G\n(4G)\n\n\nLog output file (%j = job number)\n-o\n--output=slurm-fastqc-%j.out\n\n\n\nError output (stderr)\n-e\n--error=slurm-fastqc-%j.err\n\n\n\nJob name (displayed in the queue)\n-\n--job-name=fastqc\n\n\n\nPartition (=queue type)\n-\n--partition=longserial (long jobs)  --partition=hugemem (jobs needing lots of memory)\n\n\n\nGet email when job starts, ends, fails,  or all of the above\n-\n--mail-type=START  --mail-type=END  --mail-type=FAIL  --mail-type=ALL\n\n\n\nLet job begin at/after specific time\n-\n--begin=2021-02-01T12:00:00\n\n\n\nLet job begin after other job is done\n-\n--dependency=afterany:123456"
  },
  {
    "objectID": "sessions/01-intro.html#what-you-will-learn",
    "href": "sessions/01-intro.html#what-you-will-learn",
    "title": "Introduction to the Workshop",
    "section": "What you will learn",
    "text": "What you will learn\nThe focus of the workshop is on building some general (foundational) skills for analyzing genomics data.\nCommand-line programs are preferred for many of the steps to analyze genomic sequencing data. Because such datasets tend to contain a lot of data, it is also preferable to run your analyses not on a laptop or desktop, but at a compute cluster like the Ohio Supercomputer Center (OSC).\nThese realities mean that in the field of genomics, you need the following set of skills that you may not have been thought during your biology education:\n\nHaving a basic understanding of a compute cluster (supercomputer)\n\nAnd being able to:\n\nUse the Unix shell (work in a terminal)\nWrite small shell scripts\nSubmit scripts to a cluster’s “queue” and monitor and manage the resulting compute jobs\nActivate and probably install software in a Linux environment where you don’t have “admin rights”\n\nWe will teach the basics of these skills during this workshop!\n\n\n\n\n\n\nWhat you won’t learn\n\n\n\nIt may be useful to point out that we will not teach you much, if anything, about:\n\nDetails of genomic data file types\nDetails of specific (genomic) analyses\nMaking biological inferences from your data"
  },
  {
    "objectID": "modules/08-slurm.html",
    "href": "modules/08-slurm.html",
    "title": "Compute Jobs with Slurm",
    "section": "",
    "text": "We have so far been working on login nodes at OSC, but in order to run some actual analyses, you will need access to compute nodes.\nAutomated scheduling software allows hundreds of people with different requirements to access compute nodes effectively and fairly. For this purpose, OSC uses the Slurm scheduler (Simple linux utility for resource management).\nA temporary reservation of (parts of) a compute node is called a compute job. What are the options to start a compute job at OSC?\nWhen running command-line programs for genomics analyses, batch jobs are the most useful and will be the focus of this module. We’ll also touch on interactive shell jobs, which can occasionally be handy and are requested and managed in a very similar way to batch jobs."
  },
  {
    "objectID": "modules/08-slurm.html#setup",
    "href": "modules/08-slurm.html#setup",
    "title": "Compute Jobs with Slurm",
    "section": "1 Setup",
    "text": "1 Setup\n\n\n\n\n\n\nStarting a VS Code session with an active terminal (click here)\n\n\n\n\n\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 4 or more in the box Number of hours\nTo avoid having to switch folders within VS Code, enter /fs/ess/scratch/PAS2250/participants/<your-folder> in the box Working Directory (replace <your-folder> by the actual name of your folder).\nClick Launch.\n\nOn the next page, once the top bar of the box is green and says Runnning, click Connect to VS Code.\nOpen a terminal:    => Terminal => New Terminal.\nIn the terminal, type bash and press Enter.\nType pwd to check you are in /fs/ess/scratch/PAS2250. If not, click    =>   File   =>   Open Folder and enter /fs/ess/scratch/PAS2250/<your-folder> (replace <your-folder> by the actual name of your folder)."
  },
  {
    "objectID": "modules/08-slurm.html#interactive-shell-jobs",
    "href": "modules/08-slurm.html#interactive-shell-jobs",
    "title": "Compute Jobs with Slurm",
    "section": "2 Interactive shell jobs",
    "text": "2 Interactive shell jobs\nInteractive shell jobs will grant you interactive shell access on a compute node. Working in an interactive shell job is operationally identical to working on a login node as we’ve been doing so far, but the difference is that it’s now okay to use significant computing resources. (How much and for how long depends on what you reserve.)\n\n2.1 Using srun\nA couple of different commands can be used to start an interactive shell job. I prefer the general srun command1, which we can use with the --pty /bin/bash option to get an interactive Bash shell.\nHowever, if we run that command without additional options, we get an error:\n\nsrun --pty /bin/bash\n\n\nsrun: error: ERROR: Job invalid: Must specify account for job\nsrun: error: Unable to allocate resources: Unspecified error\n\nAs the error message Must specify account for job tries to tell us, we need to indicate which OSC project (or as SLURM puts it, “account”) we want to use for this compute job. This is because an OSC project always has to be charged for the computing resources used during a compute job.\nTo specify the project/account, we can use the --account= option followed by the project number:\n\nsrun --account=PAS2250 --pty /bin/bash\n\n\nsrun: job 12431932 queued and waiting for resources\nsrun: job 12431932 has been allocated resources\n[…regular login info, such as quota, not shown…]\n[jelmer@p0133 PAS2250]$\n\nThere we go! First we got some Slurm scheduling info:\n\nInitially, the job is “queued”: that is, waiting to start.\nVery soon (usually!), the job has been “allocated resources”: that is, computing resources such as a compute node were found and reserved for the job.\n\nThen:\n\nThe job starts and because we’ve reserved an interactive shell job, this means that a new Bash shell is initiated: for that reason, we get to see our regular login info once again.\nMost importantly, we are no longer on a login node but on a compute node, as our prompt hints at: we switched from something like [jelmer@pitzer-login04 PAS2250]$ to the [jelmer@p0133 PAS2250]$ shown above.\nNote also that the job has a number (above: job 12431932): every compute job has such a unique identifier among all jobs by all users at OSC, and we can use this number to monitor and manage it. All of us will therefore see a different job number pop up.\n\n\n\n\n\n\n\nOSC projects\n\n\n\nDuring this workshop, we can all use the project PAS2250, which is actually a project that OSC has freely given me to introduce people to working at OSC. The project will still be charged but the credits on it were freely awarded.\nTo work on your own research project at OSC, you will either have to get your own project (typically, PIs get one for their lab or for a specific research project) or you can become an MCIC member and use the MCIC project.\n\n\n\n\n2.2 Compute job options\nThe --account= option is just one of out of many options we can use when reserving a compute job, but is the only one that always has to be specified (including for batch jobs and for Interactive Apps).\nDefaults exist for all other options, such as the amount of time (1 hour) and the number of cores (1). These options are all specified in the same way for interactive and batch jobs, and we’ll dive into them below.\n\n\n\n\n\n\nTip\n\n\n\nMany SLURM options have a long format (--account=PAS2250) and a short format (-A PAS2250), which can generally be used interchangeably. For clarity, we’ll try to stick to long format options during this workshop."
  },
  {
    "objectID": "modules/08-slurm.html#intro-to-batch-jobs",
    "href": "modules/08-slurm.html#intro-to-batch-jobs",
    "title": "Compute Jobs with Slurm",
    "section": "3 Intro to batch jobs",
    "text": "3 Intro to batch jobs\nWhen requesting batch jobs, we are asking the Slurm scheduler to run a script on a compute node. For this reason, we can also refer to it as “submitting a script (to the queue)”.\nIn contrast to interactive shell jobs, we stay in our current shell on a login node when submitting a script, and cannot really interact with the process on the compute node, other than:\n\nOutput from the script that would normally be printed to screen ends up in a file.\nWe can do things like monitoring whether the job is still running and cancelling the job, which will revoke the compute node reservation and stop the ongoing process.\n\n\n\n\n\n\n\nTip\n\n\n\nThe script that we submit can be in different languages but typically, including in all examples in this workshop, they are shell (Bash) scripts.\n\n\n\n3.1 The sbatch command\nWhereas we used Slurm’s srun command to start an interactive shell job, we use its sbatch command to submit a script.\nRecall from the Bash scripting module that we can run a Bash script as follows:\n\nbash printname.sh Jane Doe\n\nFirst name: Jane\nLast name: Doe\n\n\n\n\n\n\n\n\nCan’t find the printname.sh script?\n\n\n\n\n\n\nOpen a new file in the VS Code editor (     =>   File   =>   New File) and save it as printname.sh\nCopy the code below into the script:\n\n\n#!/bin/bash\nset -ueo pipefail\n\nfirst_name=$1\nlast_name=$2\n  \necho \"First name: $first_name\"\necho \"Last name: $last_name\"\n\n\n\n\nThe above command ran the script on our current node, a login node. To instead submit the script to the Slurm queue, we would start by simply replacing bash by sbatch:\n\nsbatch printname.sh Jane Doe\n\n\nsrun: error: ERROR: Job invalid: Must specify account for job\nsrun: error: Unable to allocate resources: Unspecified error\n\nAs we’ve learned, we always have to specify the OSC account when submitting a compute job. Conveniently, we can also specify Slurm/sbatch options inside our script, but first, let’s add the --account option on the command line:\n\nsbatch --account=PAS2250 printname.sh Jane Doe\n\n\nSubmitted batch job 12431935\n\n\n\n\n\n\n\nsbatch options vs. script arguments\n\n\n\n\nsbatch [sbatch-options] myscript.sh [script-arguments]\n\n\nsbatch printname.sh                             # No options/arguments for either\nsbatch printname.sh Jane Doe                    # Script arguments but no sbatch option\nsbatch --account=PAS2250 printname.sh           # sbatch option but no script arguments\nsbatch --account=PAS2250 printname.sh Jane Doe  # Both sbatch option and script arguments\n\n\n\n\n\n3.2 Adding sbatch options in scripts\nInstead of specifying Slurm/sbatch options on the command-line when we submit the script, we can also add these options inside the script.\nThis is handy because even though we have so far only seen the account= option, you often want to specify several options. That would lead to very long sbatch commands. Additionally, it can be practical to store a script’s typical Slurm options along with the script itself.\nWe add the options in the script using another type of special comment line akin to the shebang line, marked by #SBATCH. The equivalent of adding --account=PAS2250 after sbatch on the command line, is a line in a script that reads #SBATCH --account=PAS2250.\nJust like the shebang line, the #SBATCH line(s) should be at the top of the script. Let’s add one to the printname.sh script, such that the first few lines read:\n\n#!/bin/bash\n#SBATCH --account=PAS2250\n\nset -ueo pipefail\n\nAfter having added this to the script, we can successfully run our earlier sbatch command without options:\n\nsbatch printname.sh Jane Doe\n\n\nSubmitted batch job 12431942\n\n\n\n\n\n\n\nRunning a script with #SBATCH in other contexts\n\n\n\nBecause #SBATCH lines are special comment lines, they will simply be ignored and not throw any errors when you run a script that contains them in other contexts: when not running them as a batch job at OSC, or even when running them on a computer without Slurm installed.\n\n\n\n\n\n\n\n\nsbatch option precedence\n\n\n\nAny sbatch option provided on the command line will override the equivalent option provided inside the script. This is sensible: we can provide “defaults” inside the script, and change one or more of those when needed on the command line.\n\n\n\n\n3.3 Where does the output go?\nAbove, we saw that when we ran the printname.sh script directly, its output was printed to the screen, whereas when we submitted it as a batch job, we merely got Submitted batch job 12431942 printed to screen. So where did our output go?\nIt ended up in a file slurm-12431942.out (i.e., slurm-<job-number>.out), which we might call a Slurm log file.\n\n\n\n\n\n\nAny idea why we might not want batch job output printed to screen, even if we could?\n\n\n\n\n\nThe power of submitting batch jobs is that you can submit many at once, e.g. one per sample, running the same script. If the output from all those scripts ends up on your screen, things become a big mess, and you have no lasting record of what happened.\n\n\n\nIt’s important to conceptually distinguish two broad types of output that a script may have:\n\nOutput that is printed to screen when we directly run a script, such as what was produced by our echo statements, by any errors that may occur, and possibly by a program that we run in the script.2 As we saw, this output ends up in the Slurm log file when we submit the script as a batch job.\nOutput that we redirect to a file (> myfile.txt) or that a program that we run in the script writes to file(s). This type of output will always end up in those very same files regardless of whether we run the script directly or as a batch job.\n\n\n\n\n\n\n\nTip\n\n\n\nBoth interactive and batch jobs start in the directory that they were submitted from: that is, your working directory will remain the same."
  },
  {
    "objectID": "modules/08-slurm.html#common-sbatch-options",
    "href": "modules/08-slurm.html#common-sbatch-options",
    "title": "Compute Jobs with Slurm",
    "section": "4 Common sbatch options",
    "text": "4 Common sbatch options\n\n4.1 --account: The OSC project\nAs seen above. Always specify the project when submitting a batch job.\n\n\n4.2 --time: Time limit (“wall time”)\nSpecify the maximum amount of time your job will run for. Wall time is a term meant to distinguish it from, say “core hours”: if a job runs for 2 hour and used 8 cores, the wall time was 2 hours and the number of core hours was 2 x 8 = 16.\n\nYour job gets killed as soon as it hits the specified time limit!\nYou will only be charged for the time your job actually used.\nIn general, shorter jobs are likely to start running sooner\nThe default is 1 hour. Acceptable time formats include:\n\nminutes\nhours:minutes:seconds\ndays-hours\n\nFor single-node jobs, up to 168 hours (7 days) can be requested. If that’s not enough, you can request access to the longserial queue for jobs of up to 336 hours (14 days).\n\n\n#!/bin/bash\n#SBATCH --time=1:00:00\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are uncertain about the time your job will take, ask for (much) more time than you think you will need.\n\n\n\n\n4.3 --mem: RAM memory\nSpecify a maximum amount of RAM (Random Access Memory) that your job can use.\n\nThe default unit is MB (MegaBytes) — use “G” for GB.\nThe default amount is 4 GB per core that you reserve (see below).\nLike with the time limit, your job gets killed when it hits the memory limit. But this is not that common so I would usually not specify unless the program I’m running reports that it needs a lot of memory, or I got “out-of-memory” errors when trying to run the script before.\n\n#!/bin/bash\n#SBATCH --mem=20G\n\n\n4.4 Cores (& nodes and tasks)\nSpecify the number of nodes (≈ computers), cores, or “tasks” (processes). These are separate but related options, and this is where things can get confusing!\n\nSlurm for the most part uses “core” and “CPU” interchangeably3. More generally, “thread” is also commonly used interchangeably with core/CPU4.\n\n\nRunning a program that uses multiple threads/cores/CPUs (“multi-threading”) is common. In such cases, specify the number of threads/cores/CPUs n with --cpus-per-task=n (and keep --nodes and --ntasks at their defaults of 1).\nThe program you’re running may have an argument like --cores or --threads, which you should then set to n as well.\n\n\n\n\n\n\n\nUncommon cases\n\n\n\n\nOnly ask for >1 node when you have explicit parallelization with something like “MPI”, which is uncommon in bioinformatics.\nFor jobs with multiple processes (tasks), use --ntasks=n or --ntasks-per-node=n.\n\n\n\n\n\n\n\n\n\n\n\n\nResource/use\nshort\nlong\ndefault\n\n\n\n\nNr. of cores/CPUs/threads (per task)\n-c 1\n--cpus-per-task=1\n1\n\n\nNr. of “tasks” (processes)\n-n 1\n--ntasks=1\n1\n\n\nNr. of tasks per node\n-\n--ntasks-per-node=1\n1\n\n\nNr. of nodes\n-N 1\n--nodes=1\n1\n\n\n\n\n#!/bin/bash\n#SBATCH --cpus-per-task=2\n\n\n\n4.5 --output: Slurm log files\nAs we saw above, by default, all output from a script that would normally5 be printed to screen will end up in Slurm log file when we submit the script as a batch job. This file will be created in the directory from which you submitted the script, and will be called slurm-<job-number>.out, e.g. slurm-12431942.out.\nBut it is possible to change the name of this file. For instance, it can be useful to include the name of the program that the script runs, so that it’s easier to recognize this file later.\nWe can do this with the --output option, e.g. --output=slurm-fastqc.out if we were running FastQC.\nHowever, you’ll generally want to keep the batch job number in the file name too6. Since we won’t know the batch job number in advance, we need a trick here and that is to use %j, which represents the batch job number:\n\n#!/bin/bash\n#SBATCH --output=slurm-fastqc-%j.out\n\n\n\n\n\n\n\nstdout and stderr\n\n\n\nBy default, two output streams “standard output” (stdout) and “standard error” (stderr) all end up in the same Slurm log file, but it is also possible to separate them into two separate files.\nBecause stderr, as you might have guessed, often contains error messages, it could be useful to have those in a separate file. You can make that happen with the --error option, e.g. --error=slurm-fastqc-%j.err.\nHowever, reality is more messy: some programs print their main output not to a file but to standard out, and their logging output, errors and regular messages alike, to standard error. Yet other programs use stdout or stderr for all messages. I therefore usually only use --output.\n\n\n\n\n4.6 Other sbatch options\nHere are some other sbatch options that can be useful in certain cases:\n\n\n\n\n\n\n\nResource/use\nlong option\n\n\n\n\nJob name (displayed in the queue)\n--job-name=fastqc\n\n\nPartition (=queue type)\n--partition=longserial (long jobs)  --partition=hugemem (jobs needing lots of memory)\n\n\nGet email when job starts, ends, fails,  or all of the above\n--mail-type=START  --mail-type=END  --mail-type=FAIL  --mail-type=ALL\n\n\nLet job begin at/after specific time\n--begin=2021-02-01T12:00:00\n\n\nLet job begin after other job is done\n--dependency=afterany:123456"
  },
  {
    "objectID": "modules/08-slurm.html#addendum-table-with-all-discussed-sbatch-options",
    "href": "modules/08-slurm.html#addendum-table-with-all-discussed-sbatch-options",
    "title": "Compute Jobs with Slurm",
    "section": "5 Addendum: Table with all discussed sbatch options",
    "text": "5 Addendum: Table with all discussed sbatch options\n\n\n\n\n\n\n\n\n\nResource/use\nshort\nlong\ndefault\n\n\n\n\nProject to be billed\n-A PAS0471\n--account=PAS0471\nN/A\n\n\nTime limit\n-t 4:00:00\n--time=4:00:00\n1:00:00\n\n\nNr of nodes\n-N 1\n--nodes=1\n1\n\n\nNr of cores\n-c 1\n--cpus-per-task=1\n1\n\n\nNr of “tasks” (processes)\n-n 1\n--ntasks=1\n1\n\n\nNr of tasks per node\n-\n--ntasks-per-node\n1\n\n\nMemory limit per node\n-\n--mem=4G\n(4G)\n\n\nLog output file (%j = job number)\n-o\n--output=slurm-fastqc-%j.out\n\n\n\nError output (stderr)\n-e\n--error=slurm-fastqc-%j.err\n\n\n\nJob name (displayed in the queue)\n-\n--job-name=fastqc\n\n\n\nPartition (=queue type)\n-\n--partition=longserial (long jobs)  --partition=hugemem (jobs needing lots of memory)\n\n\n\nGet email when job starts, ends, fails,  or all of the above\n-\n--mail-type=START  --mail-type=END  --mail-type=FAIL  --mail-type=ALL\n\n\n\nLet job begin at/after specific time\n-\n--begin=2021-02-01T12:00:00\n\n\n\nLet job begin after other job is done\n-\n--dependency=afterany:123456"
  },
  {
    "objectID": "modules/06-scripts.html",
    "href": "modules/06-scripts.html",
    "title": "Shell Scripting",
    "section": "",
    "text": "Shell scripts (or to be slightly more precise, Bash scripts) enable us to run sets of commands non-interactively. This is especially beneficial or necessary when a set of commands:\nScripts form the basis for analysis pipelines and if we code things cleverly, it should be straightforward to rerun much of our project workflow:"
  },
  {
    "objectID": "modules/06-scripts.html#setup",
    "href": "modules/06-scripts.html#setup",
    "title": "Shell Scripting",
    "section": "1 Setup",
    "text": "1 Setup\n\n\n\n\n\n\nStarting a VS Code session with an active terminal (click here)\n\n\n\n\n\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 4 or more in the box Number of hours\nTo avoid having to switch folders within VS Code, enter /fs/ess/scratch/PAS2250/participants/<your-folder> in the box Working Directory (replace <your-folder> by the actual name of your folder).\nClick Launch.\n\nOn the next page, once the top bar of the box is green and says Runnning, click Connect to VS Code.\nOpen a terminal:    => Terminal => New Terminal.\nIn the terminal, type bash and press Enter.\nType pwd to check you are in /fs/ess/scratch/PAS2250. If not, click    =>   File   =>   Open Folder and enter /fs/ess/scratch/PAS2250/<your-folder> (replace <your-folder> by the actual name of your folder)."
  },
  {
    "objectID": "modules/06-scripts.html#script-header-lines-and-zombie-scripts",
    "href": "modules/06-scripts.html#script-header-lines-and-zombie-scripts",
    "title": "Shell Scripting",
    "section": "2 Script header lines and zombie scripts",
    "text": "2 Script header lines and zombie scripts\n\n2.1 Shebang line\nWe use a so-called “shebang” line as the first line of a script to indicate which language our script uses. More specifically, this line tell the computer where to find the binary (executable) that will run our script.\nSuch a line starts with #!, basically marking it as a special type of comment. After that, we provide the location to the relevant program: in our case Bash, which is located at /bin/bash on Linux and Mac computers.\n#!/bin/bash\nAdding a shebang line is good practice in general, and is necessary when we want to submit our script to OSC’s Slurm queue, which we’ll do tomorrow.\n\n\n2.2 Bash script settings\nAnother line that is good practice to add to your Bash scripts changes some default settings to safer alternatives. Two Bash default settings are bad ideas inside scripts:\nFirst, and as we’ve seen in the previous module, Bash does not complain when you reference a variable that does not exist (in other words, it does not consider that an error).\nIn scripts, this can lead to all sorts of downstream problems, because you very likely tried and failed to do something with an actual variable. Even more problematically, it can lead to potentially very destructive file removal:\n\n# Using a variable, we try to remove some temporary files whose names start with tmp_\ntemp_prefix=\"temp_\"\nrm \"$tmp_prefix\"*     # DON'T TRY THIS!\n\n\n# Using a variable, we try to remove a temporary directory\ntempdir=output/tmp\nrm -rf $tmpdir/*      # DON'T TRY THIS!\n\n\n\n\n\n\n\nThe comments above specified the intent we had. What would have actually happened?\n\n\n\n\n\nIn both examples, there is a similar typo: temp vs. tmp, which means that we are referencing a (likely) non-existent variable.\nIn the first example, rm \"$tmp_prefix\"* would have been interpreted as rm *, because the non-existent variable is simply ignored. Therefore, we would have removed all files in the current working directory.\nIn the second example, along similar lines, rm -rf $tmpdir/* would have been interpreted as rm -rf /*. Horrifyingly, this would attempt to remove the entire filesystem (recall that a leading / in a path is a computer’s root directory).1 (-r makes the removal recursive and -f makes forces removal).\n\n\n\n\nSecond, a Bash script keeps running after encountering errors. That is, if an error is encountered when running line 2 of a script, any remaining lines in the script will nevertheless be executed.\nIn the best case, this is a waste of computer resources, and in worse cases, it can lead to all kinds of unintended consequences. Additionally, if your script prints a lot of output, you might not notice an error somewhere in the middle if it doesn’t produce more errors downstream. But the downstream results from what we at that point might call a “zombie script” may still be completely wrong.\n\nThe following three settings will make your Bash scripts more robust and safer. With these settings, the script terminates, with an appropriate error message, if:\n\nset -u — An unset (non-existent) variable is referenced.\nset -e — Almost any error occurs.\nset -o pipefail — An error occurs in a shell “pipeline” (e.g., sort | uniq).\n\nWe can change all of these settings in one line in a script:\n\nset -u -e -o pipefail     # (For in a script - don't run in the terminal)\n\nOr even more concisely:\n\nset -ueo pipefail         # (For in a script - don't run in the terminal)\n\n\n\n2.3 Our header lines as a rudimentary script\n Let’s go ahead and start a script with the header lines that we have so far discussed.\n\nInside your personal directory within /fs/ess/scratch/PAS2250/participants, make a directory called scripts and one called sandbox (e.g. mkdir scripts sandbox, or use the VS Code menus.\nOpen a new file in the VS Code editor (     =>   File   =>   New File) and save it as printname.sh within the newly created scripts dir.\n\n\n\n\n\n\nShell scripts, including Bash scripts, most commonly have the extension .sh\n\n\n\n\n\n\nType the following lines in that script (please type instead of copy-pasting):\n\n#!/bin/bash\nset -ueo pipefail\n\n# (Note: this is a partial script. Don't enter this directly in your terminal.)\n\n\nAlready now, we could run (execute) the script. One way of doing this is calling the bash command followed by the name of the script2:\n\nbash scripts/printname.sh\n\nDoing this won’t print anything to screen (or file). This makes sense because our script doesn’t have any output, and as we’ve seen before with Bash, no output can be a good sign because it means that no errors were encountered."
  },
  {
    "objectID": "modules/06-scripts.html#command-line-arguments-for-scripts",
    "href": "modules/06-scripts.html#command-line-arguments-for-scripts",
    "title": "Shell Scripting",
    "section": "3 Command-line arguments for scripts",
    "text": "3 Command-line arguments for scripts\n\n3.1 Calling a script with arguments\nWhen you call a script, you can pass it command-line arguments, such as a file to operate on.\nThis is much like when you provide a command like ls with arguments:\n\n# Run ls without arguments:\nls\n\n# Pass 1 filename as an argument to ls:\nls data/sampleA.fastq.gz\n\n# Pass 2 filenames as arguments to ls, separated by spaces:\nls data/sampleA.fastq.gz data/sampleB.fastq.gz\n\nLet’s see what this would look like with our printname.sh script and a fictional script fastqc.sh (to run the FastQC program – we’ll make such a script later):\n\n# Run scripts without any arguments:\nbash fastqc.sh                            # (Fictional script)\nbash scripts/printname.sh\n\n# Run scripts with 1 or 2 arguments:\nbash fastqc.sh data/sampleA.fastq.gz      # 1 argument, a filename\nbash scripts/printname.sh John Doe        # 2 arguments, strings representing names\n\nIn the next section, we’ll see what happens when we pass arguments to a script on the command line (in short: command-line arguments).\n\n\n3.2 Placeholder variables\nInside the script, any command-line arguments are automatically available in placeholder variables.\nA first argument will be assigned to the variable $1, any second argument will be assigned to $2, any third argument will be assigned to $3, and so on.\n\n\n\n\n\n\nIn the calls to fastqc.sh and printname.sh above, what are the placeholder variables and their values?\n\n\n\n\n\nIn bash fastqc.sh data/sampleA.fastq.gz, a single argument, data/sampleA.fastq.gz, is passed to the script, and will be assigned to $1.\nIn bash scripts/printname.sh John Doe, two arguments are passed to the script: the first one (John) will be stored in $1, and the second one (Doe) in $2.\n\n\n\n\n\n\n\n\n\nPlaceholder variables are not automagically used\n\n\n\nArguments passed to a script are merely made available in placeholder variables — unless we explicitly include code in the script to do something with those variables, nothing else happens.\n\n\n Let’s add code to our printname.sh script to “process” any first and last name that are passed to it as command-line arguments. First, our small script will simply echo the placeholder variables, so that we can see what happens. We’ll add two echo commands so that it now reads:\n\n#!/bin/bash\nset -ueo pipefail\n\necho \"First name: $1\"\necho \"Last name: $2\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\n\n\n\nNext, we’ll run the script, passing the arguments John and Doe:\n\nbash scripts/printname.sh John Doe\n\nFirst name: John\nLast name: Doe\n\n\n\nOn Your Own: Command-line arguments\nIn each case below, think about what might happen before you run the script. If you didn’t make a successful predictions, try to figure out what happened instead.\n\nRun the script without passing arguments to it.\nDeactivate (“comment out”) the line with set settings by inserting a # as the first character. Then, run the script again without passing arguments to it.\nDouble-quote John Doe when you run the script, i.e. run bash scripts/printname.sh \"John Doe\"\nRemove the # you inserted in the script in step 2 above.\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\nThe script will error out because we are referencing variables that don’t exist: since we didn’t pass command-line arguments to the script, the $1 and $2 have not been set.\n\n\nbash scripts/printname.sh\n\n\nprintname.sh: line 4: $1: unbound variable\n\n\nThe script will run in its entirety and not throw any errors, because we are now using default Bash settings such that referencing non-existent variables does not throw an error. Of course, no names are printed either, since we didn’t specify any:\n\n\n\n\n\nbash scripts/printname.sh\n\nFirst name: \nLast name: \n\n\nThe set line should read:\n\n#set -ueo pipefail\n\n\nBecause we are quoting \"John Doe\", both names are passed as a single argument and both names end up in $1, the “first name”:\n\n\nbash scripts/printname.sh \"John Doe\"\n\nFirst name: John Doe\nLast name: \n\n\n\n\n\n\n\n\n\n\n\n3.3 Descriptive variable names\nWhile you can use the $1-style placeholder variables throughout your script, I find it very useful to copy them to more descriptively named variables as follows:\n\n#!/bin/bash\nset -ueo pipefail\n\nfirst_name=$1\nlast_name=$2\n  \necho \"First name: $first_name\"\necho \"Last name: $last_name\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\nUsing descriptively named variables in your scripts has several advantages. It will make your script easier to understand for others and for yourself. It will also make it less likely that you make errors in your script in which you use the wrong variable in the wrong place.\n\n\n\n\n\n\nOther variables that are automatically available inside scripts\n\n\n\n\n$0 contains the script’s file name\n$# contains the number of command-line arguments passed\n\n\n\n\nOn Your Own: A script to print a specific line\nWrite a script that prints a specific line (identified by line number) from a file.\n\nSave the script as scripts/printline.sh\nStart with the shebang and set lines\nYour script takes two arguments: a file name ($1) and a line number ($2)\nCopy the $1 and $2 variables to descriptively named variables\nTo print a specific line, think how you might combine head and tail to do this. If you’re at a loss, feel free to check out the solution below.\nTest the script by printing line 4 from data/meta/meta.tsv.\n\n\n\n\n\n\n\nSolution: how to print a specific line number\n\n\n\n\n\nFor example, to print line 4 of data/meta/meta.tsv directly:\n\nhead -n 4 data/meta/meta.tsv | tail -n 1\n\nJust note that in the script, you’ll be using variables instead of the “hardcode values” 4 and data/meta/meta.tsv.\nHow this command works:\n\nhead -n 4 data/meta/meta.tsv will print the first 4 lines of data/meta/meta.tsv\nWe pipe those 4 lines into the tail command\nWe ask tail to just print the last line of its input, which will in this case be line 4 of the original input file.\n\n\n\n\n\n\n\n\n\n\nFull solution\n\n\n\n\n\n\n#!/bin/bash\nset -ueo pipefail\n  \ninput_file=$1\nline_nr=$2\n\nhead -n \"$line_nr\" \"$input_file\" | tail -n 1\n\n\n\n\nTo run the script and make it print the 4th line of meta.tsv:\n\nbash scripts/printline.sh data/meta/meta.tsv 4\n\nSRR7609471  beach   control 3   40982374    78.70"
  },
  {
    "objectID": "modules/06-scripts.html#script-variations-and-improvements",
    "href": "modules/06-scripts.html#script-variations-and-improvements",
    "title": "Shell Scripting",
    "section": "4 Script variations and improvements",
    "text": "4 Script variations and improvements\n\n4.1 A script to serve as a starting point\nWe’ve learned that the head command prints the first lines of a file, whereas the tail command prints the last lines. Sometimes it’s nice to be able to quickly see both ends of a file, so let’s write a little script that can do that for us, as a starting point for the next few modifications.\nOpen a new file, save it as scripts/headtail.sh, and add the following code to it:\n\n#!/bin/bash\nset -ueo pipefail\n\ninput_file=$1\n\nhead -n 2 \"$input_file\"\necho \"---\"\ntail -n 2 \"$input_file\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\n\n\n\nNext, let’s run our headtail.sh script:\n\nbash scripts/headtail.sh data/meta/meta.tsv\n\naccession   location    treatment   replicate   nreads_raw  pct_mapped\nSRR7609473  beach   control 1   45285752    76.01\n---\nSRR7609467  inland  treatment   2   47303936    79.25\nSRR7609474  inland  treatment   3   55728624    78.80\n\n\n\n\n4.2 Redirecting output to a file\nSo far, the output of our scripts was printed to screen, e.g.:\n\nIn printnames.sh, we simply echo’d, inside sentences, the arguments passed to the script.\nIn headtail.sh, we printed the first and last few lines of a file.\n\nAll this output was printed to screen because that is the default output mode of Unix commands, and this works the same way regardless of whether those commands are run directly on the command line, or are run inside a script.\nAlong those same lines, we have already learned that we can “redirect” output to a file using > (write/overwrite) and >> (append) when we run shell commands — and this, too, works exactly the same way inside a script.\n\nWhen working with genomics data, we commonly have files as input, and new/modified files as output. So let’s practice with this and modify our headtail.sh script so that it writes output to a file.\nWe’ll make the following changes:\n\nWe will have the script accept a second argument: the output file name. Of course, we could also simply write the output to a predefined (“hardcoded”) file name such as out.txt, but in general, it’s better practice to keep this flexible via an argument.\nWe will redirect the output of our head, echo, and tail commands to the output file. We’ll have to append (>>) in the last two cases.\n\n\n#!/bin/bash\nset -ueo pipefail\n\ninput_file=$1\noutput_file=$2\n\nhead -n 2 \"$input_file\" > \"$output_file\"\necho \"---\" >> \"$output_file\"\ntail -n 2 \"$input_file\" >> \"$output_file\"\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\n\n\n\nNow we run the script again, this time also passing the name of an output file:\n\nbash scripts/headtail.sh data/meta/meta.tsv sandbox/samples_headtail.txt\n\nThe script will no longer print any output to screen, and our output should instead be in sandbox/samples_headtail.txt:\n\n# Check that the file exists and was just modified:\nls -lh sandbox/samples_headtail.txt\n\n-rw-rw-r-- 1 jelmer jelmer 197 Aug 17 16:51 sandbox/samples_headtail.txt\n\n\n\n# Print the contents of the file to screen\ncat sandbox/samples_headtail.txt\n\naccession   location    treatment   replicate   nreads_raw  pct_mapped\nSRR7609473  beach   control 1   45285752    76.01\n---\nSRR7609467  inland  treatment   2   47303936    79.25\nSRR7609474  inland  treatment   3   55728624    78.80\n\n\n\n\n4.3 Report what’s happening\nIt is often useful to have your scripts “report” or “log” what is going on. Let’s keep thinking about a script that has file(s) as the main output, but instead of having no output printed to screen at all, we’ll print some logging output to screen. For instance: what is the date and time, which arguments were passed to the script, what are the output files, and perhaps even summaries of the output. All of this can help with troubleshooting.3\nLet’s try this with our headtail.sh script.\n\n#!/bin/bash\nset -ueo pipefail\n\n## Process command-line arguments\ninput_file=$1\noutput_file=$2\n\n## Initial logging \necho \"Starting script $0\"           # Print name of script\ndate                                # Print date & time\necho \"Input file:   $input_file\"\necho \"Output file:  $output_file\" \necho                                # Print empty line to separate initial & final logging\n\n## Print the first and last two lines to a separate file\nhead -n 2 \"$input_file\" > \"$output_file\"\necho \"---\" >> \"$output_file\"\ntail -n 2 \"$input_file\" >> \"$output_file\"\n\n## Final logging\necho \"Listing the output file:\"\nls -lh \"$output_file\"\necho \"Done with script $0\"\ndate\n\n# (Note: this is a script. Don't enter this directly in your terminal.)\n\nA couple of notes about the lines that were added to the script above:\n\nPrinting the date at the end of the script as well will allow you to check for how long the script ran, which can be informative for longer-running scripts.\nPrinting the input and output files (and the command-line arguments more generally) can be particularly useful for troubleshooting\nPrinting a “marker line” like Done with script, indicating that the end of the script was reached, is handy because due to our set settings, seeing this line printed means that no errors were encountered.\nBecause our script grew so much, I also added some comment headers like “Initial logging” to make the script easier to read, and such comments can be made more extensive to really explain what is being done.\n\n\n\n\nLet’s run the script again:\n\nbash scripts/headtail.sh data/meta/meta.tsv sandbox/tmp.txt\n\nStarting script scripts/headtail.sh\nWed Aug 17 04:51:54 PM CEST 2022\nInput file:   data/meta/meta.tsv\nOutput file:  sandbox/tmp.txt\n\nListing the output file:\n-rw-rw-r-- 1 jelmer jelmer 197 Aug 17 16:51 sandbox/tmp.txt\nDone with script scripts/headtail.sh\nWed Aug 17 04:51:54 PM CEST 2022\n\n\nThe script printed some details for the output file, but not its contents (that would have worked here, but is usually not sensible when working with genomics data). Let’s take a look, though, to make sure the script worked:\n\ncat sandbox/tmp.txt\n\naccession   location    treatment   replicate   nreads_raw  pct_mapped\nSRR7609473  beach   control 1   45285752    76.01\n---\nSRR7609467  inland  treatment   2   47303936    79.25\nSRR7609474  inland  treatment   3   55728624    78.80\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe reporting (echo-ing) may have started to seem silly for our litle script, but fairly extensive reporting (as well as testing, which is outside the scope of this workshop) can be very useful — and will be eventually a time-saver.\nThis is especially true for long-running scripts, or scripts that you often reuse and perhaps share with others."
  },
  {
    "objectID": "modules/09-examples.html#tba",
    "href": "modules/09-examples.html#tba",
    "title": "Batch Jobs in Practice",
    "section": "2 TBA",
    "text": "2 TBA\n\n\n\n\n\n\nKeyboard shortcut to run shell commands from the editor\n\n\n\nTo add a keyboard shortcut that will send code selected in the editor pane to the terminal (such that you don’t have to copy and paste):\n\nClick the      (bottom-left) => Keyboard Shortcuts.\nFind Terminal: Run Selected Text in Active Terminal, click on it, then add a shortcut, e.g. Ctrl+Enter."
  },
  {
    "objectID": "modules/02-osc.html#tba",
    "href": "modules/02-osc.html#tba",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "1 TBA",
    "text": "1 TBA"
  },
  {
    "objectID": "modules/04-shell.html#tba",
    "href": "modules/04-shell.html#tba",
    "title": "The Unix Shell",
    "section": "1 TBA",
    "text": "1 TBA"
  },
  {
    "objectID": "modules/07-software.html",
    "href": "modules/07-software.html",
    "title": "Using Software at OSC",
    "section": "",
    "text": "So far, we have only used commands that are available in any Unix shell. But to actually analyze genomics data sets, we also need to use specialized bioinformatics software.\nMost software that is already installed at OSC must nevertheless be “loaded” (“activated”) before we can use it; and if our software of choice is not installed, we have to do so ourselves. We will cover those topics in this module."
  },
  {
    "objectID": "modules/07-software.html#setup",
    "href": "modules/07-software.html#setup",
    "title": "Using Software at OSC",
    "section": "1 Setup",
    "text": "1 Setup\n\n\n\n\n\n\nStarting a VS Code session with an active terminal (click here)\n\n\n\n\n\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 4 or more in the box Number of hours\nTo avoid having to switch folders within VS Code, enter /fs/ess/scratch/PAS2250/participants/<your-folder> in the box Working Directory (replace <your-folder> by the actual name of your folder).\nClick Launch.\n\nOn the next page, once the top bar of the box is green and says Runnning, click Connect to VS Code.\nOpen a terminal:    => Terminal => New Terminal.\nIn the terminal, type bash and press Enter.\nType pwd to check you are in /fs/ess/scratch/PAS2250. If not, click    =>   File   =>   Open Folder and enter /fs/ess/scratch/PAS2250/<your-folder> (replace <your-folder> by the actual name of your folder)."
  },
  {
    "objectID": "modules/07-software.html#running-command-line-programs",
    "href": "modules/07-software.html#running-command-line-programs",
    "title": "Using Software at OSC",
    "section": "2 Running command-line programs",
    "text": "2 Running command-line programs\nAs pointed out in the introduction to the workshop, bioinformatics software (programs) that we use to analyze genomic data are typically run from the command line. That is, they have “command-line interfaces” (CLIs) rather than “graphical user interfaces” (GUIs), and are run using commands that are structurally very similar to how we’ve been using basic Unix commands.\nFor instance, we can run the program FastQC as follows, instructing it to process the FASTQ file sampleA.fastq.gz with default options:\n\nfastqc sampleA.fastq.gz       # Don't run\n\nSo, what we have learned in the previous modules can easily be applied to run command-line programs. But, we first need to load and/or install these programs.\n\n\n\n\n\n\nRunning inside a script or interactively\n\n\n\nLike any other command, we could in principle run the line of code above either in our interactive shell or from inside a script. In practice, it is better to do this in a script, especially at OSC, because:\n\nSuch programs typically take a while to run\nWe are not supposed to run processes that use significant resources on login nodes\nWe can run the same script simultaneously for different input files."
  },
  {
    "objectID": "modules/07-software.html#software-at-osc-with-lmod",
    "href": "modules/07-software.html#software-at-osc-with-lmod",
    "title": "Using Software at OSC",
    "section": "3 Software at OSC with Lmod",
    "text": "3 Software at OSC with Lmod\nOSC administrators manage software with the Lmod system of software modules. For us users, this means that even though a lot of software is installed, most of it can only be used after we explicitly load it.\n(That may seem like a drag, but on the upside, this practice enables the use of different versions of the same software, and of mutually incompatible software on a single system.)\n\n3.1 Checking for available software\nThe OSC website has a list of software that has been installed at OSC. You can also search for available software in the shell:\n\nmodule spider lists modules that are installed.\nmodule avail lists modules that can be directly loaded, given the current environment (i.e., depending on which other software has been loaded).\n\nSimply running module spider or module avail would spit out complete lists — more usefully, we can provide search terms as arguments to these commands:\n\nmodule spider python\n\n\n\n\n\npython:\n\n\n\n Versions:\n    python/2.7-conda5.2\n    python/3.6-conda5.2\n    python/3.7-2019.10\n\n\nmodule avail python\n\n\npython/2.7-conda5.2         python/3.6-conda5.2 (D)         python/3.7-2019.10\n\n\n\n\n\n\n\n(D) = default version\n\n\n\nThe (D) in the output above marks the default version of the program; that is, the version of the program that would be loaded if we don’t specify a version (see examples below).\n\n\n\n\n3.2 Loading software\nAll other Lmod software functionality is also accessed using module “subcommands” (we call module the command and e.g. spider the subcommand). For instance, to load and unload software:\n\n# Load a module:\nmodule load python              # Load the default version\nmodule load python/3.7-2019.10  # Load a specific version\n\n# Unload a module:\nmodule unload python\n\nTo check which modules have been loaded (the list will include modules that have been loaded automatically):\n\nmodule list\n\n\nCurrently Loaded Modules:\n    1) xalt/latest       2) gcc-compatibility/8.4.0       3) intel/19.0.5       4) mvapich2/2.3.3       5) modules/sp2020\n\n\n\n3.3 A practical example\nLet’s load a very commonly used bioinformatics program that we will also use in examples later on: FastQC. FastQC performs quality control (hence: “QC”) on FASTQ files.\nFirst, let’s test that we indeed cannot currently use fastqc by running fastqc with the --help flag:\n\nfastqc --help\n\n\nbash: fastqc: command not found\n\n\n\n\n\n\n\nHelp!\n\n\n\nA solid majority of command-line programs can be run with with a --help (and/or -h) flag, and this is often a good thing to try first, since it will tell use whether we can use the program, and if we can, we immediately get some usage information.\n\n\nNext, let’s check whether FastQC is available at OSC, and if so, in which versions:\n\nmodule avail fastqc\n\n\nfastqc/0.11.8\n\nThere is only one version available (0.11.8), which means that module load fastqc and module load fastqc/0.11.8 would each load that same version.\n\n\n\n\n\n\nWhat might still be an argument to specify the version when we load FastQC?\n\n\n\n\n\nWhen we use it inside a script:\n\nThis would ensure that when we run the same script a year later, the same version would be used (assuming it hasn’t been removed) — otherwise, it’s possible a newer version would has been installed in the meantime, which might produce different results.\nIt will make it easy to see which version we used, which is something we typically report in papers.\n\n\n\n\nLet’s load the FastQC module:\n\nmodule load fastqc/0.11.8\n\nAfter we have loaded the module, we can retry our --help attempt:\n\nfastqc --help\n\n\n        FastQC - A high throughput sequence QC analysis tool\nSYNOPSIS\n    fastqc seqfile1 seqfile2 .. seqfileN\n\nfastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] \n       [-c contaminant file] seqfile1 .. seqfileN\n       \n[…and much more]"
  },
  {
    "objectID": "modules/07-software.html#when-software-isnt-installed-at-osc",
    "href": "modules/07-software.html#when-software-isnt-installed-at-osc",
    "title": "Using Software at OSC",
    "section": "4 When software isn’t installed at OSC",
    "text": "4 When software isn’t installed at OSC\nIt’s not too uncommon that software you need for your project is not installed at OSC, or that you need a more recent version of the software than those available. The main options available to you in such a case are to:\n\n“Manually” install the software, which in the best case involves downloading a directly functioning binary (executable), but more commonly requires you to “compile” (build) the program. This is sometimes straightforward but can also become extremely tricky, especially at OSC where you don’t have “administrator privileges”1 at OSC and will often have difficulties with “dependencies”2.\nSend an email to OSC Help. They might be able to help you with your installation, or in case of commonly used software, might be willing to perform a system-wide installation (that is, making it available through module).\nUse conda, which creates software environments that are activated like in the module system.\nUse Apptainer / Singularity “containers”. Containers are software environments that are more self-contained, akin to mini virtual machines.\n\nConda and containers are useful not only at OSC, where they bypass issues with dependencies and administrator privileges, but more generally, for reproducible and portable software environments. They also allow you to easily maintain distinct “environments”, each with a different version of the same software, or with mutually incompatible software.\nWe will teach conda here because it is easier to learn and use than containers, and because nearly all open-source bioinformatics software is available as a conda package.\n\n\n\n\n\n\nWhen to use containers instead of Conda\n\n\n\n\nIf you need to use software that requires a different Operating System (OS) or OS version than the one at OSC.\nIf you want or require even greater reproducibility and portability to create an isolated environment that can be exported and used anywhere."
  },
  {
    "objectID": "modules/07-software.html#using-conda",
    "href": "modules/07-software.html#using-conda",
    "title": "Using Software at OSC",
    "section": "5 Using conda",
    "text": "5 Using conda\nConda creates so-called environments in which you can install one or more software packages. As mentioned above, these environments are activated and deactivated in a similar manner as with the Lmod system – but the key difference is that we can create and manage these environments ourselves.\n\n\n\n\n\n\nWhat’s in an environment?\n\n\n\nOne environment per software, or one per project\nNote that even when you install a single program, many things are usually installed: dependencies\n\n\n\n5.1 Loading the (mini)conda module\nWhile it is also fairly straightforward to install conda for yourself 3, we will use OSC’s system-wide installation of conda in this workshop. Therefore, we first need to use a module load command to make it available:\n\n# (The most common installation of conda is actually called \"miniconda\")\nmodule load miniconda3\n\n\n\n5.2 One-time conda configuration\nWe will also do some one-time configuration, which will set the conda “channels” (basically, software repositories) that we want to use when we install software. This config also includes setting relative priorities among the channels, since one software package may be available from multiple channels.\nLike with module commands, conda commands consist of two parts, the conda command itself and a subcommand, such as config:\n\nconda config --add channels defaults     # Added first => lowest priority\nconda config --add channels bioconda\nconda config --add channels conda-forge  # Added last => highest priority\n\nLet’s check whether this configuration step worked:\n\nconda config --get channels\n\n\n\n5.3 Creating an environment for cutadapt\nTo practice using conda, we will now create a conda environment with the program cutadapt installed.\ncutadapt is a commonly used program to remove adapters or primers from sequence reads in FASTQ files; in particular, it is ubiquitous for primer removal in (e.g. 16S rRNA) microbiome metabarcoding studies. But there is no Lmod module on OSC for it, so if we want to use, our best option is to resort to conda.\nHere is the command to create a new environment and install cutadapt into that environment:\n\nconda create -y -n cutadapt -c bioconda cutadapt\n\nLet’s break the above command down:\n\ncreate is the conda subcommand to create a new environment.\n-y is a flag that prevents us from being asked to confirm installation.\nFollowing the -n option, we can specify the name of the environment, so -n cutadapt means that we want our environment to be called cutadapt. We can use whatever name we like for the environment, but of course a descriptive yet concise name is a good idea. Since we are making a single-program environment, it makes sense to simply name it after the program.\nFollowing the -c option, we can specify a channel from which we want to install, so -c bioconda indicates we want to use the bioconda channel. (Given that we’ve done some config above, this is not always necessary, but it can be good to be explicit.)\nThe cutadapt at the end of the line simply tells conda to install the package of that name. This is a “positional” argument to the command (note that there’s no option like -s before it): we put any software package(s) we want to install at the end of the command.\n\n\n\n\n\n\n\nSpecifying a version\n\n\n\nIf we want to be explicit about the version we want to install, we can add the version after = following the package name. We do that below, and we also include the version in the environment name:\n\nconda create -y -n cutadapt-4.1 -c bioconda cutadapt=4.1\n\n Let’s run the command above and see if we can install cutadapt\n\n\n\n\n5.4 Creating an environment for any program\nMinor variations on the conda create command above can be used to install almost any program for which is conda package is available.\nHowever, you may be wondering how we would know:\n\nWhether the program is available and what its conda package’s name is\nWhich versions are available\nWhich conda channel we should use\n\nADD…\n\n\n5.5 Activating conda environments\nWhereas we use the term “load” for Lmod modules, we use “activate” to the same effect for conda environments.\nOddly enough, the most foolproof way to activate a conda environment is to use source activate rather than the expected conda activate — for instance:\n\nsource activate cutadapt-4.1\n\n\n(cutadapt-4.1) [jelmer@pitzer-login03 PAS2250]$\n\n\n\n\n\n\n\nEnvironment indicator\n\n\n\nWhen we have an active conda environment, its name is conveniently displayed in our prompt, as depicted above.\n\n\nAfter we have activated the cutadapt environment, we should be able to actually use the program. To test this, we’ll again simply run it with a --help option:\n\ncutadapt --help\n\n\n\n\n5.6 Lines to add to your scripts\nWhile you’ll typically want to do installation interactively and only need to do to it once (see note below), you should always include the necessary code to load/activate your programs in your shell scripts.\nWhen your program is available as an Lmod module, this simply entails a module load call — e.g., for fastqc:\n\n#!/bin/bash\nset -ueo pipefail\n\nmodule load fastqc\n\nWhen your program is available in a conda environment, this entails a module load command to load conda itself, followed by a source activate command to load the relevant conda environment:\n\n#!/bin/bash\n\n## Load software\nmodule load miniconda3\nsource activate cutadapt-4.1\n\n## Strict/safe Bash settings \nset -ueo pipefail\n\n\n\n\n\n\n\nWarning\n\n\n\nWe’ve moved the set -ueo pipefail line below the source activate command, because the conda activation procedure may otherwise throw “unbound variable” errors.\n\n\n\n\n\n\n\n\nInstall once, load always\n\n\n\n\nProvided you don’t need to switch versions, you only need to install a program once. This is true also at OSC and also when using conda (your environments won’t disappear unless you delete them).\nIn every single “session” that you want to use a program via an Lmod module or conda environment, you …"
  },
  {
    "objectID": "modules/07-software.html#addendum-a-few-other-useful-conda-commands",
    "href": "modules/07-software.html#addendum-a-few-other-useful-conda-commands",
    "title": "Using Software at OSC",
    "section": "7 Addendum: a few other useful conda commands",
    "text": "7 Addendum: a few other useful conda commands\n\nDeactivate the currently active conda environment:\n\nconda deactivate   \n\nActivate one environment and then “stack” an additional environment (a regular conda activate command would switch environments):\n\nsource activate cutadapt         # Now, the env \"cutadapt\" is active\nconda activate --stack multiqc   # Now, both \"cutadapt\" and \"multiqc\" are active\n\nRemove an environment entirely:\n\nconda env remove -n cutadapt\n\nList all your conda environments:\n\nconda env list\n\nList all packages (programs) installed in an environment:\n\nconda list -n cutadapt"
  },
  {
    "objectID": "modules/01-intro.html#what-you-will-learn",
    "href": "modules/01-intro.html#what-you-will-learn",
    "title": "Introduction to the Workshop",
    "section": "What you will learn",
    "text": "What you will learn\nThe focus of the workshop is on building some general (foundational) skills for analyzing genomics data — specifically, for doing so with command-line programs at the Ohio Supercomputer Center (OSC).\nCommand-line programs are preferred for many of the steps to analyze genomic sequencing data. Because such datasets tend to contain a lot of data, it is also preferable to run your analyses not on a laptop or desktop, but at a supercomputer like OSC.\nThese realities mean that in the field of genomics, you need the following set of skills that you may not have been taught previously:\n\nHaving a basic understanding of a supercomputer\n\nAnd being able to:\n\nUse the Unix shell (work in a terminal)\nWrite small shell scripts that run command-line programs\nSubmit these scripts to a supercomputer’s “queue” and monitor and manage the resulting “batch jobs”\nActivate and probably install software in a Linux environment where you don’t have admin rights\n\nWe will teach the basics of these skills during this workshop!\n\n\n\n\n\n\nWhat you won’t learn\n\n\n\nIt may be useful to point out that we will not teach you much, if anything, about:\n\nDetails of genomic data file types — except, briefly, FASTQ\nDetails of specific (genomic) analyses\nMaking biological inferences from your data"
  },
  {
    "objectID": "modules/01-intro.html#mechanics-of-a-hybrid-workshop",
    "href": "modules/01-intro.html#mechanics-of-a-hybrid-workshop",
    "title": "Introduction to the Workshop",
    "section": "Mechanics of a hybrid workshop",
    "text": "Mechanics of a hybrid workshop\nWe have a slightly complicated set up with participants in-person in Wooster with an instructor (Jelmer now via Zoom), in-person in Columbus with an instructor, and directly via Zoom. Some notes:\n\nThis website has all the material that we will go through during each of the modules! See the links in the schedule as well as in the top bar menus to access it.\nIn-person participants don’t need to connect to the Zoom call, since Zoom will be broadcast on the large screen (but you can of course connect if you can better see the instructor’s screen that way).\nBecause we’re not all on Zoom, we’ll use this Google Doc to share links, inpromptu code that is not on the website, and non-urgent questions.\nWhenever you have a question, please feel free to interrupt and speak up, both in-person and on Zoom. Because we will mute the in-person rooms on the Zoom call by default, signal to Mike (Columbus) or Menuka (Wooster) tat you have a question, who will unmute.\nOnly if your question is not urgent and you don’t want to interrupt the flow, put it in the Google Doc or ask about it during a break."
  },
  {
    "objectID": "modules/01-intro.html#personal-introductions",
    "href": "modules/01-intro.html#personal-introductions",
    "title": "Introduction to the Workshop",
    "section": "Personal introductions",
    "text": "Personal introductions\n\nInstructors\n\nJelmer Poelstra, Molecular and Cellular Imaging Center (MCIC), Wooster\nMike Sovic, Center for Applied Plant Sciences (CAPS), Wooster\n\nA special mention goes out to Menuka Bhandari who is managing the Selby in-person room. Menuka is also knowledgeable about the workshop material and may be able to help you if you need/prefer someone in the room itself for a certain question.\n\n\nParticipants\nPlease very briefly introduce yourself – include your position, department, and why you wanted to go to this workshop."
  },
  {
    "objectID": "modules/05-vars-loops.html",
    "href": "modules/05-vars-loops.html",
    "title": "Variables and Loops",
    "section": "",
    "text": "In this module, we will cover two topics that are good to know about before you start writing and running shell scripts:"
  },
  {
    "objectID": "modules/05-vars-loops.html#setup",
    "href": "modules/05-vars-loops.html#setup",
    "title": "Variables and Loops",
    "section": "1 Setup",
    "text": "1 Setup\nStarting a VS Code session with an active terminal:\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 4 or more in the box Number of hours\nTo avoid having to switch folders within VS Code, enter /fs/ess/scratch/PAS2250/participants/<your-folder> in the box Working Directory (replace <your-folder> by the actual name of your folder).\nClick Launch.\n\nOn the next page, once the top bar of the box is green and says Runnning, click Connect to VS Code.\nOpen a terminal:    => Terminal => New Terminal.\nIn the terminal, type bash and press Enter.\nType pwd to check you are in /fs/ess/scratch/PAS2250. If not, click    =>   File   =>   Open Folder and enter /fs/ess/scratch/PAS2250/<your-folder> (replace <your-folder> by the actual name of your folder)."
  },
  {
    "objectID": "modules/05-vars-loops.html#variables",
    "href": "modules/05-vars-loops.html#variables",
    "title": "Variables and Loops",
    "section": "2 Variables",
    "text": "2 Variables\nIn programming, we use variables for things that:\n\nWe refer to repeatedly and/or\nAre subject to change.\n\nThese tend to be settings like the paths to input and output files, and parameter values for programs.\n\n2.1 Assigning and referencing variables\nTo assign a value to a variable in Bash (in short: to assign a variable), use the syntax variable=value. For example:\n\n# Assign the value \"low\" to the variable \"treatment\":\ntreatment=low\n\n# Assign the value \"200\" to the variable \"nlines\":\nnlines=200\n\n\n\n\n\n\n\nSpace-sensitive\n\n\n\nBe aware that there can be no spaces around the equals sign (=)!\n\n\nTo reference a variable (i.e., to access its value), you need to put a dollar sign $ in front of its name. We’ll use the echo command to review the values that our variables contain:\n\necho $treatment\n\n\n\nlow\n\n\n\necho $nlines\n\n\n\n200\n\n\nConveniently, we can directly use variables in lots of contexts, as if we had instead typed their values:\n\nls_options=\"-lh\"\n\nls $ls_options data/meta\n\ntotal 4.0K\n-rw-r--r-- 1 jelmer jelmer 583 Aug 16 10:36 meta.tsv\n\n\n\ninput_file=data/fastq/SRR7609467.fastq.gz\n\nls -lh $input_file \n\n-rw-r--r-- 1 jelmer jelmer 8.3M Aug 16 13:45 data/fastq/SRR7609467.fastq.gz\n\n\n\n\n2.2 Rules for naming variables\nVariable names:\n\nCan contain letters, numbers, and underscores\nCannot contain spaces, periods, or other special symbols\nCannot start with a number\n\n\n\n2.3 Command substitution\nIf you want to store the result of a command in a variable, you can use a construct called “command substitution” by wrapping the command inside $():\n\n# (date +%F will return the date in YYYY-MM-DD format)\ntoday=$(date +%F)\n\n## Let's check:\necho $today\n\n2022-08-17\n\n\n\n# Create a file with our $today variable:\ntouch README_$today.txt\n\n# Check the name of our newly created file:\nls README_*\n\n\n\nREADME_2022-08-17.txt\n\n\n\n# Define a filename that we'll use in the next few commands:\ninput_file=data/meta/meta.tsv\n\n# `wc -l` will count the number of lines\n# Using `<` (input redirection) is a trick to avoid the filename from being printed \nnlines=$(wc -l < $input_file)\n\n# We can directly use the variables in our quoted echo statement:\necho \"The file $input_file has $nlines lines\"\n\nThe file data/meta/meta.tsv has 13 lines\n\n\nAmong many other uses, command substitution is handy when you want your script to report some results, or when a next step in the script depends on a previous result.\n\n\n2.4 Environment variables\nThere are also predefined variables in the Unix shell: that is, variables that exist in your environment by default. These so-called “environment variables” are always spelled in all-caps:\n\n# Environment variable $USER contains your user name \necho $USER\n\njelmer\n\n\n\n# Environment variable $HOME contains the path to your home directory\necho $HOME\n\n\n/users/PAS0471/jelmer\n\nEnvironment variables can provide useful information. We’ll see them again when we talk about the Slurm compute job scheduler.\n\n\n2.5 Quoting variables\nAbove, we learned that a variable name cannot contain spaces. But what happens if our variable’s value contains spaces?\nFirst off, when we try to assign the variable without using quotes, we get an error:\n\ntoday=Thu, Aug 18\n\n\nAug: command not found\n\n\n\n\n\n\n\nWhy do you think we got this error?\n\n\n\n\n\nBash tried assign everything up to the first space (i.e., Thu,) to today. After that, since we used a space, it assumed the next word (Aug) was another command.\n\n\n\nBut it works when we quote (with double quotes, \"...\") the entire string that makes up the value:\n\ntoday=\"Thu, Aug 18\"\necho $today\n\nThu, Aug 18\n\n\nNow, let’s try to reference this variable in another context:\n\ntouch README2_$today.txt\nls README2_*\n\n\n\nREADME2_Thu,\n\n\n\n\n\n\n\n\nWhat went wrong here? How many files were created?\n\n\n\n\n\nThe shell performed so-called field splitting using a space as a separator, splitting the value into three separate units – as a result, three files were created: README2_Thu, (listed above), as well as Aug and 18.txt.\nThe following code will list all these three files:\n\n# `ls -t` will sort by last-modified date, and `head -n 3` prints the top 3\n# Therefore, this will print the last 3 files that were created/modified\nls -t | head -n 3\n\n18.txt\nAug\nREADME2_Thu,\n\n\n\n\n\nLike with assignment, our problems can be avoided by quoting the variable when we reference it:\n\ntouch README3_\"$today\".txt\nls README3_*\n\n\n\nREADME3_Thu, Aug 18.txt\n\n\n\nAnother issue we can run into when we don’t quote variables is that we can’t explicitly define where a variable name ends within a longer string of text:\n\necho README_$today_final.txt\n\n\n\nREADME_.txt\n\n\n\n\n\n\n\n\nWhat went wrong here? (Hint: check the coloring highlighting above)\n\n\n\n\n\n\nFollowing a $, the shell will stop interpreting characters as being part of the variable name only when it encounters a character that cannot be part of a variable name, such as a space or a period.\nSince variable names can contain underscores, it will look for the variable $today_final, which does not exist.\nImportantly, the shell does not error out when you reference a non-existing variable – it basically ignores it, such that README_$today_final.txt becomes README_.txt, as if we hadn’t referenced any variable.\n\n\n\n\nQuoting solves this issue, too:\n\necho README_\"$today\"_final.txt\n\n\n\nREADME_Thu, Aug 18_final.txt\n\n\n\nAll in all, it is good practice to quote variables when you reference them: it never hurts, and avoids unexpected surprises.\n\n\n\n\n\n\nBonus: More on quoting – and double vs. single quotes\n\n\n\n\n\nBy double-quoting a variable, we are essentially escaping (or “turning off”) the default special meaning of the space as a separator, and are asking the shell to interpret it as a literal space.\nSimilarly, we are escaping other “special characters”, such as globbing wildcards, with double quotes. Compare:\n\necho *\n\n18.txt Aug data headtail.sh line.sh printname.sh README_2022-08-17.txt README2_Thu, README3_Thu, Aug 18.txt scripts tmp.txt\n\n\n\necho \"*\"\n\n*\n\n\nHowever, as we also saw above, double quotes do not turn off the special meaning of $ (i.e., denoting a string as a variable):\n\necho \"$today\"\n\n\n\nThu, Aug 18\n\n\n…but single quotes will:\n\necho '$today'\n\n$today"
  },
  {
    "objectID": "modules/05-vars-loops.html#for-loops",
    "href": "modules/05-vars-loops.html#for-loops",
    "title": "Variables and Loops",
    "section": "3 For loops",
    "text": "3 For loops\nLoops are a universal element of programming languages, and are extremely useful to repeat operations, such as when you want to run the same script or command for multiple files.\nHere, we’ll only cover what is by far the most common type of loop: the for loop.\nfor loops iterate over a collection, such as a list of files: that is, they allow you to perform one or more actions for each element in the collection, one element at a time.\n\n3.1 for loop syntax and mechanics\nLet’s see a first example, where our collection is just a very short list of numbers (1, 2, and 3):\n\nfor a_number in 1 2 3; do\n    echo \"In this iteration of the loop, the number is $a_number\"\n    echo \"--------\"\ndone\n\nIn this iteration of the loop, the number is 1\n--------\nIn this iteration of the loop, the number is 2\n--------\nIn this iteration of the loop, the number is 3\n--------\n\n\nfor loops contain the following mandatory keywords:\n\n\n\n\n\n\n\nKeyword\nPurpose\n\n\n\n\nfor\nAfter for, we set the variable name\n\n\nin\nAfter in, we specify the collection we are looping over\n\n\ndo\nAfter do, we have one ore more lines specifying what to do with each item\n\n\ndone\nTells the shell we are done with the loop\n\n\n\n\n\n\n\n\n\nWhy the semicolon ; before do?\n\n\n\n\n\nA semicolon separates two commands written on a single line – for instance, instead of:\n\nmkdir results\ncd results\n\n…you could equivalently type:\n\nmkdir results; cd results\n\nThe ; in the for loop syntax has the same function, and as such, an alternative way to format a for loop is:\n\nfor a_number in 1 2 3\ndo\n    echo \"In this iteration of the loop, the number is $a_number\"\ndone\n\nBut that’s one line longer and a bit awkwardly asymmetric.\n\n\n\nIt is important to realize that the loop runs sequentially for each item in the collection, and will therefore run as many times as there are items in the collection.\nThe following example, where we let the computer sleep for 1 second before printing the date and time with the date command, demonstrates that the loop is being executed sequentially:\n\nfor a_number in 1 2 3; do\n    echo \"In this iteration of the loop, the number is $a_number\"\n    #sleep 1s          # Let the computer sleep for 1 second\n    date              # Print the date and time\n    echo \"--------\"\ndone\n\nIn this iteration of the loop, the number is 1\nWed Aug 17 04:18:16 PM CEST 2022\n--------\nIn this iteration of the loop, the number is 2\nWed Aug 17 04:18:16 PM CEST 2022\n--------\nIn this iteration of the loop, the number is 3\nWed Aug 17 04:18:16 PM CEST 2022\n--------\n\n\nThe aspect that is perhaps most difficult to understand is that in each iteration of the loop, one element in the collection (in the example above, either 1, 2, or 3) is being assigned to the variable specified after for (in the example above, a_number).\nWhen we specify the collection “manually”, like we did above with numbers, we separate the elements by a space, as this example also shows:\n\nfor mushroom in morel destroying_angel eyelash_cup; do\n    echo \"$mushroom is an Ohio mushroom\"\ndone\n\nmorel is an Ohio mushroom\ndestroying_angel is an Ohio mushroom\neyelash_cup is an Ohio mushroom\n\n\n\n\n3.2 Looping over files with globbing\nIn practice, we rarely manually list the collection of items we want to loop over. Instead, we commonly loop over files directly using globbing:\n\n# We make sure we only select gzipped FASTQ files using the `*fastq.gz` glob\nfor fastq_file in data/raw/*fastq.gz; do\n    echo \"File $fastq_file has $(wc -l < $fastq_file) lines.\"\n    # More processing...\ndone\n\nThis technique is extremely useful, and I use it all the time. Take a moment to realize that we’re not doing a separate ls and storing the results: as mentioned, we can directly use a globbing pattern to select our files.\nIf needed, you can use your globbing / wild card skills to narrow down the file selection:\n\n# Perhaps we only want to select R1 files (forward reads): \nfor fastq_file in data/raw/*R1*fastq.gz; do\n    # Some file processing...\ndone\n\n# Or only filenames starting with A or B:\nfor fastq_file in data/raw/[AB]*fastq.gz; do\n    # Some file processing...\ndone\n\n\n\n\n\n\n\nBonus: Alternatives to looping with a glob\n\n\n\n\n\nWith genomics data, the routine of looping over an entire directory of files, or selections made with simple globbing patterns, should serve you very well.\nBut in some cases, you may want to iterate only over a specific list of filenames (or partial filenames such as sample IDs) that represent a complex selection.\n\nIf this is a short list, you could directly specify it in the loop:\n\nfor sample in A1 B6 D3; do\n    R1=data/fastq/\"$sample\"_R1.fastq.gz\n    R2=data/fastq/\"$sample\"_R2.fastq.gz\n    # Some file processing...\ndone\n\nIf it is a longer list, you could create a simple text file with one line per sample ID / filename, and use command substitution as follows:\n\nfor fastq_file in $(cat file_of_filenames.txt); do\n    # Some file processing...\ndone\n\n\nIn cases like this, Bash arrays (basically, variables that consist of multiple values, like a vector in R) or while loops may provide more elegant solutions, but those are outside the scope of this introduction."
  },
  {
    "objectID": "modules/03-vscode.html",
    "href": "modules/03-vscode.html",
    "title": "The VS Code Text Editor",
    "section": "",
    "text": "In this module, we will learn the basics of a fancy text editor called VS Code (in full, Visual Studio Code). Conveniently, we can use a version of this editor (sometimes referred to as Code Server) in our browser via the OSC OnDemand website.\nWe will use VS Code throughout the workshop as practically a one-stop solution for our computing activities at OSC. This is also how I use this editor in my daily work.\nTo emphasize the additional functionality relative to basic text editors like Notepad and TextEdit, editors like VS Code are also referred to as “IDEs”: Integrated Development Environments. If you’ve ever worked with R, the RStudio program is another good example of an IDE."
  },
  {
    "objectID": "modules/03-vscode.html#starting-vs-code-at-osc",
    "href": "modules/03-vscode.html#starting-vs-code-at-osc",
    "title": "The VS Code Text Editor",
    "section": "1 Starting VS Code at OSC",
    "text": "1 Starting VS Code at OSC\nIn the previous module, Mike already guided us through starting a VS Code session via OnDemand, but for the sake of completeness, instructions to do so are also shown below.\n\n\n\n\n\n\nStarting VS Code at OSC\n\n\n\n\n\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nMake sure that Number of hours is at least 4\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code."
  },
  {
    "objectID": "modules/03-vscode.html#getting-started-with-vs-code",
    "href": "modules/03-vscode.html#getting-started-with-vs-code",
    "title": "The VS Code Text Editor",
    "section": "2 Getting started with VS Code",
    "text": "2 Getting started with VS Code\n\n\n\n\n2.1 Side bars\nThe Activity Bar (narrow side bar) on the far left has:\n\nA      (“hamburger menu” icon) in the top, which has most of the standard menu items that you often find in a top bar, like File.\nA      (cog wheel icon) in the bottom, through which you can mainly access settings.\nA bunch of icons in the middle that serve to switch between different options for the (wide) Side Bar, which can show one of the following:\n\nExplorer: File browser (and, e.g., an outline for the active file)\nSearch: To search recursively across all files in the active folder\nSource Control: To work with version control systems like Git (not used in this workshop)\nRun and Debug: For debugging your code (not used in this workshop)\nExtensions: To install extensions (we’ll install one later)\n\n\n\n\n2.2 Editor pane and Welcome document\nThe main part of the VS Code is the editor pane. Whenever you open VS Code, a tab with a Welcome document is automatically opened. This provides some help for beginners, but also, for example, a handy overview of recently opened folders.\nWe can also use the Welcome document to open a new text file by clicking New file below Start (alternatively, click      =>   File   =>   New File). We’ll work with our own text files (scripts) starting tomorrow.\n\n\n\n\n\n\nRe-open the Welcome document\n\n\n\nIf you’ve closed the Welcome document but want it back, click      =>   Help   =>   Welcome.\n\n\n\n\n2.3 Terminal (with a Unix shell)\n By default, no terminal is open in VS Code – open one by clicking      => Terminal => New Terminal.\nIn the terminal, the prompt says Singularity>. This is because in OSC OnDemand, VS Code runs inside a Singularity container (for our purposes, it is not important what that means, exactly).\n Break out of the Singularity shell to get a regular Bash Unix shell: type bash and press Enter.\nIn the next module, Mike will teach us how to use the Unix shell."
  },
  {
    "objectID": "modules/03-vscode.html#some-tips-and-tricks",
    "href": "modules/03-vscode.html#some-tips-and-tricks",
    "title": "The VS Code Text Editor",
    "section": "3 Some tips and tricks",
    "text": "3 Some tips and tricks\n\n3.1 Making use of your screen’s real estate\nSince we are using VS Code inside a browser window, we are unfortunately losing some screen space. Make sure to maximize the browser window and if you have a bookmarks bar, try to hide it (for Chrome: Ctrl/⌘+Shift+B).\n\n\n3.2 Resizing panes\nYou can resize panes (the terminal, editor, and wide sidebar) by hovering your cursor over the borders and then dragging it.\n\n\n3.3 The Command Palette\nTo access all the menu options that are available in VS Code, the so-called “Command Palette” can be handy, especially if you know what you are looking for.\nTo access the Command Palette, click      and then Command Palette or press F1 (or Ctrl/⌘+Shift+P).\nFor a quick test, open the Command Palette and start typing “color theme”, and you’ll see the relevant options pop up.\n\n\n3.4 Color themes\nTo try out different color themes for the entire program, click      and then Color Theme. (I like “Quiet Light”.)"
  },
  {
    "objectID": "modules/03-vscode.html#working-directory",
    "href": "modules/03-vscode.html#working-directory",
    "title": "The VS Code Text Editor",
    "section": "4 Working directory",
    "text": "4 Working directory\nSetting a “working directory” means that you designate a folder on a computer as the starting point for your operations.\n\n\n\n\n\n\nFolder vs. directory\n\n\n\n“Folder” and “directory” mean the same thing – the latter is most commonly used in a Unix shell context.\n\n\nVS Code has a concept of a working directory that is effective in all parts of the program: in the file explorer in the side bar, in the terminal, and when saving or opening files in the editor.\nIn this workshop, we’ll exclusively work within the folder /fs/ess/scratch/PAS2250 (you’ll make personal folders within there shortly). By opening this folder in VS Code (we did this in the form on the OnDemand site), we make sure that VS Code always takes it as a starting point, which will make navigation and saving files much easier.\n\n\n\n\n\n\nTaking off where you were\n\n\n\nAdditionally, when you reopen a folder later, VS Code will to some extent resume where you were before! It will reopen the text files that you had open and if you had an active terminal, it will also open a terminal. This is very convenient, especially when you start working on multiple projects (different folders) in VS Code and switch between those.\n\n\n\n\n\n\n\n\nSwitching folders\n\n\n\nTo switch to a different folder from within VS Code, click      =>   File   =>   Open Folder."
  },
  {
    "objectID": "modules/03-vscode.html#addendum-keyboard-shortcuts",
    "href": "modules/03-vscode.html#addendum-keyboard-shortcuts",
    "title": "The VS Code Text Editor",
    "section": "Addendum: keyboard shortcuts",
    "text": "Addendum: keyboard shortcuts\nWorking with keyboard shortcuts (also called “keybindings”) for common operations can be a lot faster than using your mouse. Below are some useful ones for VS Code (for Mac, replace Ctrl with ⌘).\n\n\n\n\n\n\nKeyboard shortcut cheatsheet\n\n\n\nFor a single-page PDF overview of keyboard shortcuts for your operating system:      =>   Help   =>   Keyboard Shortcut Reference. (Or for direct links to these PDFs: Windows / Mac / Linux.)\n\n\n\nOpen a terminal: Ctrl+` or Ctrl+Shift+C.\nToggle between the terminal and the editor pane: Ctrl+` and Ctrl+1.\nToggle the (wide) Side Bar: Ctrl+B\nLine actions:\n\nCtrl+X / C will cut/copy the entire line where the cursor is, when nothing is selected (!)\nCtrl+Shift+K will delete a line\nAlt+⬆/⬇ will move lines up or down.\n\nMultiple cursors: Press & hold Ctrl+Shift, then ⬆/⬇ arrows to add cursors upwards or downwards.\nToggle line comment (“comment out” code, and removing those comment signs): Ctrl+/\nSplit the editor window vertically: Ctrl+\\ (See also the options in      View => Editor Layout)\n\n\n\n\n\n\n\nBrowser interference\n\n\n\nUnfortunately, some VS Code and terminal keyboard shortcuts don’t work in this setting where we are using VS Code inside a browser, because existing browser keyboard shortcuts take precedence.\nIf you end up using VS Code a lot in your work, it is therefore worth switching to your own installation of the program (see At-home bonus: local installation)"
  },
  {
    "objectID": "modules/03-vscode.html#at-home-bonus-local-installation",
    "href": "modules/03-vscode.html#at-home-bonus-local-installation",
    "title": "The VS Code Text Editor",
    "section": "At-home bonus: local installation",
    "text": "At-home bonus: local installation\nAnother nice feature of VS Code is that it is freely available for all operating systems (and even though it is made by Microsoft, it is also open source).\nTherefore, if you like the program, you can also install it on your own computer and do your local text editing / script writing in the same environment at OSC (it is also easy to install on OSU-managed computers, because it is available in the Self Service software installer).\nEven better, the program can be “tunneled into” OSC, so that your working directory for the entire program can be at OSC rather than on your local computer. This gives the same experience as using VS Code through OSC OnDemand, except that you’re not working witin a browser window, which has some advantages (also: no need to fill out a form, or to break out of the Singularity shell).\nTo install VS Code on your own machine, follow these instructions: Windows / Mac / Linux.\nTo SSH-tunnel VS Code into OSC, see these instructions (they are a bit rudimentary, ask Jelmer if you get stuck)."
  },
  {
    "objectID": "modules/01-intro.html#sign-up-form-responses",
    "href": "modules/01-intro.html#sign-up-form-responses",
    "title": "Introduction to the Workshop",
    "section": "Sign-up form responses",
    "text": "Sign-up form responses"
  },
  {
    "objectID": "modules/01-intro.html#what-you-will-learna",
    "href": "modules/01-intro.html#what-you-will-learna",
    "title": "Introduction to the Workshop",
    "section": "What you will learna",
    "text": "What you will learna\nThe focus of the workshop is on building some general (foundational) skills for analyzing genomics data.\nCommand-line programs are preferred for many of the steps to analyze genomic sequencing data. Because such datasets tend to contain a lot of data, it is also preferable to run your analyses not on a laptop or desktop, but at a compute cluster like the Ohio Supercomputer Center (OSC).\nThese realities mean that in the field of genomics, you need the following set of skills that you may not have been thought during your biology education:\n\nHaving a basic understanding of a compute cluster (supercomputer)\n\nAnd being able to:\n\nUse the Unix shell (work in a terminal)\nWrite small shell scripts\nSubmit scripts to a cluster’s “queue” and monitor and manage the resulting compute jobs\nActivate and probably install software in a Linux environment where you don’t have “admin rights”\n\nWe will teach the basics of these skills during this workshop!\n\n\n\n\n\n\nWhat you won’t learn\n\n\n\nIt may be useful to point out that we will not teach you much, if anything, about:\n\nDetails of genomic data file types\nDetails of specific (genomic) analyses\nMaking biological inferences from your data"
  },
  {
    "objectID": "modules/03-vscode.html#some-vs-code-tips-and-tricks",
    "href": "modules/03-vscode.html#some-vs-code-tips-and-tricks",
    "title": "The VS Code Text Editor",
    "section": "4 Some VS Code tips and tricks",
    "text": "4 Some VS Code tips and tricks\n\n4.1 Making use of your screen’s real estate\nSince we are using VS Code inside a browser window, we are unfortunately losing some screen space. Make sure to maximize the browser window and if you have a bookmarks bar, try to hide it (for Chrome: Ctrl/⌘+Shift+B).\nYou may also opt to hide the side bars using the    =>   View   =>   Appearance menu (or Ctrl/⌘+B for the (wide) Side Bar).\n\n\n4.2 Resizing panes\nYou can resize panes (the terminal, editor, and wide sidebar) by hovering your cursor over the borders and then dragging it.\n\n\n4.3 The Command Palette / Color themes\nTo access all the menu options that are available in VS Code, the so-called “Command Palette” can be handy, especially if you know what you are looking for.\nTo access the Command Palette, click      and then Command Palette (or press F1 or Ctrl/⌘+Shift+P).\n\nOn Your Own: Try a few color themes\nOpen the Command Palette and start typing “color theme”, and you’ll see the relevant option pop up.\nThen, try out a few themes and see what you like!\n(Besides via the Command Palette, you can also access the Color Themes option via click      and then Color Theme.)\n\n\n“Solution” (click here)\n\n“Quiet Light” is the best one"
  },
  {
    "objectID": "modules/03-vscode.html#a-folder-as-a-starting-point",
    "href": "modules/03-vscode.html#a-folder-as-a-starting-point",
    "title": "The VS Code Text Editor",
    "section": "3 A folder as a starting point",
    "text": "3 A folder as a starting point\nConveniently, VS Code takes a specific folder (directory) as a starting point in all parts of the program:\n\nIn the file explorer in the side bar\nIn the terminal\nWhen saving files in the editor pane.\n\nBy default, VS Code via OnDemand will open your Home directory. But in this workshop, we’ll work within the scratch directory for the PAS2250 project, to which you have all been added, which is /fs/ess/scratch/PAS2250.\n Let’s open that folder. Click Open folder... in the Welcome tab (or      =>   File   =>   Open Folder).\nYou’ll notice that the program completely reloads.\n\n\n\n\n\n\nTaking off where you were\n\n\n\nAdditionally, when you reopen a folder later, VS Code will to some extent resume where you were before!\nIt will reopen any files you had open, and if you had an active terminal, it will re-open a terminal. This is very convenient, especially when you start working on multiple projects (different folders) in VS Code and frequently switch between those."
  },
  {
    "objectID": "modules/06-scripts.html#starting-a-vs-code-session-with-an-active-terminal-click-here-1",
    "href": "modules/06-scripts.html#starting-a-vs-code-session-with-an-active-terminal-click-here-1",
    "title": "Shell Scripting",
    "section": "2 Starting a VS Code session with an active terminal (click here)",
    "text": "2 Starting a VS Code session with an active terminal (click here)\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 3 in the box Number of hours\nEnter /fs/ess/scratch/PAS2250 in the box Working Directory\nClick Launch.\n\nOn the next page, once the top bar of the box is green and says Runnning, click Connect to VS Code.\nOpen a terminal:    => Terminal => New Terminal.\nIn the terminal, type bash and press Enter."
  },
  {
    "objectID": "modules/07-software.html#starting-a-vs-code-session-with-an-active-terminal-click-here-1",
    "href": "modules/07-software.html#starting-a-vs-code-session-with-an-active-terminal-click-here-1",
    "title": "Using Software at OSC",
    "section": "2 Starting a VS Code session with an active terminal (click here)",
    "text": "2 Starting a VS Code session with an active terminal (click here)\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 3 in the box Number of hours\nEnter /fs/ess/scratch/PAS2250 in the box Working Directory\nClick Launch.\n\nOn the next page, once the top bar of the box is green and says Runnning, click Connect to VS Code.\nOpen a terminal:    => Terminal => New Terminal.\nIn the terminal, type bash and press Enter."
  },
  {
    "objectID": "modules/09-examples.html#setup",
    "href": "modules/09-examples.html#setup",
    "title": "Batch Jobs in Practice",
    "section": "1 Setup",
    "text": "1 Setup\n\n\n\n\n\n\nStarting a VS Code session with an active terminal (click here)\n\n\n\n\n\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 4 or more in the box Number of hours\nTo avoid having to switch folders within VS Code, enter /fs/ess/scratch/PAS2250/participants/<your-folder> in the box Working Directory (replace <your-folder> by the actual name of your folder).\nClick Launch.\n\nOn the next page, once the top bar of the box is green and says Runnning, click Connect to VS Code.\nOpen a terminal:    => Terminal => New Terminal.\nIn the terminal, type bash and press Enter.\nType pwd to check you are in /fs/ess/scratch/PAS2250. If not, click    =>   File   =>   Open Folder and enter /fs/ess/scratch/PAS2250/<your-folder> (replace <your-folder> by the actual name of your folder)."
  },
  {
    "objectID": "modules/02-osc.html#the-ohio-supercomputer-center-osc",
    "href": "modules/02-osc.html#the-ohio-supercomputer-center-osc",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "1 The Ohio Supercomputer Center (OSC)",
    "text": "1 The Ohio Supercomputer Center (OSC)\nThe Ohio Supercomputer Center is a resource provided by the State of Ohio (not The Ohio State University), so it’s available for use by any person/entity in Ohio including anyone at OSU. Further, academic folks like us are able to use it at heavily subsidized (=cheap!) rates. Physically, it’s located in Columbus - not on campus, but close by.\nWe’ll get you introduced to OSC in this session, and of course, you’ll get experience working with OSC resources as we go though the workshop, but we can’t cover everything here. The good news though is that OSC has lots of good support/training materials online. You can find them at the OSC website."
  },
  {
    "objectID": "modules/02-osc.html#what-is-a-supercomputer",
    "href": "modules/02-osc.html#what-is-a-supercomputer",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "2 What is a supercomputer?",
    "text": "2 What is a supercomputer?\nWe’ll start with the basics here - for our purposes, a supercomputer is basically a bunch of smaller computers connected together, allowing for computing jobs that require more resources than any one of the individual computers can alone provide."
  },
  {
    "objectID": "modules/02-osc.html#some-terminology",
    "href": "modules/02-osc.html#some-terminology",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "3 Some Terminology",
    "text": "3 Some Terminology\nThere are some terms that you’ll almost certainly hear (and maybe have used before yourself) when working with a supercomputer - either at OSC or elsewhere. Let’s think about how these terms relate to each other…\n\nSupercomputer\nNode\nCluster\nCore\nProcessor\n\n\n\n\nThere’s flexibility when using a supercomputer at OSC - the smallest job you can run would use a single processor, or core. Or you could run a slightly bigger job that uses say 10 cores that are all within one physical node. Or you could run an even bigger job that uses 100 cores that exist across 4 nodes by connecting those nodes together so their resources are shared (no single node at OSC has 100 cores)."
  },
  {
    "objectID": "modules/02-osc.html#the-structure-of-a-supercomputer",
    "href": "modules/02-osc.html#the-structure-of-a-supercomputer",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "4 The Structure Of A Supercomputer",
    "text": "4 The Structure Of A Supercomputer\nWe’re going to think about a supercomputer has having three main parts…\n\nLogin Node(s)\nCompute Nodes\nFile Storage System\n\n\n\n\n\n4.1 File Storage\nThere are 3 main areas/file systems where you can store files at OSC…\n\nHome Directories: /users/\nProject Directories: /fs/project/ OR /fs/ess/project/\nScratch Directories: /fs/scratch/ OR /fs/ess/scratch/\n\n\n\n\n\n4.1.1 Home Directory\nWhen you initially get an account with OSC, a HOME directory is created for you, named with your OSC username. This directory will always be within /users/. What subfolder it exists in depends on what project you were initially associated with. For example, the first project I was associated with at OSC was PAS0656. I don’t work on that project anymore, but the location of my HOME directory remains /users/PAS0656/osu6672 (note osu6672 is my OSC username).\nThis HOME directory provides you with up to 500 GB of storage (or up to 1,000,000 files). This is considered permanent storage, so it is well backed-up.\n\n\n4.1.2 Project Directories\nProject directories can’t be set up by just anyone, as the storage there has to be paid for. Typically, here at the University, Project directories are set up by PI’s. Project directories offer flexibility in terms of the amount of storage available (requested at the time of set-up, and then easily adjustable after that), and also in terms of who can access files in the directory.\nUnlike your HOME directory, where typically only you will have access to the files it contains, Projects are meant to be available to a specified group of people - say the members of a lab all working on a project. Like HOME directories, these are also backed up routinely. Project directories are located inside either /fs/project/ or /fs/ess/.\n\n\n4.1.3 Scratch Storage\nThe Scratch file system is meant to provide temporary storage space. Every Project directory has a corresponding Scratch directory. One advantage to Scratch space is that it is effectively unlimited. However, it’s not backed up, and files that are unmodified for a specific amount of time are automatically deleted.\n\n\n\n4.2 Login Nodes\n\n\n\nLogin nodes are set aside as an initial landing spot for everyone who logs in to a supercomputer. These are meant to be used to do things like organize your files, create scripts that specify future compute jobs, etc.\nThey are NOT meant for doing any serious computing on. Resources on the login nodes are limited and shared by everyone who logs in. Attempting large computing efforts on these nodes risks taxing the resources (i.e. RAM) and bogging things down for everyone. There are checks built in that limit what you are able to do on the login nodes (i.e. jobs running for longer than 20 min will be killed), but it’s best to just not push it at all. Any bigger computing jobs are better sent to the compute cluster.\n\n\n4.3 Compute Cluster\n\n\n\nThe compute cluster is really the powerhouse of the supercomputer, and is where you run your compute jobs. The details in terms of the size of the compute cluster itself, and the sizes of individual nodes contributing to the cluster (i.e. number of cores available on each node, amount of RAM available, etc) depends on which supercomputer you’re connected to, but regardless, you’ll have access to much more computing power than you’ll get on your local system.\nEach time you send a compute job to the compute cluster, you also make a request for the number of resources the job will need - specifically, the number of nodes, cores, and how long the job will run. As there are jobs being sent by different users all the time, there is software called a job scheduler that considers each request and assigns the necessary resources to the job as they are available. We’ll talk about that more later in the workshop.\nAll these parts are connected together to create a supercomputer…"
  },
  {
    "objectID": "modules/02-osc.html#supercomputers-at-osc",
    "href": "modules/02-osc.html#supercomputers-at-osc",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "5 Supercomputers at OSC",
    "text": "5 Supercomputers at OSC\nOSC technially has 3 separate supercomputers…\n\nRuby\nOwens\nPitzer\n\nRuby is reserved for specific uses, so the two you’ll interact with are Owens and Pitzer. This is what Owens looks like (and Pitzer is similar)…"
  },
  {
    "objectID": "modules/02-osc.html#connecting-to-a-supercomputer",
    "href": "modules/02-osc.html#connecting-to-a-supercomputer",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "6 Connecting to a Supercomputer",
    "text": "6 Connecting to a Supercomputer\nWhether you’re working on a supercomputer at OSC or elsewhere, you’ll connect to it via a connection between your personal computer and the remote computer. Sometimes this connection is a one-way connection, in which you’re working exclusively on the remote computer, and in other cases it can be a two-way connection. The two-way connection scenario allows you to transfer files between your local computer and the remote system (supercomputer).\n\n6.1 ssh\nA one-way connection that allows you to connect to and work on a remote supercomputer is usually achieved through secure shell - often abbreviated and referred to as ssh.\n\n\n\nThe two most common ways of connecting to OSC by way of ssh are though a Terminal or the OnDemand system.\n\n6.1.1 Terminal\nTraditionally if you’re on a Unix-based system (Mac or Linux), you can open the Terminal application and connect through ssh with something like this…\n\n\n\nUnfortunately, while this can also be done on a Windows-based system, it requires an extra piece of software (an ssh client - typically one called putty), as the ssh command isn’t available by default in Windows like it is on Unix-based systems.\nThe good news for us is that OSC provides an alternative way to interact with their supercomputers: the OnDemand system.\n\n\n6.1.2 OnDemand\nOnDemand allows you to access OSC resources through a web browser - it not only provides ssh access, but other forms of access too, including the ability to upload and download files between the remote and local computer. The OSC OnDemand starting point can be accessed at https://ondemand.osc.edu. After logging in, you should see a page similar to the one below:\n\n\n\nWe’re going to focus on some of the options in the blue bar along the top. Let’s start with Files. Hovering over this gives a list of directories you have access to. If you’re account is new, you might only have one or two. I’m associated with quite a few different projects, so I have a bunch. I’ll select my HOME directory.\n\n\n\nHere I can see a list of directories and files that are in my HOME directory.\n\n\n\nThere are also a series of useful buttons across the top. One note about the Upload/Download buttons here - these are great in many cases, but likely won’t be a good option for especially large files. Other options, such as Globus, are available for these (more info on Globus here).\nMore generally, this general interface is somewhat unique at OSC. Not all supercomputers will have this convenient GUI interface (the OnDemand system) that allows you to perform tasks like creating new folders, moving files, etc. in a point-and-click manner.\nHaving these is certainly nice, but they only go so far. Using a supercomputer effectively is likely to require interacting with the system through a command line approach as opposed to a point-and-click interface. One option available for doing that is under the Clusters option in the blue top bar:\n\n\n\nHere I’m selecting shell access to the Pitzer supercomputer, which will open a new browser tab looking like this:\n\n\n\nThis shell system works just like the Terminal program we briefly came across earlier. Basically, there’s a prompt (at the bottom of the image above) where you can enter a command for the supercomputer to execute. This is a common way to interact with the supercomputer.\nHowever, we’re going to check out one more option - a program called VS Code, which gives us the same basic shell access, but also includes some additional features that we might find useful later on. We can access VS Code by selecting Code Server under the Interactive Apps option on the OnDemand homepage (near the bottom of the list).\n\n\n\nIn this case, we need to provide a maximum time to keep the App open. The default is 6 hours. That will be fine for our purposes - you could even go a bit less, but remember that if you select 2 hours, you’ll be kicked off at the 2-hour mark, so usually better to err on the side of overestimating here.\n\n\n\nClick on Launch at the bottom and this will send your request to run the App for a maximum of the number of hours you chose. Once it’s ready, you’ll get a screen that looks like…\n\n\n\nWhen it’s available, click on the blue Connect to VS Code button and you should see the following Welcome screen…\n\n\n\nJelmer will talk about VS Code in the next section."
  },
  {
    "objectID": "modules/04-shell.html#the-unix-shellcommand-line-computing",
    "href": "modules/04-shell.html#the-unix-shellcommand-line-computing",
    "title": "The Unix Shell",
    "section": "1 The Unix Shell/Command Line Computing",
    "text": "1 The Unix Shell/Command Line Computing\nMany of the things you typically do by pointing and clicking can alternatively be done with command line approaches. The Unix shell allows you to interact with the supercomputer in a command-line environment. The Code Server/VS Code software Jelmer just introduced is one of several methods available for accessing the Unix shell, and the one we’ll use through the workshop. Now that we have the platform for interacting with the supercomputer, we’ll dive into command line computing."
  },
  {
    "objectID": "modules/04-shell.html#overview",
    "href": "modules/04-shell.html#overview",
    "title": "The Unix Shell",
    "section": "2 Overview",
    "text": "2 Overview\nWorking effectively on a remote supercomputer requires doing command-line computing. But there are more advantages to doing command line computing than just allowing you to work on a supercomputer.\n\n2.1 Advantages to Command Line Computing\n\nInteract with a remote computer\nWork efficiently with large files\nAchieve reproducibility in research\nPerform general computing tasks more efficiently\n\n\n\n2.2 Structure Of Command Line Expressions\nWe’ll think of command line expressions as having 3 main parts. Don’t worry about following along here - there will be plenty of chances to try this out shortly. For now, just treat this as a demonstration and focus on these 3 components…\n\nCommands\nOptions or Arguments\nOutput\n\n\n2.2.1 Command Line Commands\nThe prompt indicates the shell is ready for a command.\n\n\n\nLet’s start with the ls command.\n\n\n\n\n\n\nHere, we’ve given a command, ls, and the shell has returned some output – in this case, listing the contents of the current directory. It has also returned another prompt, meaning it’s ready for another command.\nNow we’ll run the same command again, but this time we’ll add in an option -l (a dash followed by a lowercase L). Options allow you to modify the behavior of a command.\n\n\n\n\n\n\nNotice that the same four items are returned, but this time, they’re printed in a different format, and additional information is included.\nLet’s try adding one more option/argument – -h.\n\n\n\n\n\n\nCan you pick out what -h did to modify the output? Note the difference in the format of the column reporting the sizes of the items listed.\n\n\n\n2.3 Commonly-Used Commands\nBelow are some commands I find myself using quite a bit, grouped into some general categories based on their primary function…\n\nGetting Info About a Command\n\nman\n\nNavigating in the Terminal\n\npwd\ncd\n\nViewing Files\n\nless\nhead\ntail\ncat\n\nManaging/Organizing Files\n\nls\nmkdir\nrm\ncp\nmv\n\nWorking With Compressed Files\n\ngzip\ngunzip\nzcat/gzcat\nunzip\n\nAssessing Files\n\ndiff\nmd5\ngrep\nwc\n\nEditing Files\n\nsed\nawk\nsort\ntr\nuniq\n\nObtaining/Sharing Files\n\ncurl\nsftp\nwget\n\nFeatures\n\nTab completion\nCommand History (up arrow)\nCtrl+r\nCtrl+c\n\nSpecial Notation\n\n|\n~\n.\n..\n$PATH\n$HOME\n\nWildcards\n\n*\n?\n[]\n^\n\n\nWhile it’s not an exhaustive list, getting a grasp on some of the commands and features above will go a long way in allowing you to start to work at command line. We won’t get to all of them in this session, but we’ll explore quite a few on this list."
  },
  {
    "objectID": "modules/04-shell.html#initial-practice",
    "href": "modules/04-shell.html#initial-practice",
    "title": "The Unix Shell",
    "section": "3 Initial Practice",
    "text": "3 Initial Practice\nHere we’ll start practicing with some common command-line commands.\n\n3.1 Working Directories\nBefore we start with our first command, we should talk about directory paths and working directories. All the files on a computer exist within a hierarchical system of directories (folders). When working at command line, you are always “in” one of these directories. The directory you’re “in” at any given time is referred to as your working directory. It’s useful to know what this is at any given time, and there’s a command that tells you: pwd. This brings us to our first command.\n\n\n3.2 Common Commands\n\n3.2.1 pwd\nThe pwd command prints the absolute path of your current working directory.\n\n\n\n\n\n\nThe default working directory when you log on to OSC is your HOME directory. You’ll likely make lots of use of this directory if you work at OSC, but for the workshop, we’re all going to work in the scratch directory associated with the project set up for the workshop. So we’ll move to that directory next.\n\n\n3.2.2 cd\ncd allows you to change your working directory to any directory you have permission to access on the computer you’re working on. And this is a great place to introduce Tab Completion, which you really should get in the habit of using…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbsolute Paths vs Relative Paths\n\n\n\nWhat we’ve used here is an absolute (full) path. If you want to change directories, the cd command needs to know where you want to move to. As we just saw, we can give it that information by providing the absolute path to the directory on the computer we want to move to (starting with the root directory, which is indicated by the first / in the path /fs/ess/scratch/PAS2250 above).\nProviding absolute paths will always work, but it’s often a bit more typing than we need (or want) to do. An alternative is to work with relative paths. These work by assuming you’re staring in your current working directory, and then, by default, looking forward in the directory structure (or down if you like to think from top to bottom). We’ll come back to relative paths shortly.\n\n\nOK, we made it to a directory created specifically for this workshop. Let’s see what’s in there.\n\n\n3.2.3 ls\nThe ls command lists everything inside the current directory.\n\n\n\n\n\n\nHere we see 3 directories - ‘data’, ‘jelmer’, and ‘participants’.\nLet’s see what’s inside the ‘data’ directory (and another good chance to try Tab Completion)…\n\n\n\n\n\n\n\n\n\nTwo more directories this time. Try viewing the contents of the fastq directory yourself…\n\n3.2.3.1 On Your Own\n\n\n\nTry to list the contents of the ‘fastq’ directory we just saw.\n\n\nSolutions (click here)\n\nls data/fastq/\nOR\ncd data/fastq/\nls\n\n\n\n\nLet’s check to make sure we’re still in the /fs/ess/scratch/PAS2250 directory, and also remind ourselves exactly what’s in there…\n\n\n\nIf you’re not in the PAS2250 directory for some reason, you can use cd to get back there. You should see ‘data’, ‘jelmer’, and ‘participants’ when listing the contents of your current directory.\nNow let’s move our working directory again – this time we’ll go in to participants. We could use cd with the absolute path – similar to what we did before. However, we’ll save ourselves some typing and use a relative path. Keep using Tab Completion.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.4 mkdir\nThe mkdir command allows you to create a new directory. Create one for yourself in the current directory (participants) according to this example (replace ‘mikes_dir’ with a name for your folder – avoid spaces and special characters in the name)…\n\n\n\n\n\n\n\n\n3.2.5 Summary Thus Far\nSo far, we’ve used…\n\npwd (print working directory)\ncd (change directory)\nls (list)\nmkdir (make new directory)\nTab Completion (not really a command, but a useful feature)\n\nWe also have a directory structure that looks like…\n\n\n\nThe data files in the ‘data/fastq’ directory are fastq formatted files from an RNA experiment, and are what we’ll be analyzing as we go through the workshop. We’ll talk more about them soon, but for now, let’s make sure everyone has a copy of the data. We’ll copy the ‘data’ directory and its contents into the new directory you just made.\n\n\n\n\n\n3.2.6 cp\nThe cp command allows you to copy files or directories from one location to another. It has 2 required arguments – what you want to copy, and where you want to copy it to. Let’s start with what we want to copy. It’s the ‘data’ directory and all of its contents. Notice in the diagram above that ‘data’ is at the same level in the directory structure as our current working directory, participants. This means using ‘data’ as a relative path won’t work, because the computer looks down the directory structure (it will see the contents of ‘participants’). But there’s a way to deal with that. We can use .. to move us up a level in the directory structure.\n\n\n\n\n\n\nNotice we get a message that it omitted copying the directory ‘data’ (which is what we wanted to copy). Indeed, the copy didn’t work (you can ls the contents of the target directory to check – it will still be empty). cp works in this simplest form with individual files, but not with directories that have contents inside them. If you want to copy a directory and all of its contents, we need one of those options that modify the behavior of the cp command. In this case, -r, which tells it to copy in a recursive manner.\nAnd this is a good spot to introduce the Command History. At the prompt, try hitting the up arrow. A record of all your previous commands is kept, so you can scroll back through them. Use this to get the previous cp command, and then add the -r argument.\n\n\n\n\n\n\nAnd we can check to make sure the copy worked…\n\n\n\n\n\n3.2.7 man\nWe haven’t talked about man yet. This stands for manual, and is a great way to get details on any command. For example, we can check out the man page for cp…\n\n\n\n\n\n\nIf you scroll down, you’ll see information on the -r option we just used (among others). As it says at the bottom of the page, type ‘q’ to quit and get back to your command line prompt."
  },
  {
    "objectID": "modules/04-shell.html#working-with-text-files-at-command-line",
    "href": "modules/04-shell.html#working-with-text-files-at-command-line",
    "title": "The Unix Shell",
    "section": "4 Working With Text Files At Command Line",
    "text": "4 Working With Text Files At Command Line\nNow let’s start to explore our fastq files a bit. In preparation, it’s a good chance to practice a few of the commands we’ve seen so far.\n\n4.0.0.1 On Your Own\n\n\n\nSet your working directory to the ‘data/fastq’ directory inside the folder you created for yourself. Then list the contents of that ‘fastq’ directory. How many files are in there? See if you can get the sizes of each file.\n\n\nHint (click here)\n\nUse cd and a relative path (your_dir/data/fastq/) to change you working directory. Once you’re there, use ls to list the contents of the current directory. Check out the man page for ls to see if you can find an option that will give you more detailed information about each file.\n\n\n\nSolutions (click here)\n\ncd *your_dir*/data\nls\nls -l\n\n\n\n\n\n\n4.1 Compressed Files\nYou might have noticed these files all have a ‘.gz’ extension, indicating they are ‘gz-compressed’. This is a common type of compression for large genomic-scale files. The fact that they’re compressed means we can’t just open them up and look inside – we need to uncompress them first. The gunzip command would allow us to do this – it uncompresses the file it’s given and writes the uncompressed version to a new file. We could do this, but there’s another approach. Fastq files can get big, and sometimes it helps to be able to keep them compressed as much as possible. It’s a good time for us to explore the pipe.\n\n4.1.1 |\nWe talked earlier about command line expressions having 3 parts – the command itself, options, and output. By default, any output is printed to the screen. That’s what we’ve seen so far. But you can also redirect the output, and there are two primary ways to redirect it…\n\nthe ‘>’, which is followed by the name of a new text file the output will be written to\nthe ‘|’ (pipe), which takes the output of one command and “pipes” it in as input for a subsequent command.\n\nLet’s try to preview the contents of one of the compressed files.\n\n\n4.1.2 head\nThe head command is a great way to preview the contents of a text file. By default, head prints the first 10 lines of a file. Since these are fastq files, let’s print 8 lines (a multiple of 4 – it will become clear why shortly). We can use the -n argument to specify the number of lines that will be returned.\n\n\n\n\n\n\nThis isn’t what we want – we’re seeing the first 8 lines of the compressed files - not helpful.\n\n\n4.1.3 zcat\nThe zcat function prints human-readable contents of a gz-compressed file to the screen. We can try running it on the file, but remember the file is pretty big – there are lots of lines of text in there that will all get printed to the screen. Instead, we can pipe the output of zcat to the head command.\n\n\n\n\n\n\nMuch better – this is what the raw RANseq data look like!"
  },
  {
    "objectID": "modules/04-shell.html#fastq-format",
    "href": "modules/04-shell.html#fastq-format",
    "title": "The Unix Shell",
    "section": "5 Fastq Format",
    "text": "5 Fastq Format\nIf you’re not familiar with it, fastq is a very common format for genomic data files. The raw data produced by a high-throughput sequencer will almost certainly be returned to you in this format. These are plain text files, and each sequence that is read by the sequencer is represented by 4 lines:\n\na name line\nthe sequence itself\na plus sign\nquality scores corresponding to each base position in the sequence\n\n\n5.0.1 wc\nSince each read in a fastq file is represented by 4 lines, we should expect the number of lines in each of the fastq files to be a multiple of 4. Let’s check one. The wc command stands for word count, but by default, it returns the number of words, lines, and characters in a file. The -l option tells it to return just the number of lines, so we’ll use it since that’s all we’re interested in right now. And remember, we’ll want to do this on the uncompressed data.\n\n\n\n\n\n\n\n\n5.0.2 grep\ngrep allows you to search through a file for specific patterns of text and returns the matching lines. For example, let’s say we wanted to see what sequences in sample SRR7609467 contain the sequence ‘ACCGATACG’\n\n\n\n\n\n\n\n5.0.2.1 On Your Own\n\n\n\nHow many sequences in sample SRR7609467 contain the sequence ‘CCAGTA’\n\n\nHint (click here)\n\nPipe the results of the grep to wc -l. Alternatively, check out the -c option to grep in the man page.\n\n\n\nSolutions (click here)\n\nzcat SRR7609467.fastq.gz | grep 'CCAGTA' | wc -l\nOR\nzcat SRR7609467.fastq.gz | grep -c 'CCAGTA'"
  },
  {
    "objectID": "modules/04-shell.html#downloading-files-from-online",
    "href": "modules/04-shell.html#downloading-files-from-online",
    "title": "The Unix Shell",
    "section": "6 Downloading Files From Online",
    "text": "6 Downloading Files From Online\nAt command line, you can’t just open a web browser and download a file you might want. But of course there are commands to do that. As we move toward starting to analyze our example RNAseq dataset, one thing we’ll need is the reference genome for the species these sequences came from – in this case, Phaseolus vulgaris. Before we get that, it’s another good time to practice some of those common commands…\n\n6.0.0.1 On Your Own\n\n\n\nCreate a new (empty) directory named ‘reference’ that will later store the reference genome for our analyses. Put it in your own directory inside ‘participants’. Then make this ‘reference/’ directory your working directory.\n\n\nHint (click here)\n\nUse the ‘mkdir’ command (and cd as necessary). Remember that .. moves you up/back one directory, and these can be combined. For example, ../../../ would move you up/back 3 directories.\n\n\n\nSolution (click here)\n\nmkdir ../../reference\ncd ../../reference\nOR\ncd ../../\nmkdir reference\ncd reference\n\n\n\n\n\n\n6.0.1 curl\ncurl is one command that allows you to download files from online (wget is another). Technically, all you need for curl is the name of the command and the web address for what you want to download. However, remember that the default is to print the downloaded contents to the screen. This usually isn’t what we want. Instead, we want to save them to a file. One option would be to redirect the output to a text file with ‘>’. But curl also has a built-in option to write the contents to a file: -o, so we’ll use that. Since the file that gets downloaded is a gz-compressed, fasta-formatted text file, we’ll give the name of the file a ‘.fa.gz’ extension. The reference genome for Phaseolus vulgaris is available at https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/499/845/GCF_000499845.1_PhaVulg1_0/GCF_000499845.1_PhaVulg1_0_genomic.fna.gz.\n\n\n\n\n\n\n\n6.0.1.1 On Your Own\n\n\n\nTry previewing the contents of the reference genome file you just downloaded.\n\n\nHint (click here)\n\nRemember the file is gz-compressed. Use zcat and pipe the results to head.\n\n\n\nSolution (click here)\n\nzcat Pvulg.fa.gz | head\n\n\n\n\nOK, now we’ve got our raw data (fastq) and our reference genome (fasta). This is a good start in terms of getting ready to start analyzing the data. One more thing we can do now is try to understand a little bit about the samples themselves. There is a tab-separated text file named ‘meta.tsv’ in the ‘data/meta’ directory. Let’s take a look at its contents…\n\n\n\n6.0.2 less\nless is a command that opens up a text file within your shell. Once you’re finished viewing the file, type ‘q’ to quit and return to your prompt."
  },
  {
    "objectID": "modules/04-shell.html",
    "href": "modules/04-shell.html",
    "title": "The Unix Shell",
    "section": "",
    "text": "OK, we made it to a directory created specifically for this workshop. Let’s see what’s in there."
  },
  {
    "objectID": "modules/04-shell.html#absolute-paths-vs-relative-paths",
    "href": "modules/04-shell.html#absolute-paths-vs-relative-paths",
    "title": "The Unix Shell",
    "section": "4 Absolute Paths vs Relative Paths",
    "text": "4 Absolute Paths vs Relative Paths\nWhat we’ve used here is an absolute (full) path. If you want to change directories, the cd command needs to know where you want to move to. As we just saw, we can give it that information by providing the absolute path to the directory on the computer we want to move to (starting with the root directory, which is indicated by the first ‘/’ in the path /fs/ess/scratch/PAS2250 above). Providing absolute paths will always work, but it’s often a bit more typing than we need (or want) to do. An alternative is to work with relative paths. These work by assuming you’re staring in your current working directory, and then, by default, looking forward in the directory structure (or down if you like to think from top to bottom). We’ll come back to relative paths shortly."
  },
  {
    "objectID": "modules/04-shell.html#working-directories",
    "href": "modules/04-shell.html#working-directories",
    "title": "The Unix Shell",
    "section": "3 Working Directories",
    "text": "3 Working Directories\nBefore we start with our first command, we should talk about directory paths and working directories. All the files on a computer exist within a hierarchical system of directories (folders). When working at command line, you are always “in” one of these directories. The directory you’re “in” at any given time is referred to as your working directory. It’s useful to know what this is at any given time, and there’s a command that tells you: pwd. This brings us to our first command."
  },
  {
    "objectID": "modules/04-shell.html#common-commands",
    "href": "modules/04-shell.html#common-commands",
    "title": "The Unix Shell",
    "section": "4 Common Commands",
    "text": "4 Common Commands\nHere we’ll start practicing with some common command-line commands.\n\n4.0.1 pwd\nThe pwd command prints the absolute path of your current working directory.\n\n\n\n\n\n\nThe default working directory when you log on to OSC is your HOME directory. You’ll likely make lots of use of this directory if you work at OSC, but for the workshop, we’re all going to work in the scratch directory associated with the project set up for the workshop. So we’ll move to that directory next.\n\n\n4.0.2 cd\ncd allows you to change your working directory to any directory you have permission to access on the computer you’re working on. And this is a great place to introduce Tab Completion, which you really should get in the habit of using…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbsolute Paths vs Relative Paths\n\n\n\nWhat we’ve used here is an absolute (full) path. If you want to change directories, the cd command needs to know where you want to move to. As we just saw, we can give it that information by providing the absolute path to the directory on the computer we want to move to (starting with the root directory, which is indicated by the first / in the path /fs/ess/scratch/PAS2250 above).\nProviding absolute paths will always work, but it’s often a bit more typing than we need (or want) to do. An alternative is to work with relative paths. These work by assuming you’re staring in your current working directory, and then, by default, looking forward in the directory structure (or down if you like to think from top to bottom). We’ll come back to relative paths shortly.\n\n\nOK, we made it to a directory created specifically for this workshop. Let’s see what’s in there.\n\n\n4.0.3 ls\nThe ls command lists everything inside the current directory.\n\n\n\n\n\n\nHere we see 3 directories - ‘data’, ‘jelmer’, and ‘participants’.\nLet’s see what’s inside the ‘data’ directory (and another good chance to try Tab Completion)…\n\n\n\n\n\n\n\n\n\nTwo more directories this time. Try viewing the contents of the fastq directory yourself…\n\n4.0.3.1 On Your Own\n\n\n\nTry to list the contents of the ‘fastq’ directory we just saw.\n\n\nSolutions (click here)\n\nls data/fastq/\nOR\ncd data/fastq/\nls\n\n\n\n\nLet’s check to make sure we’re still in the /fs/ess/scratch/PAS2250 directory, and also remind ourselves exactly what’s in there…\n\n\n\nIf you’re not in the PAS2250 directory for some reason, you can use cd to get back there. You should see ‘data’, ‘jelmer’, and ‘participants’ when listing the contents of your current directory.\nNow let’s move our working directory again – this time we’ll go in to participants. We could use cd with the absolute path – similar to what we did before. However, we’ll save ourselves some typing and use a relative path. Keep using Tab Completion.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.0.4 mkdir\nThe mkdir command allows you to create a new directory. Create one for yourself in the current directory (participants) according to this example (replace ‘mikes_dir’ with a name for your folder – avoid spaces and special characters in the name)…\n\n\n\n\n\n\n\n\n4.0.5 Summary Thus Far\nSo far, we’ve used…\n\npwd (print working directory)\ncd (change directory)\nls (list)\nmkdir (make new directory)\nTab Completion (not really a command, but a useful feature)\n\nWe also have a directory structure that looks like…\n\n\n\nThe data files in the ‘data/fastq’ directory are fastq formatted files from an RNA experiment, and are what we’ll be analyzing as we go through the workshop. We’ll talk more about them soon, but for now, let’s make sure everyone has a copy of the data. We’ll copy the ‘data’ directory and its contents into the new directory you just made.\n\n\n\n\n\n4.0.6 cp\nThe cp command allows you to copy files or directories from one location to another. It has 2 required arguments – what you want to copy, and where you want to copy it to. Let’s start with what we want to copy. It’s the ‘data’ directory and all of its contents. Notice in the diagram above that ‘data’ is at the same level in the directory structure as our current working directory, participants. This means using ‘data’ as a relative path won’t work, because the computer looks down the directory structure (it will see the contents of ‘participants’). But there’s a way to deal with that. We can use .. to move us up a level in the directory structure.\n\n\n\n\n\n\nNotice we get a message that it omitted copying the directory ‘data’ (which is what we wanted to copy). Indeed, the copy didn’t work (you can ls the contents of the target directory to check – it will still be empty). cp works in this simplest form with individual files, but not with directories that have contents inside them. If you want to copy a directory and all of its contents, we need one of those options that modify the behavior of the cp command. In this case, -r, which tells it to copy in a recursive manner.\nAnd this is a good spot to introduce the Command History. At the prompt, try hitting the up arrow. A record of all your previous commands is kept, so you can scroll back through them. Use this to get the previous cp command, and then add the -r argument.\n\n\n\n\n\n\nAnd we can check to make sure the copy worked…\n\n\n\n\n\n4.0.7 man\nWe haven’t talked about man yet. This stands for manual, and is a great way to get details on any command. For example, we can check out the man page for cp…\n\n\n\n\n\n\nIf you scroll down, you’ll see information on the -r option we just used (among others). As it says at the bottom of the page, type ‘q’ to quit and get back to your command line prompt."
  },
  {
    "objectID": "modules/04-shell.html#practice-with-common-commands",
    "href": "modules/04-shell.html#practice-with-common-commands",
    "title": "The Unix Shell",
    "section": "3 Practice with Common Commands",
    "text": "3 Practice with Common Commands\nHere we’ll start practicing with some common command-line commands.\n\n\n\n\n\n\nWorking Directories\n\n\n\nBefore we start with our first command, we should talk about directory paths and working directories. All the files on a computer exist within a hierarchical system of directories (folders). When working at command line, you are always “in” one of these directories. The directory you’re “in” at any given time is referred to as your working directory.\nIt’s useful to know what this is at any given time, and there’s a command that tells you: pwd. This brings us to our first command.\n\n\n\n3.1 pwd\nThe pwd command prints the absolute path of your current working directory.\n\n\n\n\n\n\nThe default working directory when you log on to OSC is your HOME directory. You’ll likely make lots of use of this directory if you work at OSC, but for the workshop, we’re all going to work in the scratch directory associated with the project set up for the workshop. So we’ll move to that directory next.\n\n\n3.2 cd\ncd allows you to change your working directory to any directory you have permission to access on the computer you’re working on. And this is a great place to introduce Tab Completion, which you really should get in the habit of using…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbsolute Paths vs Relative Paths\n\n\n\nWhat we’ve used here is an absolute (full) path. If you want to change directories, the cd command needs to know where you want to move to. As we just saw, we can give it that information by providing the absolute path to the directory on the computer we want to move to (starting with the root directory, which is indicated by the first / in the path /fs/ess/scratch/PAS2250 above).\nProviding absolute paths will always work, but it’s often a bit more typing than we need (or want) to do. An alternative is to work with relative paths. These work by assuming you’re staring in your current working directory, and then, by default, looking forward in the directory structure (or down if you like to think from top to bottom). We’ll come back to relative paths shortly.\n\n\nOK, we made it to a directory created specifically for this workshop. Let’s see what’s in there.\n\n\n3.3 ls\nThe ls command lists everything inside the current directory.\n\n\n\n\n\n\nHere we see 3 directories – data, jelmer, and participants.\nLet’s see what’s inside the data directory (and another good chance to try Tab Completion)…\n\n\n\n\n\n\n\n\n\nTwo more directories this time. Try viewing the contents of the fastq directory yourself…\n\nOn Your Own: list the contents of a directory\nTry to list the contents of the fastq directory we just saw.\n\n\nSolutions (click here)\n\n\nls data/fastq/\n\nOR\n\ncd data/fastq/\nls\n\n\n\nLet’s check to make sure we’re still in the /fs/ess/scratch/PAS2250 directory, and also remind ourselves exactly what’s in there…\n\n\n\nIf you’re not in the PAS2250 directory for some reason, you can use cd to get back there. You should see data, jelmer, and participants when listing the contents of your current directory.\nNow let’s move our working directory again – this time we’ll go in to participants. We could use cd with the absolute path – similar to what we did before. However, we’ll save ourselves some typing and use a relative path. Keep using Tab Completion.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.4 mkdir\nThe mkdir command allows you to create a new directory. Create one for yourself in the current directory (participants) according to this example (replace mikes_dir with a name for your folder – avoid spaces and special characters in the name)…\n\n\n\n\n\n\n\n\n3.5 Summary Thus Far\nSo far, we’ve used…\n\npwd (print working directory)\ncd (change directory)\nls (list)\nmkdir (make new directory)\nTab Completion (not really a command, but a useful feature)\n\nWe also have a directory structure that looks like…\n\n\n\nThe data files in the data/fastq directory are FASTQ formatted files from an RNA experiment, and are what we’ll be analyzing as we go through the workshop. We’ll talk more about them soon, but for now, let’s make sure everyone has a copy of the data. We’ll copy the data directory and its contents into the new directory you just made.\n\n\n\n\n\n3.6 cp\nThe cp command allows you to copy files or directories from one location to another. It has 2 required arguments – what you want to copy, and where you want to copy it to.\nLet’s start with what we want to copy. It’s the data directory and all of its contents. Notice in the diagram above that data is at the same level in the directory structure as our current working directory, participants. This means using data as a relative path won’t work, because the computer looks down the directory structure (it will see the contents of ‘participants’). But there’s a way to deal with that. We can use .. to move us up a level in the directory structure.\n\n\n\n\n\n\nNotice we get a message that it omitted copying the directory data (which is what we wanted to copy). Indeed, the copy didn’t work (you can ls the contents of the target directory to check – it will still be empty). cp works in this simplest form with individual files, but not with directories that have contents inside them. If you want to copy a directory and all of its contents, we need one of those options that modify the behavior of the cp command. In this case, -r, which tells it to copy in a recursive manner.\nAnd this is a good spot to introduce the Command History. At the prompt, try hitting the up arrow. A record of all your previous commands is kept, so you can scroll back through them. Use this to get the previous cp command, and then add the -r argument.\n\n\n\n\n\n\nAnd we can check to make sure the copy worked…\n\n\n\n\n\n3.7 man\nWe haven’t talked about man yet. This stands for manual, and is a great way to get details on any command. For example, we can check out the man page for cp…\n\n\n\n\n\n\nIf you scroll down, you’ll see information on the -r option we just used (among others). As it says at the bottom of the page, type q to quit and get back to your command line prompt."
  },
  {
    "objectID": "modules/04-shell.html#working-with-text-files",
    "href": "modules/04-shell.html#working-with-text-files",
    "title": "The Unix Shell",
    "section": "4 Working With Text Files",
    "text": "4 Working With Text Files\nNow let’s start to explore our FASTQ files a bit. In preparation, it’s a good chance to practice a few of the commands we’ve seen so far.\n\nOn Your Own: Explore the Files\nSet your working directory to the data/fastq directory inside the folder you created for yourself. Then list the contents of that fastq directory. How many files are in there? See if you can get the sizes of each file.\n\n\nHint (click here)\n\nUse cd and a relative path (<your_dir>/data/fastq/) to change you working directory.\nOnce you’re there, use ls to list the contents of the current directory. Recall the option that we used above to give more detailed information about each file, or check out the man page for ls.\n\n\n\nSolutions (click here)\n\n\ncd <your_dir>/data\n\nls\n\nls -l\n\n\n\n\n4.1 Compressed Files\nYou might have noticed these files all have a .gz extension, indicating they are ‘gzip-compressed’. This is a common type of compression for large genomic-scale files. The fact that they’re compressed means we can’t just open them up and look inside – we need to uncompress them first. The gunzip command would allow us to do this – it uncompresses the file it’s given and writes the uncompressed version to a new file.\nWe could do this, but there’s another approach. FASTQ files can get big, and sometimes it helps to be able to keep them compressed as much as possible. It’s a good time for us to explore the pipe.\n\n\n4.2 | (pipe)\nWe talked earlier about command line expressions having 3 parts – the command itself, options and arguments, and output. By default, any output is printed to the screen. That’s what we’ve seen so far. But you can also redirect the output, and there are three primary ways to redirect it…\n\nWith >, which is followed by the name of a text file the output will be written to\nWith >>, which is simlar to > but will append the output (that is, it won’t overwrite any existing content like >)\nWith | (pipe), which takes the output of one command and “pipes” it as input for a subsequent command.\n\nLet’s try to preview the contents of one of the compressed files.\n\n\n4.3 head\nThe head command is a great way to preview the contents of a text file. By default, head prints the first 10 lines of a file. Since these are FASTQ files, let’s print 8 lines (a multiple of 4 – it will become clear why shortly). We can use the -n argument to specify the number of lines that will be returned.\n\n\n\n\n\n\nThis isn’t what we want – we’re seeing the first 8 lines of the compressed files - not helpful.\n\n\n4.4 zcat\nThe zcat function prints human-readable contents of a gzip-compressed file to the screen. We can try running it on the file, but remember the file is pretty big – there are lots of lines of text in there that will all get printed to the screen. Instead, we can pipe the output of zcat to the head command.\n\n\n\n\n\n\nMuch better – this is what the raw RNAseq data look like!\n\n\n4.5 FASTQ Format\nIf you’re not familiar with it, FASTQ is a very common format for genomic data files. The raw data produced by a high-throughput sequencer will almost certainly be returned to you in this format. These are plain text files, and each sequence that is read by the sequencer is represented by 4 lines:\n\na name (header) line\nthe sequence itself\na plus sign\nquality scores corresponding to each base position in the sequence\n\n\n\n4.6 wc\nSince each read in a FASTQ file is represented by 4 lines, we should expect the number of lines in each of the FASTQ files to be a multiple of 4. Let’s check one. The wc command stands for word count, but by default, it returns the number of words, lines, and characters in a file. The -l option tells it to return just the number of lines, so we’ll use it since that’s all we’re interested in right now. And remember, we’ll want to do this on the uncompressed data.\n\n\n\n\n\n\n\n\n4.7 grep\ngrep allows you to search through a file for specific patterns of text and returns the matching lines. For example, let’s say we wanted to see what sequences in sample SRR7609467 contain the sequence “ACCGATACG”:\n\n\n\n\n\n\n\nOn Your Own: Finding a Sequence\nHow many sequences in sample SRR7609467 contain the sequence “CCAGTA”?\n\n\nHint (click here)\n\nPipe the results of the grep to wc -l. Alternatively, check out the -c option to grep in the man page.\n\n\n\nSolutions (click here)\n\n\nzcat SRR7609467.fastq.gz | grep 'CCAGTA' | wc -l\n\nOR\n\nzcat SRR7609467.fastq.gz | grep -c 'CCAGTA'"
  },
  {
    "objectID": "modules/04-shell.html#downloading-files-from-the-web",
    "href": "modules/04-shell.html#downloading-files-from-the-web",
    "title": "The Unix Shell",
    "section": "5 Downloading Files from the Web",
    "text": "5 Downloading Files from the Web\nAt command line, you can’t just open a web browser and download a file you might want. But of course, there are commands to do that. As we move toward starting to analyze our example RNAseq dataset, one thing we’ll need is the reference genome for the species these sequences came from – in this case, Phaseolus vulgaris. Before we get that, it’s another good time to practice some of those common commands…\n\nOn Your Own: Create a Directory\nCreate a new (empty) directory named reference that will later store the reference genome for our analyses. Put it in your own directory inside participants. Then make this reference directory your working directory.\n\n\nHint (click here)\n\nUse the mkdir command (and cd as necessary). Remember that .. moves you up/back one directory, and these can be combined. For example, ../../../ would move you up/back 3 directories.\n\n\n\nSolution (click here)\n\n\nmkdir ../../reference\n  \ncd ../../reference\n\nOR\n\ncd ../../\n  \nmkdir reference\n   \ncd reference\n\n\n\n\n5.1 curl\ncurl is one command that allows you to download files from online (wget is another). Technically, all you need for curl is the name of the command and the web address for what you want to download.\nHowever, the default for curl is to print the downloaded contents to the screen. This usually isn’t what we want. Instead, we want to save them to a file. One option would be to redirect the output to a text file with >. But curl also has a built-in option to write the contents to a file: -o, so we’ll use that.\nSince the file that gets downloaded is a gzip-compressed, FASTA-formatted text file, we’ll give the name of the file a .fa.gz extension. The reference genome for Phaseolus vulgaris is available at:\n\nhttps://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/499/845/GCF_000499845.1_PhaVulg1_0/GCF_000499845.1_PhaVulg1_0_genomic.fna.gz\n\n\n\n\n\n\n\n\nOn Your Own: Preview a FASTA file\nTry previewing the contents of the reference genome file you just downloaded.\n\n\nHint (click here)\n\nRemember the file is gzip-compressed. Use zcat and pipe the results to head.\n\n\n\nSolution (click here)\n\n\nzcat Pvulg.fa.gz | head\n\n\n\nOK, now we’ve got our raw data (FASTQ) and our reference genome (FASTA). This is a good start in terms of getting ready to start analyzing the data. One more thing we can do now is try to understand a little bit about the samples themselves. There is a tab-separated text file named meta.tsv in the data/meta directory. Let’s take a look at its contents…\n\n\n5.2 less\nless is a command that opens up a text file within your shell. Once you’re finished viewing the file, type q to quit and return to your prompt."
  },
  {
    "objectID": "modules/03-vscode.html#section",
    "href": "modules/03-vscode.html#section",
    "title": "The VS Code Text Editor",
    "section": "5 —–",
    "text": "5 —–"
  },
  {
    "objectID": "modules/05-vars-loops.html#starting-a-vs-code-session-with-an-active-terminal-click-here",
    "href": "modules/05-vars-loops.html#starting-a-vs-code-session-with-an-active-terminal-click-here",
    "title": "Variables and Loops",
    "section": "2 Starting a VS Code session with an active terminal (click here)",
    "text": "2 Starting a VS Code session with an active terminal (click here)\n\nLog in to OSC at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then Code Server.\nIn the form that appears:\n\nEnter 4 or more in the box Number of hours\nTo avoid having to switch folders within VS Code, enter /fs/ess/scratch/PAS2250 in the box Working Directory\nClick Launch.\n\nOn the next page, once the top bar of the box is green and says Runnning, click Connect to VS Code.\nOpen a terminal:    => Terminal => New Terminal.\nIn the terminal, type bash and press Enter.\nType pwd to check you are in /fs/ess/scratch/PAS2250"
  },
  {
    "objectID": "modules/07-software.html#section",
    "href": "modules/07-software.html#section",
    "title": "Using Software at OSC",
    "section": "6 —–",
    "text": "6 —–"
  }
]